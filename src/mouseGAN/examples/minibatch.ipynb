{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "os.system(\"git clone https://github.com/matt-nann/AuthenticCursor.git\")\n",
    "try:\n",
    "  shutil.copytree(\"AuthenticCursor/src\", \"src\")\n",
    "except:\n",
    "  ...\n",
    "shutil.copy(\"AuthenticCursor/requirementsGAN.txt\", \"requirementsGAN.txt\")\n",
    "os.system(\"pip install -r requirementsGAN.txt\")\n",
    "shutil.rmtree(\"AuthenticCursor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "import torchvision.utils as vutils\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Generator\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim, output_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, output_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "    def forward(self, z):\n",
    "        return self.fc(z)\n",
    "\n",
    "# Discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim, num_kernels, kernel_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.minibatch_discrimination = MinibatchDiscrimination(input_dim, num_kernels, kernel_dim)\n",
    "        self.fc = nn.Sequential(\n",
    "            # nn.Linear(input_dim, 512),\n",
    "            nn.Linear(input_dim + num_kernels, 512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(256, 1),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.minibatch_discrimination(x)\n",
    "        return self.fc(x)\n",
    "\n",
    "# Minibatch Discrimination\n",
    "class MinibatchDiscrimination(nn.Module):\n",
    "    def __init__(self, input_features, num_kernels, kernel_dim):\n",
    "        super(MinibatchDiscrimination, self).__init__()\n",
    "        self.input_features = input_features\n",
    "        self.num_kernels = num_kernels\n",
    "        self.kernel_dim = kernel_dim\n",
    "        self.T = nn.Parameter(torch.randn(input_features, num_kernels * kernel_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        M = torch.matmul(x, self.T).view(-1, self.num_kernels, self.kernel_dim)\n",
    "        diffs = M.unsqueeze(0) - M.transpose(0, 1).unsqueeze(2)\n",
    "        abs_diffs = torch.sum(torch.abs(diffs), dim=2)\n",
    "        minibatch_features = torch.sum(torch.exp(-abs_diffs), dim=2).T\n",
    "        return torch.cat((x, minibatch_features), dim=1)\n",
    "\n",
    "# Gradient Penalty\n",
    "def compute_gradient_penalty(D, real_samples, fake_samples, phi=1):\n",
    "    assert real_samples.shape == fake_samples.shape\n",
    "    # Random weight term for interpolation between real and fake samples\n",
    "    alpha = torch.rand((real_samples.size(0), 1)).to(device).requires_grad_(False)\n",
    "    real_samples.requires_grad_(True)\n",
    "    fake_samples.requires_grad_(True)\n",
    "    # Get random interpolation between real and fake samples\n",
    "    interpolated = alpha * real_samples + (1 - alpha) * fake_samples\n",
    "    # calculate probability of interpolated examples\n",
    "    with torch.backends.cudnn.flags(enabled=False):\n",
    "        prob_interpolated = D(interpolated)\n",
    "    ones = torch.ones(prob_interpolated.size()).to(device).requires_grad_(True)\n",
    "    gradients = torch.autograd.grad(\n",
    "        outputs=prob_interpolated,\n",
    "        inputs=interpolated,\n",
    "        grad_outputs=ones,\n",
    "        create_graph=True)[0]\n",
    "    gradients = gradients.reshape(gradients.size(0), -1)\n",
    "    gradient_penalty = (\n",
    "        torch.mean((gradients.view(gradients.size(0), -1).norm(2, dim=1) - 1) ** 2)\n",
    "    )   \n",
    "    return gradient_penalty\n",
    "\n",
    "# Hyperparameters\n",
    "latent_dim = 100\n",
    "image_size = 28 * 28\n",
    "batch_size = 128\n",
    "num_epochs = 50\n",
    "learning_rate = 0.0002\n",
    "num_kernels = 5\n",
    "kernel_dim = 3\n",
    "lambda_gp = 10  # Gradient penalty lambda hyperparameter\n",
    "\n",
    "# Load the dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,)),\n",
    "])\n",
    "dataset = MNIST(root=\"./data\", train=True, transform=transform, download=True)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize generator and discriminator\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# if torch.backends.mps.is_available():\n",
    "#     device = torch.device(\"mps\")\n",
    "\n",
    "generator = Generator(latent_dim, image_size).to(device)\n",
    "discriminator = Discriminator(image_size, num_kernels, kernel_dim).to(device)\n",
    "\n",
    "# Loss weights\n",
    "real_label = -1.\n",
    "fake_label = 1.\n",
    "\n",
    "# Optimizers\n",
    "generator_optimizer = optim.Adam(generator.parameters(), lr=learning_rate, betas=(0.5, 0.9))\n",
    "discriminator_optimizer = optim.Adam(discriminator.parameters(), lr=learning_rate, betas=(0.5, 0.9))\n",
    "\n",
    "# Training Loop\n",
    "total_steps = len(dataloader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (real_images, _) in enumerate(dataloader):\n",
    "        _batch_size = real_images.size(0)\n",
    "        real_images = real_images.view(_batch_size, -1).to(device)\n",
    "\n",
    "        # -----------------\n",
    "        #  Train Generator\n",
    "        # -----------------\n",
    "        generator_optimizer.zero_grad()\n",
    "\n",
    "        z = torch.randn(_batch_size, latent_dim).to(device)\n",
    "        fake_images = generator(z)\n",
    "        fake_validity = discriminator(fake_images)\n",
    "        g_loss = -torch.mean(fake_validity)\n",
    "\n",
    "        g_loss.backward()\n",
    "        generator_optimizer.step()\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "        discriminator_optimizer.zero_grad()\n",
    "\n",
    "        real_validity = discriminator(real_images)\n",
    "        fake_validity = discriminator(fake_images.detach())\n",
    "\n",
    "        gradient_penalty = compute_gradient_penalty(discriminator, real_images.data, fake_images.data)\n",
    "        d_loss = -torch.mean(real_validity) + torch.mean(fake_validity) + lambda_gp * gradient_penalty\n",
    "\n",
    "        d_loss.backward()\n",
    "        discriminator_optimizer.step()\n",
    "\n",
    "        if (i+1) % 200 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{total_steps}], d_loss: {d_loss.item():.4f}, g_loss: {g_loss.item():.4f}\")\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        with torch.no_grad():\n",
    "            z = torch.randn(64, latent_dim).to(device)\n",
    "            fake_images = generator(z)\n",
    "\n",
    "        fake_images = fake_images.view(fake_images.size(0), 1, 28, 28)\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(\"Generated Images\")\n",
    "        plt.imshow(vutils.make_grid(fake_images.cpu(), nrow=8, padding=2, normalize=True).permute(1, 2, 0))\n",
    "        plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvDev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
