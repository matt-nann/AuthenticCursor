{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from src.mouseGAN.dataProcessing import MouseGAN_Data\n",
    "from src.mouseGAN.dataset import getDataloader\n",
    "\n",
    "USE_FAKE_DATA = True\n",
    "dataset = MouseGAN_Data(USE_FAKE_DATA=USE_FAKE_DATA, equal_length=True, lowerLimit=50, upperLimit=80)\n",
    "if USE_FAKE_DATA:\n",
    "    dataset.loadFakeWindMouseData()\n",
    "else:\n",
    "    df_moves, df_trajectory = dataset.collectRawMouseTrajectories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_input_trajectories, norm_buttonTargets = dataset.processMouseData(SHOW_ALL=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# df_cleanedSeq, buttonTarget = dataset.processMouseData(SHOW_ALL=False)\n",
    "# df_abs = dataset.convertToAbsolute(df_cleanedSeq, buttonTarget)\n",
    "# dataset.plotTrajectory(df_abs, buttonTarget, 0)\n",
    "# dataloader = getDataloader(norm_input_trajectories, norm_buttonTargets, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "dataloader = getDataloader(norm_input_trajectories, norm_buttonTargets, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sequence, df_target, start_x, start_y,left, top = dataset.processMouseData(SHOW_ONE=True, num_sequences=0)\n",
    "sequence_id = 0\n",
    "dataset.SHOW_ONE = True\n",
    "df_abs = dataset.convertToAbsolute(df_sequence, df_target)\n",
    "dataset.plotTrajectory(df_abs, df_target[['width','height','start_x','start_y']], sequence_id)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## verifying the mean trajectory is centered around zero (even class distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "averageMove = np.array(dataset.input_trajectories).mean(axis=0)\n",
    "# averageMove = averageMove * dataset.std_traj + dataset.mean_traj\n",
    "df_sequence = pd.DataFrame(averageMove, columns=['dx','dy'])\n",
    "df_sequence['velocity'] = np.sqrt(df_sequence['dx']**2 + df_sequence['dy']**2) / dataset.FIXED_TIMESTEP\n",
    "df_target = pd.DataFrame(np.array(dataset.buttonTargets).mean(axis=0), columns=['width','height','start_x','start_y'])\n",
    "sequence_id = 0\n",
    "dataset.SHOW_ONE = True\n",
    "dataset.SHOW_ALL = False\n",
    "df_abs = dataset.convertToAbsolute(df_sequence, df_target)\n",
    "dataset.plotTrajectory(df_abs, df_target[['width','height','start_x','start_y']], sequence_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(5):\n",
    "#     df_sequence = pd.DataFrame(norm_input_trajectories[i] * dataset.std_traj + dataset.mean_traj, columns=['dx','dy'])\n",
    "#     df_sequence['velocity'] = np.sqrt(df_sequence['dx']**2 + df_sequence['dy']**2) / dataset.FIXED_TIMESTEP\n",
    "#     df_target = pd.DataFrame(norm_buttonTargets[i] * dataset.std_button + dataset.mean_button, columns=['width','height','start_x','start_y'])\n",
    "#     sequence_id = 0\n",
    "#     dataset.SHOW_ONE = True\n",
    "#     df_abs = dataset.convertToAbsolute(df_sequence, df_target)\n",
    "#     dataset.plotTrajectory(df_abs, df_target[['width','height','start_x','start_y']], sequence_id)\n",
    "\n",
    "# checking the dataloader\n",
    "import plotly.graph_objects as go\n",
    "fig = go.Figure()\n",
    "dataset.SHOW_ONE = False\n",
    "dataset.SHOW_ALL = True\n",
    "\n",
    "for i, data in enumerate(dataloader, 0): \n",
    "    _input_trajectories_padded, _buttonTargets, trajectoryLengths = data\n",
    "    for ii in range(len(_input_trajectories_padded)):\n",
    "        _input_trajectories_padded[ii] * dataset.std_traj + dataset.mean_traj\n",
    "        _buttonTargets[ii] * dataset.std_button + dataset.mean_button\n",
    "        # calculate the mean movement\n",
    "\n",
    "    # print(_input_trajectories_padded[0])\n",
    "    # if i == 3:\n",
    "    #     break\n",
    "    # for ii in range(len(_input_trajectories_padded)):\n",
    "    #     df_sequence = pd.DataFrame(_input_trajectories_padded[ii] * dataset.std_traj + dataset.mean_traj, columns=['dx','dy'])\n",
    "    #     df_sequence['velocity'] = np.sqrt(df_sequence['dx']**2 + df_sequence['dy']**2) / dataset.FIXED_TIMESTEP\n",
    "    #     df_target = pd.DataFrame(_buttonTargets[ii] * dataset.std_button + dataset.mean_button, columns=['width','height','start_x','start_y'])\n",
    "    #     sequence_id = 0\n",
    "    #     dataset.SHOW_ONE = True\n",
    "    #     df_abs = dataset.convertToAbsolute(df_sequence, df_target)\n",
    "    #     dataset.plotTrajectory(df_abs, df_target[['width','height','start_x','start_y']], sequence_id, fig=fig)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### checking the dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "for i, data in enumerate(dataloader, 0): \n",
    "    _input_trajectories_padded, _buttonTargets, trajectoryLengths = data\n",
    "    # print(_input_trajectories_padded[0])\n",
    "    if i == 3:\n",
    "        break\n",
    "    for ii in range(len(_input_trajectories_padded)):\n",
    "        df_sequence = pd.DataFrame(_input_trajectories_padded[ii] * dataset.std_traj + dataset.mean_traj, columns=['dx','dy'])\n",
    "        df_sequence['velocity'] = np.sqrt(df_sequence['dx']**2 + df_sequence['dy']**2) / dataset.FIXED_TIMESTEP\n",
    "        df_target = pd.DataFrame(_buttonTargets[ii] * dataset.std_button + dataset.mean_button, columns=['width','height','start_x','start_y'])\n",
    "        sequence_id = 0\n",
    "        dataset.SHOW_ONE = True\n",
    "        df_abs = dataset.convertToAbsolute(df_sequence, df_target)\n",
    "\n",
    "        fig.add_trace(go.Scatter(x=df_abs['x'], y=df_abs['y'],\n",
    "                mode='lines+markers',\n",
    "                marker=dict(\n",
    "                            size=5, \n",
    "                            # symbol= \"arrow-bar-up\", angleref=\"previous\",\n",
    "                            # size=15,\n",
    "                            # color='grey',),\n",
    "                            color=df_abs['velocity'], colorscale='Viridis', showscale=True, colorbar=dict(title=\"Velocity\")),\n",
    "                \n",
    "                ))\n",
    "fig.update_layout(\n",
    "    width=800,\n",
    "    height=800,)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CPU with WGAN gradient penalty 45 minutes per epoch for 134 batches with 256 batch size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from src.mouseGAN.models import Generator, Discriminator\n",
    "from torch import optim\n",
    "import time\n",
    "import glob\n",
    "\n",
    "\n",
    "EPSILON = 1e-20 # value to use to approximate zero (to prevent undefined results)\n",
    "\n",
    "g_lrn_rate = 0.0002\n",
    "d_lrn_rate = 0.0002\n",
    "\n",
    "# ADAM parameters\n",
    "beta1 = 0.5\n",
    "beta2 = 0.9\n",
    "eps = 1e-8\n",
    "\n",
    "label_smoothing = False\n",
    "feature_matching = False\n",
    "conditional_freezing = False\n",
    "num_epochs = 2000\n",
    "num_feats = norm_input_trajectories[0].shape[1]\n",
    "MAX_GRAD_NORM = 1000\n",
    "latent_dim = 100\n",
    "num_target_feats = norm_buttonTargets[0].shape[1]\n",
    "MAX_SEQ_LEN = norm_input_trajectories[0].shape[0]\n",
    "\n",
    "# parameters for WGAN\n",
    "# the discriminator is trained n_critic times for each time the generator is trained, \n",
    "# which is common practice in WGANs to ensure the discriminator is well trained. \n",
    "# The Wasserstein loss is computed for the discriminator by taking the difference of the means of its outputs for real and fake samples,\n",
    "# and the gradient penalty is computed using the compute_gradient_penalty function you provided.\n",
    "lambda_gp = 10  # The coefficient for the gradient penalty\n",
    "n_critic = 5  # The number of iterations to train the critic for each iteration of the generator\n",
    "\n",
    "print(\"num_feats: \", num_feats, \"num_target_feats: \", num_target_feats, \"MAX_SEQ_LEN: \", MAX_SEQ_LEN)\n",
    "# device = torch.device('mps')\n",
    "# device = torch.device('cpu')\n",
    "device = torch.device('cuda')\n",
    "model = {\n",
    "    'g': Generator(device, num_feats, latent_dim, num_target_feats).to(device),\n",
    "    'd': Discriminator(device, num_feats, num_target_feats).to(device)\n",
    "}\n",
    "optimizer = {\n",
    "    'g': optim.Adam(model['g'].parameters(), lr=g_lrn_rate, betas=(beta1, beta2), eps=eps),\n",
    "    'd': optim.Adam(model['d'].parameters(), lr=d_lrn_rate, betas=(beta1, beta2), eps=eps)\n",
    "}\n",
    "# not used for WGAN\n",
    "# criterion = {\n",
    "#     'g': nn.BCEWithLogitsLoss(),\n",
    "#     'd': nn.BCEWithLogitsLoss()\n",
    "# }\n",
    "\n",
    "\"\"\"\n",
    "In the context of WGAN, the roles of the generator and discriminator (referred to as critic in WGANs) are slightly different than in the original GAN setup. \n",
    "The critic is trained to maximize the difference between its output for real and generated samples (which leads to a positive loss value),\n",
    "while the generator is trained to minimize the output of the critic for its generated samples (which leads to a negative loss value).\n",
    "\"\"\"\n",
    "\n",
    "def compute_gradient_penalty(D, real_samples, fake_samples, buttonTarget, d_state, phi=1):\n",
    "    \"\"\"\n",
    "    TDLR: helps ensure that the GAN learns smoothly and generates realistic samples by measuring and penalizing abrupt changes in the discriminator's predictions.\n",
    "    \n",
    "    Think of it as a measure of how much the discriminator's predictions change as we move between real and fake samples. \n",
    "    The penalty is a way to make sure that these changes are smooth and don't suddenly jump too much.\n",
    "    To calculate the penalty, the function takes the real and fake samples and creates some new samples that are \"in between\" the real and fake ones. \n",
    "    It then asks the discriminator to predict whether these in-between samples are real or fake. \n",
    "    By looking at how the discriminator's predictions change for these in-between samples, we can figure out if the discriminator is behaving smoothly or not.\n",
    "    The function calculates the penalty by measuring the size of these changes and squaring them. \n",
    "    It then averages these squared changes over all the samples. The penalty encourages the discriminator to have gradients (changes) that are close to a certain value.\n",
    "    This helps make the training of the GAN more stable and improves the quality of the generated samples.\n",
    "\n",
    "    doesn't work on MPS device -> RuntimeError: derivative for aten::linear_backward is not implemented\n",
    "\n",
    "    https://github.com/pytorch/pytorch/issues/92206 the issue is closed and solved on github but I wonder if it's not released yet\n",
    "    \"\"\"\n",
    "    assert real_samples.shape == fake_samples.shape\n",
    "    # Random weight term for interpolation between real and fake samples\n",
    "    alpha = torch.rand((real_samples.size(0), 1, 1)).to(device).requires_grad_(False)\n",
    "    # Get random interpolation between real and fake samples\n",
    "    interpolated = alpha * real_samples + (1 - alpha) * fake_samples\n",
    "    # calculate probability of interpolated examples\n",
    "    with torch.backends.cudnn.flags(enabled=False):\n",
    "      prob_interpolated, _, _ = D(interpolated, buttonTarget, d_state)\n",
    "    ones = torch.ones(prob_interpolated.size()).to(device).requires_grad_(True)\n",
    "    gradients = torch.autograd.grad(\n",
    "        outputs=prob_interpolated,\n",
    "        inputs=interpolated,\n",
    "        grad_outputs=ones,\n",
    "        create_graph=True)[0]\n",
    "    gradients = gradients.reshape(gradients.size(0), -1)\n",
    "    gradient_penalty = (\n",
    "        torch.mean((gradients.view(gradients.size(0), -1).norm(2, dim=1) - 1) ** 2)\n",
    "    )   \n",
    "    return gradient_penalty\n",
    "\n",
    "def run_epoch(model, optimizer, dataloader, freeze_g=False, freeze_d=False):\n",
    "    \"\"\"\n",
    "    https://machinelearningmastery.com/how-to-train-stable-generative-adversarial-networks/\n",
    "    A loss of 0.0 in the discriminator is a failure mode.\n",
    "    If loss of the generator steadily decreases, it is likely fooling the discriminator with garbage images.\n",
    "    \"\"\"\n",
    "    model['g'].train()\n",
    "    model['d'].train()\n",
    "\n",
    "    loss = {}\n",
    "    g_loss_total, d_loss_total = 0.0, 0.0\n",
    "    num_corrects, num_sample = 0, 0\n",
    "\n",
    "    for i, data in enumerate(dataloader, 0): \n",
    "        _input_trajectories_padded, _buttonTargets, trajectoryLengths = data\n",
    "        if len(_input_trajectories_padded) != BATCH_SIZE:\n",
    "            continue\n",
    "        _input_trajectories_padded = _input_trajectories_padded.to(device)\n",
    "        _buttonTargets = _buttonTargets.to(device)\n",
    "        real_batch_sz = _input_trajectories_padded.shape[0]\n",
    "        _buttonTargets = _buttonTargets.squeeze(1)\n",
    "\n",
    "        # get initial states\n",
    "        g_states = model['g'].init_hidden(real_batch_sz)\n",
    "        d_state = model['d'].init_hidden(real_batch_sz)\n",
    "\n",
    "        # sampling from spherical distribution\n",
    "        z = torch.randn([real_batch_sz, MAX_SEQ_LEN, num_feats]).to(device)\n",
    "        z = z / z.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        # feed inputs to generator\n",
    "        g_feats, _ = model['g'](z, _buttonTargets, g_states)\n",
    "        \n",
    "        ### DISCRIMINATOR ####\n",
    "        for _ in range(1): \n",
    "            if not freeze_d:\n",
    "                optimizer['d'].zero_grad()\n",
    "            # feed real input to discriminator\n",
    "            d_real_out, d_real_lstm_out, _state = model['d'](_input_trajectories_padded, _buttonTargets, d_state)\n",
    "            # Compute gradient penalty\n",
    "            gradient_penalty = compute_gradient_penalty(model['d'], _input_trajectories_padded, g_feats, _buttonTargets, d_state, phi=1)\n",
    "            if not freeze_d:\n",
    "                optimizer['d'].zero_grad()\n",
    "            # feed generated input to discriminator\n",
    "            d_fake_out, d_fake_lstm_out, _state = model['d'](g_feats, _buttonTargets, d_state)\n",
    "            # Compute the WGAN loss for the discriminator\n",
    "            loss['d'] = torch.mean(d_fake_out) - torch.mean(d_real_out) + lambda_gp * gradient_penalty\n",
    "            if not freeze_d:\n",
    "                loss['d'].backward() # Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed).\n",
    "                # loss['d'].backward() # Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed).\n",
    "                optimizer['d'].step()\n",
    "\n",
    "        #### GENERATOR ####\n",
    "        if not freeze_g:\n",
    "            optimizer['g'].zero_grad()\n",
    "        \n",
    "        # Generate a batch of samples\n",
    "        g_feats, _ = model['g'](z, _buttonTargets, g_states)\n",
    "        \n",
    "        # Loss measures generator's ability to fool the discriminator\n",
    "        d_logits_gen, _, _ = model['d'](g_feats, _buttonTargets, d_state)\n",
    "        # NOTE from stackoverflow The generator loss is not very meaningful in WGAN. Also in general, there is nothing wrong with negative numbers at all.\n",
    "        loss['g'] = -torch.mean(d_logits_gen)\n",
    "        \n",
    "        if not freeze_g:\n",
    "            loss['g'].backward()\n",
    "            optimizer['g'].step()\n",
    "\n",
    "        g_loss_total += loss['g'].item()\n",
    "        d_loss_total += loss['d'].item()\n",
    "        num_corrects += (d_real_out > 0.5).sum().item() + (d_fake_out < 0.5).sum().item()\n",
    "        num_sample += real_batch_sz\n",
    "        print(\"\\tBatch %d/%d, g_loss = %.3f, d_loss = %.3f, d_acc = %.3f\" % (i, len(dataloader), loss['g'].item(), loss['d'].item(), 100 * num_corrects / (2 * num_sample)), end='\\r')\n",
    "\n",
    "    g_loss_avg = g_loss_total / num_sample\n",
    "    d_loss_avg = d_loss_total / num_sample\n",
    "    d_acc = 100 * num_corrects / (2 * num_sample) # 2 because (real + generated)\n",
    "\n",
    "    return model, g_loss_avg, d_loss_avg, d_acc\n",
    "    \n",
    "CKPT_DIR = '/'\n",
    "\n",
    "try:\n",
    "  from google.colab import drive\n",
    "  # This will prompt for authorization.\n",
    "  drive.mount('/content/drive')\n",
    "\n",
    "  CKPT_DIR = '/content/drive/My Drive/mouseGAN_models'  # or the directory in your Google Drive where you want to save the models\n",
    "except:\n",
    "  ...\n",
    "\n",
    "LOAD_PRETRAINED = True\n",
    "\n",
    "# Function to find latest model file\n",
    "def find_latest_model(model_type, path):\n",
    "    list_of_files = glob.glob(os.path.join(path, model_type + '*.pt')) \n",
    "    if not list_of_files:\n",
    "        return None\n",
    "    latest_file = max(list_of_files, key=os.path.getctime)\n",
    "    return latest_file\n",
    "  \n",
    "def find_epoch_model(model_type, epoch, path):\n",
    "    list_of_files = glob.glob(os.path.join(path, model_type + f'{epoch}.pt')) \n",
    "    if not list_of_files:\n",
    "        return None\n",
    "    latest_file = max(list_of_files, key=os.path.getctime)\n",
    "    return latest_file\n",
    "\n",
    "epoch = 0\n",
    "if LOAD_PRETRAINED:\n",
    "    latest_g_model = find_latest_model('g', CKPT_DIR)\n",
    "    latest_d_model = find_latest_model('d', CKPT_DIR)\n",
    "    if latest_g_model is not None:\n",
    "        model['g'].load_state_dict(torch.load(latest_g_model))\n",
    "        print(f\"Loaded generator model: {latest_g_model}\")\n",
    "    if latest_d_model is not None:\n",
    "        model['d'].load_state_dict(torch.load(latest_d_model))\n",
    "        print(f\"Loaded discriminator model: {latest_d_model}\")\n",
    "    if latest_g_model is not None and latest_d_model is not None:\n",
    "        epoch = min(int(latest_g_model.split('/')[-1].split('.')[0][1:]), int(latest_d_model.split('/')[-1].split('.')[0][1:]))\n",
    "        print(f\"Starting from epoch {epoch}\")\n",
    "    else:\n",
    "        print(\"No pretrained models found. Starting from scratch.\")\n",
    "\n",
    "save_num_epoch = 10\n",
    "freeze_d = False\n",
    "for ep in range(epoch, num_epochs):\n",
    "    start_time = time.time()\n",
    "    model, trn_g_loss, trn_d_loss, trn_acc = run_epoch(model, optimizer, dataloader, ep, freeze_d=freeze_d)\n",
    "    if conditional_freezing:\n",
    "        # conditional freezing\n",
    "        freeze_d = False\n",
    "        if trn_acc >= 95.0:\n",
    "            freeze_d = True\n",
    "\n",
    "    if ep % save_num_epoch == 0 and ep > 0:\n",
    "        G_FN = 'g' + str(ep) + '.pt'\n",
    "        D_FN = 'd' + str(ep) + '.pt'\n",
    "        generatorPath = os.path.join(CKPT_DIR, G_FN)\n",
    "        discriminatorPath = os.path.join(CKPT_DIR, D_FN)\n",
    "        torch.save(model['g'].state_dict(), generatorPath)\n",
    "        print(\"\\tSaved generator: %s\" % generatorPath)\n",
    "        torch.save(model['d'].state_dict(), discriminatorPath)\n",
    "        print(\"\\tSaved discriminator: %s\" % discriminatorPath)\n",
    "\n",
    "    print(\"Epoch %d: G loss: %.5f, D loss: %.5f, D acc: %.5f took %.2f seconds\" % (ep, trn_g_loss, trn_d_loss, trn_acc, time.time()-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model['d'].eval()\n",
    "model['g'].eval()\n",
    "# z = torch.empty([1, MAX_SEQ_LEN, num_feats]).uniform_().to(device) # random vector\n",
    "# sampling from spherical distribution\n",
    "meanG = []\n",
    "import plotly.graph_objects as go\n",
    "fig = go.Figure()\n",
    "for i in range(10):\n",
    "    for x in range(-100,100, 10):\n",
    "        z = torch.randn([1, MAX_SEQ_LEN, num_feats]).to(device)\n",
    "        z = z / z.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        rawInput = np.array([149.59375,    100.0,       x,      100])\n",
    "        norm_rawInput = (rawInput - dataset.mean_button) / dataset.std_button\n",
    "        buttonTarget = torch.tensor([norm_rawInput], dtype=torch.float32).to(device)\n",
    "\n",
    "        g_states = model['g'].init_hidden(1)\n",
    "        d_state = model['d'].init_hidden(1)\n",
    "\n",
    "        # feed inputs to generator\n",
    "        g_feats, _ = model['g'](z, buttonTarget, g_states)\n",
    "        g_feats = g_feats.squeeze(0)\n",
    "        # meanG.append(g_feats.mean(dim=0).cpu().detach().numpy())\n",
    "\n",
    "        # convert back \n",
    "        g_feats = g_feats.cpu().detach().numpy()\n",
    "\n",
    "        input_trajectories, buttonTargets = dataset.denormalize([g_feats], [norm_rawInput])\n",
    "        input_trajectory = input_trajectories[0]\n",
    "        buttonTarget = buttonTargets[0]\n",
    "        df_sequence = pd.DataFrame(input_trajectory, columns=dataset.trajColumns)\n",
    "        df_target = pd.DataFrame([rawInput], columns=dataset.targetColumns)\n",
    "        sequence_id = 0\n",
    "        print(\"starting location \", rawInput[-2:])\n",
    "        dataset.SHOW_ONE = True\n",
    "        # display(df_sequence)\n",
    "        # display(df_target)\n",
    "        start_x = rawInput[-2]\n",
    "        start_y = rawInput[-1]\n",
    "        sequence_id = 0\n",
    "        dataset.SHOW_ONE = True\n",
    "\n",
    "        df_sequence['distance'] = np.sqrt(df_sequence['dx']**2 + df_sequence['dy']**2)\n",
    "        df_sequence['velocity'] = df_sequence['distance'] / dataset.FIXED_TIMESTEP\n",
    "        df_abs = dataset.convertToAbsolute(df_sequence, df_target)\n",
    "        dataset.plotTrajectory(df_abs, df_target[['width','height','start_x','start_y']], sequence_id, fig=fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotGeneratorSamples():\n",
    "  fig = go.Figure()\n",
    "  AXIAL_RESOLUTION = 10\n",
    "  theta = np.linspace(0, 2*np.pi, AXIAL_RESOLUTION)\n",
    "  low_radius = 100\n",
    "  high_radius = 1000\n",
    "  TOTAL_SAMPLES = 10\n",
    "\n",
    "  trajectories = []\n",
    "  buttonTargets = []\n",
    "  maxRadius = 0\n",
    "\n",
    "  TARGET_WIDTH = 150\n",
    "  TARGET_HEIGHT = 100\n",
    "\n",
    "  for i in range(TOTAL_SAMPLES // AXIAL_RESOLUTION):\n",
    "      radius = np.random.random() * (high_radius - low_radius) + low_radius\n",
    "      radius = 200\n",
    "      maxRadius = max(maxRadius, radius)\n",
    "      x = radius * np.cos(theta) \n",
    "      y = radius * np.sin(theta)\n",
    "      for (x1,y1) in zip(x,y):\n",
    "          z = torch.randn([1, MAX_SEQ_LEN, num_feats]).to(device)\n",
    "          z = z / z.norm(dim=-1, keepdim=True)\n",
    "\n",
    "          rawInput = np.array([TARGET_WIDTH,    TARGET_HEIGHT,       x1,      y1])\n",
    "\n",
    "          norm_rawInput = (rawInput - dataset.mean_button) / dataset.std_button\n",
    "          buttonTarget = torch.tensor([norm_rawInput], dtype=torch.float32).to(device)\n",
    "\n",
    "          g_states = model['g'].init_hidden(1)\n",
    "          d_state = model['d'].init_hidden(1)\n",
    "\n",
    "          # feed inputs to generator\n",
    "          g_feats, _ = model['g'](z, buttonTarget, g_states)\n",
    "          g_feats = g_feats.squeeze(0)\n",
    "          # meanG.append(g_feats.mean(dim=0).cpu().detach().numpy())\n",
    "\n",
    "          # convert back \n",
    "          g_feats = g_feats.cpu().detach().numpy()\n",
    "\n",
    "          input_trajectories, buttonTargets = dataset.denormalize([g_feats], [norm_rawInput])\n",
    "          input_trajectory = input_trajectories[0]\n",
    "          buttonTarget = buttonTargets[0]\n",
    "          df_sequence = pd.DataFrame(input_trajectory, columns=dataset.trajColumns)\n",
    "          df_target = pd.DataFrame([rawInput], columns=dataset.targetColumns)\n",
    "          sequence_id = 0\n",
    "          # print(\"starting location \", rawInput[-2:])\n",
    "          dataset.SHOW_ONE = True\n",
    "          # display(df_sequence)\n",
    "          # display(df_target)\n",
    "          start_x = rawInput[-2]\n",
    "          start_y = rawInput[-1]\n",
    "          sequence_id = 0\n",
    "          dataset.SHOW_ONE = True\n",
    "\n",
    "          df_sequence['distance'] = np.sqrt(df_sequence['dx']**2 + df_sequence['dy']**2)\n",
    "          df_sequence['velocity'] = df_sequence['distance'] / dataset.FIXED_TIMESTEP\n",
    "          df_abs = dataset.convertToAbsolute(df_sequence, df_target)\n",
    "\n",
    "          sequence_id = 0\n",
    "          dataset.SHOW_ONE = True\n",
    "          fig.add_trace(go.Scatter(x=df_abs['x'], y=df_abs['y'],\n",
    "                  mode='lines+markers',\n",
    "                  marker=dict(\n",
    "                              size=5, \n",
    "                              # symbol= \"arrow-bar-up\", angleref=\"previous\",\n",
    "                              # size=15,\n",
    "                              # color='grey',),\n",
    "                              color=df_abs['velocity'], colorscale='Viridis', showscale=True, colorbar=dict(title=\"Velocity\")),\n",
    "                  \n",
    "                  ))\n",
    "  x0, y0 = -TARGET_WIDTH/2, -TARGET_HEIGHT/2\n",
    "  x1, y1 =  TARGET_WIDTH/2, TARGET_HEIGHT/2\n",
    "  square = go.layout.Shape(\n",
    "      type='rect',\n",
    "      x0=x0,\n",
    "      y0=y0,\n",
    "      x1=x1,\n",
    "      y1=y1,\n",
    "      line=dict(color='black', width=2),\n",
    "      fillcolor='rgba(0, 0, 255, 0.3)',\n",
    "  )\n",
    "\n",
    "  fig.update_layout(\n",
    "      shapes=[square],\n",
    "      width=800,\n",
    "      height=800,\n",
    "      xaxis=dict(\n",
    "          range=[-maxRadius*1.1, maxRadius*1.1],)\n",
    "      ,yaxis=dict(\n",
    "          range=[-maxRadius*1.1, maxRadius*1.1],)\n",
    "  )\n",
    "  fig.show()\n",
    "\n",
    "for epoch in [10,20,30,40,50]:\n",
    "    latest_g_model = find_epoch_model('g', epoch, CKPT_DIR)\n",
    "    latest_d_model = find_epoch_model('d', epoch, CKPT_DIR)\n",
    "    if latest_g_model is not None:\n",
    "        model['g'].load_state_dict(torch.load(latest_g_model))\n",
    "        print(f\"Loaded generator model: {latest_g_model}\")\n",
    "    if latest_d_model is not None:\n",
    "        model['d'].load_state_dict(torch.load(latest_d_model))\n",
    "        print(f\"Loaded discriminator model: {latest_d_model}\")\n",
    "    epoch = min(int(latest_g_model.split('/')[-1].split('.')[0][1:]), int(latest_d_model.split('/')[-1].split('.')[0][1:]))\n",
    "    print(f\"Starting from epoch {epoch}\")\n",
    "    plotGeneratorSamples()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
