{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "try:\n",
    "  import google.colab\n",
    "  os.system(\"git clone https://github.com/matt-nann/AuthenticCursor.git\")\n",
    "  try:\n",
    "    shutil.copytree(\"AuthenticCursor/src\", \"src\")\n",
    "  except:\n",
    "    shutil.rmtree(\"src\")\n",
    "    shutil.copytree(\"AuthenticCursor/src\", \"src\")\n",
    "  try:\n",
    "    shutil.copy(\"AuthenticCursor/requirementsGAN.txt\", \"requirementsGAN.txt\")\n",
    "  except:\n",
    "    shutil.rmtree(\"requirementsGAN.txt\")\n",
    "    shutil.copy(\"AuthenticCursor/requirementsGAN.txt\", \"requirementsGAN.txt\")\n",
    "  # remove conflicting dependencies with google colab preinstalled libraries\n",
    "  with open(\"requirementsGAN.txt\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "    with open(\"requirementsGAN.txt\", \"w\") as f:\n",
    "      for line in lines:\n",
    "        if \"numpy\" not in line and 'pillow' not in line:\n",
    "          f.write(line)\n",
    "  os.system(\"pip install -r requirementsGAN.txt\")\n",
    "  shutil.rmtree(\"AuthenticCursor\")\n",
    "  # installing and logging into weights and biases\n",
    "  os.system(\"pip install wandb\")\n",
    "  os.system(\"wandb login\")\n",
    "except Exception as e:\n",
    "  print(e)\n",
    "\n",
    "import torch\n",
    "import wandb # will be prompted for API key in google colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.mouseGAN.dataProcessing import MouseGAN_Data\n",
    "from src.mouseGAN.dataset import getDataloader, visuallyVertifyDataloader\n",
    "\n",
    "USE_FAKE_DATA = True\n",
    "SAVE_FAKE_DATA = False\n",
    "RELOAD_FAKE_DATA = True\n",
    "TRAIN_TEST_SPLIT = 0.8\n",
    "dataset = MouseGAN_Data(USE_FAKE_DATA=USE_FAKE_DATA, TRAIN_TEST_SPLIT=TRAIN_TEST_SPLIT, \n",
    "                        equal_length=False)\n",
    "\n",
    "SAMPLES = 1000\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "if USE_FAKE_DATA:\n",
    "    if RELOAD_FAKE_DATA:\n",
    "        dataset.createFakeWindMouseDataset(save=SAVE_FAKE_DATA, samples=SAMPLES,\n",
    "                                        low_radius = 200, high_radius = 1000,\n",
    "                                        max_width = 300, min_width = 25,\n",
    "                                        max_height = 300, min_height = 25,)\n",
    "    else:\n",
    "        dataset.loadFakeWindMouseData()\n",
    "else:\n",
    "    df_moves, df_trajectory = dataset.collectRawMouseTrajectories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "s_time = time.time()\n",
    "train_trajs, train_targets, test_trajs, test_targets = dataset.processMouseData(SHOW_ALL=False)\n",
    "print(f\"Time to process data: {time.time() - s_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.mouseGAN.model_config import Config, LR_SCHEDULERS, LOSS_FUNC, \\\n",
    "    C_MiniBatchDisc, C_Discriminator, C_Generator, C_EMA_Plateua_Sch, \\\n",
    "    C_Step_Sch, C_LossGap_Sch\n",
    "from src.mouseGAN.models import MouseGAN\n",
    "from src.mouseGAN.experimentTracker import initialize_wandb\n",
    "\n",
    "# IN_COLAB = True\n",
    "LOAD_PRETRAINED = False\n",
    "BATCH_SIZE = 256\n",
    "num_epochs = 1000\n",
    "num_feats = train_trajs[0].shape[1]\n",
    "latent_dim = 100\n",
    "num_target_feats = 4 # width, height, start_x, start_y\n",
    "MAX_SEQ_LEN = max([len(traj) for traj in train_trajs + test_trajs])\n",
    "\n",
    "D_config = C_Discriminator(lr=0.0001, bidirectional=True, hidden_units=128, \n",
    "                            num_lstm_layers=4, useEndDeviationLoss=True,\n",
    "                            gradient_maxNorm = 1.0,)\n",
    "G_config = C_Generator(lr=0.0001, hidden_units=128, num_lstm_layers=4, useOutsideTargetLoss=True, drop_prob=0.1,\n",
    "                layer_normalization = True,\n",
    "                residual_connections = True,\n",
    "                gradient_maxNorm = 1.0,)\n",
    "\n",
    "# D_sch_config = C_Step_Sch(2, 0.5)\n",
    "D_sch_config = C_LossGap_Sch(cooldown=int(BATCH_SIZE)/8, lr_shrinkMin=0.1, lr_growthMax=2.0, \n",
    "                            discLossDecay=0.8, lr_max = 0.0005, lr_min = 1*10**(-9))\n",
    "# G_sch_config = C_Step_Sch(2, 0.5)\n",
    "# G_sch_config = C_EMA_Plateua_Sch(patience=BATCH_SIZE, cooldown=int(BATCH_SIZE/8), factor=0.5, ema_alpha=0.4)\n",
    "\n",
    "config = Config(num_epochs, BATCH_SIZE, num_feats, latent_dim, num_target_feats, MAX_SEQ_LEN,\n",
    "                discriminator=D_config, generator=G_config, \n",
    "                D_lr_scheduler=D_sch_config, #G_lr_scheduler=G_sch_config,\n",
    "                locationMSELoss = False)\n",
    "\n",
    "## verifying the mean trajectory is centered around zero (even class distribution)\n",
    "# dataset.plotMeanPath()\n",
    "trainDataloader = getDataloader(train_trajs, train_targets, config.BATCH_SIZE)\n",
    "testDataloader = getDataloader(test_trajs, test_targets, config.BATCH_SIZE)\n",
    "\n",
    "visuallyVertifyDataloader(trainDataloader, dataset, showNumBatches=1)\n",
    "\n",
    "if IN_COLAB:\n",
    "    run = initialize_wandb(config)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "gan = MouseGAN(dataset, trainDataloader, testDataloader, device, config, IN_COLAB=IN_COLAB, verbose=True, printBatch=True)\n",
    "if LOAD_PRETRAINED:\n",
    "    \n",
    "    gan.loadPretrained(startingEpoch='final')\n",
    "\n",
    "print(gan.discriminator)\n",
    "print(gan.generator)\n",
    "\n",
    "gan.train(modelSaveInterval=3, catchErrors=False)\n",
    "if IN_COLAB:\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan.visualTrainingVerfication(samples=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan.train(modelSaveInterval=3, catchErrors=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gan.save_models('final')\n",
    "gan.loadPretrained(startingEpoch=99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in ['final']:\n",
    "    gan.loadPretrained(startingEpoch=epoch)\n",
    "    gan.visualTrainingVerfication()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4000 training samples\n",
    "print('E1B0 : start of g_loss -> before g_stop_token_loss', 3245.4140625 - 3244.5859375)\n",
    "print('E1B1 : start of g_loss -> before g_stop_token_loss', 3254.94921875 - 3249.27734375)\n",
    "print('E1B2 : start of g_loss -> before g_stop_token_loss', 3261.1328125 - 3255.4609375)\n",
    "print('E1B3 : start of g_loss -> before g_stop_token_loss', 3270.91796875 - 3261.89453125)\n",
    "print('E1B4 : start of g_loss -> before g_stop_token_loss', 3277.6171875 - 3271.6875)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "epoch:  761144832\n",
    "\t batch:  0 761144832\n",
    "d_loss 4.928228855133057 d_loss_real 0.9162368774414062 d_loss_fake 1.0765330791473389 d_loss_dev 3.9318437576293945\n",
    "g_loss 0.9645814299583435 g_stop_token_loss_length 66.0078125 g_stop_token_loss * lambda_stopLoss 3.3003906250000004\n",
    "\tBatch 1/32, d_loss = 4.928, g_loss = 30.607\n",
    "\t batch:  1 788964352\n",
    "d_loss 4.848016738891602 d_loss_real 0.9385039210319519 d_loss_fake 1.0357764959335327 d_loss_dev 3.8608765602111816\n",
    "g_loss 1.0467503070831299 g_stop_token_loss_length 68.86328125 g_stop_token_loss * lambda_stopLoss 3.4431640625\n",
    "\tBatch 2/32, d_loss = 4.848, g_loss = 31.141\n",
    "\t batch:  2 788964864\n",
    "d_loss 5.018941402435303 d_loss_real 0.9796853065490723 d_loss_fake 0.9515709280967712 d_loss_dev 4.053313255310059\n",
    "g_loss 1.2331814765930176 g_stop_token_loss_length 67.1484375 g_stop_token_loss * lambda_stopLoss 3.357421875\n",
    "\tBatch 3/32, d_loss = 5.019, g_loss = 29.385\n",
    "\t batch:  3 788964864\n",
    "d_loss 4.711278438568115 d_loss_real 1.0567747354507446 d_loss_fake 0.8240370750427246 d_loss_dev 3.7708725929260254\n",
    "g_loss 1.42510187625885 g_stop_token_loss_length 67.98828125 g_stop_token_loss * lambda_stopLoss 3.3994140625\n",
    "\tBatch 4/32, d_loss = 4.711, g_loss = 25.259\n",
    "\t batch:  4 789058560\n",
    "d_loss 4.616765022277832 d_loss_real 1.1126352548599243 d_loss_fake 0.6521314382553101 d_loss_dev 3.734381914138794\n",
    "g_loss 1.7698607444763184 g_stop_token_loss_length 65.2578125 g_stop_token_loss * lambda_stopLoss 3.2628906250000003\n",
    "\tBatch 5/32, d_loss = 4.617, g_loss = 24.820\n",
    "\t batch:  5 788964864\n",
    "d_loss 4.494575023651123 d_loss_real 1.1078312397003174 d_loss_fake 0.4396367073059082 d_loss_dev 3.7208409309387207\n",
    "g_loss 2.6413750648498535 g_stop_token_loss_length 74.1796875 g_stop_token_loss * lambda_stopLoss 3.708984375\n",
    "\tBatch 6/32, d_loss = 4.495, g_loss = 28.195\n",
    "\t batch:  6 788964864\n",
    "d_loss 4.501669883728027 d_loss_real 1.3164254426956177 d_loss_fake 0.24257078766822815 d_loss_dev 3.7221715450286865\n",
    "g_loss 1.6450245380401611 g_stop_token_loss_length 66.55078125 g_stop_token_loss * lambda_stopLoss 3.3275390625\n",
    "\tBatch 7/32, d_loss = 4.502, g_loss = 23.690\n",
    "\t batch:  7 788964864\n",
    "d_loss 4.322404861450195 d_loss_real 0.8055024743080139 d_loss_fake 0.5061573386192322 d_loss_dev 3.6665749549865723\n",
    "g_loss 2.041877269744873 g_stop_token_loss_length 62.91015625 g_stop_token_loss * lambda_stopLoss 3.1455078125\n",
    "\tBatch 8/32, d_loss = 4.322, g_loss = 23.193\n",
    "\t batch:  8 788964864\n",
    "d_loss 4.225168228149414 d_loss_real 0.7835075259208679 d_loss_fake 0.37089186906814575 d_loss_dev 3.6479687690734863"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[788964352, 788964864, 788964864, 789058560, 788964864, 788964864, 788964864, 788964864]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import os\n",
    "import psutil\n",
    "\n",
    "def print_memory_usage(tag):\n",
    "    process = psutil.Process(os.getpid())\n",
    "    print(f'{tag} : Memory usage: {process.memory_info().rss / 1024 ** 2} MB')\n",
    "\n",
    "\n",
    "# Create some tensors\n",
    "x = torch.randn((10, 10), requires_grad=True)\n",
    "y = torch.randn((10, 10), requires_grad=True)\n",
    "graphList = []\n",
    "with torch.profiler.profile(profile_memory=True, record_shapes=True) as prof:\n",
    "    for i in range(20):\n",
    "        print_memory_usage('gi')\n",
    "        # Run your training step\n",
    "        z = x + y\n",
    "        graphList.append(z.sum().backward())\n",
    "\n",
    "# Print profiler output\n",
    "print(prof.key_averages().table(sort_by=\"self_cpu_memory_usage\", row_limit=10))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
