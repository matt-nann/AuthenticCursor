{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "try:\n",
    "  import google.colab\n",
    "  os.system(\"git clone https://github.com/matt-nann/AuthenticCursor.git\")\n",
    "  try:\n",
    "    shutil.copytree(\"AuthenticCursor/src\", \"src\")\n",
    "  except:\n",
    "    shutil.rmtree(\"src\")\n",
    "    shutil.copytree(\"AuthenticCursor/src\", \"src\")\n",
    "  try:\n",
    "    shutil.copy(\"AuthenticCursor/requirementsGAN.txt\", \"requirementsGAN.txt\")\n",
    "  except:\n",
    "    shutil.rmtree(\"requirementsGAN.txt\")\n",
    "    shutil.copy(\"AuthenticCursor/requirementsGAN.txt\", \"requirementsGAN.txt\")\n",
    "  # remove conflicting dependencies with google colab preinstalled libraries\n",
    "  with open(\"requirementsGAN.txt\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "    with open(\"requirementsGAN.txt\", \"w\") as f:\n",
    "      for line in lines:\n",
    "        if \"numpy\" not in line and 'pillow' not in line:\n",
    "          f.write(line)\n",
    "  os.system(\"pip install -r requirementsGAN.txt\")\n",
    "  shutil.rmtree(\"AuthenticCursor\")\n",
    "  # installing and logging into weights and biases\n",
    "  os.system(\"pip install wandb\")\n",
    "  os.system(\"wandb login\")\n",
    "except Exception as e:\n",
    "  print(e)\n",
    "\n",
    "import torch\n",
    "import wandb # will be prompted for API key in google colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.mouseGAN.dataProcessing import MouseGAN_Data\n",
    "from src.mouseGAN.dataset import getDataloader, visuallyVertifyDataloader\n",
    "\n",
    "USE_FAKE_DATA = True\n",
    "SAVE_FAKE_DATA = False\n",
    "RELOAD_FAKE_DATA = True\n",
    "TRAIN_TEST_SPLIT = 0.8\n",
    "dataset = MouseGAN_Data(USE_FAKE_DATA=USE_FAKE_DATA, TRAIN_TEST_SPLIT=TRAIN_TEST_SPLIT, \n",
    "                        equal_length=False)\n",
    "\n",
    "SAMPLES = 10000\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "if USE_FAKE_DATA:\n",
    "    if RELOAD_FAKE_DATA:\n",
    "        # dataset.createFakeWindMouseDataset(save=SAVE_FAKE_DATA, samples=SAMPLES,\n",
    "        #                                 low_radius = 200, high_radius = 1000,\n",
    "        #                                 max_width = 300, min_width = 25,\n",
    "        #                                 max_height = 300, min_height = 25,)\n",
    "        dataset.createFakeWindMouseDataset(save=SAVE_FAKE_DATA, samples=SAMPLES,\n",
    "                                        low_radius = 65, high_radius = 200,\n",
    "                                        max_width = 60, min_width = 50,\n",
    "                                        max_height = 60, min_height = 50,)\n",
    "    else:\n",
    "        dataset.loadFakeWindMouseData()\n",
    "else:\n",
    "    df_moves, df_trajectory = dataset.collectRawMouseTrajectories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "s_time = time.time()\n",
    "train_trajs, train_targets, test_trajs, test_targets = dataset.processMouseData(SHOW_ALL=False)\n",
    "print(f\"Time to process data: {time.time() - s_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.mouseGAN.model_config import Config, LR_SCHEDULERS, LOSS_FUNC, \\\n",
    "    C_MiniBatchDisc, C_Discriminator, C_Generator, C_EMA_Plateua_Sch, \\\n",
    "    C_Step_Sch, C_LossGap_Sch\n",
    "from src.mouseGAN.models import MouseGAN\n",
    "from src.mouseGAN.experimentTracker import initialize_wandb\n",
    "\n",
    "# IN_COLAB = True\n",
    "LOAD_PRETRAINED = False\n",
    "BATCH_SIZE = 64\n",
    "num_epochs = 1000\n",
    "num_feats = train_trajs[0].shape[1]\n",
    "latent_dim = 100\n",
    "num_target_feats = 4 # width, height, start_x, start_y\n",
    "MAX_SEQ_LEN = max([len(traj) for traj in train_trajs + test_trajs])\n",
    "\n",
    "D_config = C_Discriminator(lr=0.004, bidirectional=True, hidden_units=128, \n",
    "                            num_lstm_layers=4, useEndDeviationLoss=True,\n",
    "                            gradient_maxNorm = 1.0,)\n",
    "G_config = C_Generator(lr=0.001, hidden_units=128, num_lstm_layers=4, drop_prob=0.1,\n",
    "                layer_normalization = True,\n",
    "                residual_connections = True,\n",
    "                gradient_maxNorm = 1.0,\n",
    "                useLengthLoss=False,\n",
    "                useOutsideTargetLoss=False)\n",
    "\n",
    "# D_sch_config = C_Step_Sch(2, 0.5)\n",
    "D_sch_config = C_LossGap_Sch(cooldown=int(BATCH_SIZE)/8, lr_shrinkMin=0.1, lr_growthMax=2.0, \n",
    "                            discLossDecay=0.8, lr_max = D_config.lr, lr_min = 1*10**(-9))\n",
    "# G_sch_config = C_Step_Sch(2, 0.5)\n",
    "# G_sch_config = C_EMA_Plateua_Sch(patience=BATCH_SIZE, cooldown=int(BATCH_SIZE/8), factor=0.5, ema_alpha=0.4)\n",
    "\n",
    "config = Config(num_epochs, BATCH_SIZE, num_feats, latent_dim, num_target_feats, MAX_SEQ_LEN,\n",
    "                discriminator=D_config, generator=G_config, \n",
    "                # D_lr_scheduler=D_sch_config, #G_lr_scheduler=G_sch_config,\n",
    "                locationMSELoss = False)\n",
    "\n",
    "## verifying the mean trajectory is centered around zero (even class distribution)\n",
    "# dataset.plotMeanPath()\n",
    "trainLoader = getDataloader(train_trajs, train_targets, config.BATCH_SIZE)\n",
    "testLoader = getDataloader(test_trajs, test_targets, config.BATCH_SIZE)\n",
    "\n",
    "visuallyVertifyDataloader(trainLoader, dataset, showNumBatches=1)\n",
    "\n",
    "if IN_COLAB:\n",
    "    run = initialize_wandb(config)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "gan = MouseGAN(dataset, trainLoader, testLoader, device, config, IN_COLAB=IN_COLAB, verbose=True, printBatch=True)\n",
    "if LOAD_PRETRAINED:\n",
    "    \n",
    "    gan.loadPretrained(startingEpoch='final')\n",
    "\n",
    "print(gan.discriminator)\n",
    "print(gan.generator)\n",
    "\n",
    "# gan.find_learning_rates_for_GAN()\n",
    "gan.train(modelSaveInterval=3, catchErrors=False)\n",
    "if IN_COLAB:\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "\n",
    "class LRFinder:\n",
    "    def __init__(self, model, optimizer, criterion, device):\n",
    "        self.optimizer = optimizer\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.device = device\n",
    "        torch.save(model.state_dict(), 'init_params.pt')\n",
    "\n",
    "    def range_test(self, loader, end_lr=10, num_iter=100, smooth_f=0.05, diverge_th=5):\n",
    "        lrs = []\n",
    "        losses = []\n",
    "        best_loss = float('inf')\n",
    "        lr_scheduler = ExponentialLR(self.optimizer, end_lr, num_iter)\n",
    "        for batch_idx, (inputs, targets) in enumerate(loader):\n",
    "            inputs = inputs.to(self.device)\n",
    "            targets = targets.to(self.device)\n",
    "\n",
    "            # Forward pass\n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.model(inputs)\n",
    "            loss = self.criterion(outputs, targets)\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            # Update lr\n",
    "            lr_scheduler.step()\n",
    "            lrs.append(lr_scheduler.get_lr()[0])\n",
    "\n",
    "            # Update smooth loss\n",
    "            if batch_idx == 0:\n",
    "                smooth_loss = loss.item()\n",
    "            else:\n",
    "                smooth_loss = smooth_f * loss.item() + (1 - smooth_f) * smooth_loss\n",
    "            losses.append(smooth_loss)\n",
    "\n",
    "            # Check if the loss has diverged; if it has, stop the test\n",
    "            if batch_idx > 0 and smooth_loss > diverge_th * best_loss:\n",
    "                break\n",
    "                \n",
    "            # Record best loss\n",
    "            if smooth_loss < best_loss or batch_idx == 0:\n",
    "                best_loss = smooth_loss\n",
    "\n",
    "        return lrs, losses\n",
    "\n",
    "class ExponentialLR(_LRScheduler):\n",
    "    def __init__(self, optimizer, end_lr, num_iter, last_epoch=-1):\n",
    "        self.end_lr = end_lr\n",
    "        self.num_iter = num_iter\n",
    "        super(ExponentialLR, self).__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        curr_iter = self.last_epoch\n",
    "        r = curr_iter / self.num_iter\n",
    "        return [base_lr * (self.end_lr / base_lr) ** r for base_lr in self.base_lrs]\n",
    "    \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def find_learning_rate(self):\n",
    "    lr_finder = LRFinder(gan.generator, gan.optimizer_G, gan.generatorLoss, device=\"cuda\")\n",
    "    lrs, losses = lr_finder.range_test(trainLoader, end_lr=1, num_iter=100)\n",
    "\n",
    "    # Plot learning rate sweep for generator\n",
    "    plt.figure()\n",
    "    plt.plot(lrs, losses)\n",
    "    plt.xscale('log')\n",
    "    plt.xlabel('Learning rate')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Generator Learning rate sweep')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    lr_finder = LRFinder(self.discriminator, gan.optimizer_D, gan.discriminatorLoss, device=\"cuda\")\n",
    "    lrs, losses = lr_finder.range_test(self.trainLoader, end_lr=1, num_iter=100)\n",
    "\n",
    "    # Plot learning rate sweep for discriminator\n",
    "    plt.figure()\n",
    "    plt.plot(lrs, losses)\n",
    "    plt.xscale('log')\n",
    "    plt.xlabel('Learning rate')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Discriminator Learning rate sweep')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan.visualTrainingVerfication(samples=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan.train(modelSaveInterval=3, catchErrors=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gan.save_models('final')\n",
    "gan.loadPretrained(startingEpoch=99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in ['final']:\n",
    "    gan.loadPretrained(startingEpoch=epoch)\n",
    "    gan.visualTrainingVerfication()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
