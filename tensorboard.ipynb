{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir=logdir "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 62\u001b[0m\n\u001b[1;32m     60\u001b[0m fixed_noise \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn(\u001b[39m100\u001b[39m, z_dim)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     61\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[0;32m---> 62\u001b[0m     \u001b[39mfor\u001b[39;00m i, (real_images, _) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dataloader):\n\u001b[1;32m     63\u001b[0m         real_images \u001b[39m=\u001b[39m real_images\u001b[39m.\u001b[39mview(real_images\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m), \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     64\u001b[0m         batch_size \u001b[39m=\u001b[39m real_images\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Code/AuthenticCursor/venvDev/lib/python3.8/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/Documents/Code/AuthenticCursor/venvDev/lib/python3.8/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/Documents/Code/AuthenticCursor/venvDev/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/Documents/Code/AuthenticCursor/venvDev/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/Documents/Code/AuthenticCursor/venvDev/lib/python3.8/site-packages/torchvision/datasets/mnist.py:142\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    138\u001b[0m img, target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata[index], \u001b[39mint\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtargets[index])\n\u001b[1;32m    140\u001b[0m \u001b[39m# doing this so that it is consistent with all other datasets\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[39m# to return a PIL Image\u001b[39;00m\n\u001b[0;32m--> 142\u001b[0m img \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39;49mfromarray(img\u001b[39m.\u001b[39;49mnumpy(), mode\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mL\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    144\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    145\u001b[0m     img \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform(img)\n",
      "File \u001b[0;32m~/Documents/Code/AuthenticCursor/venvDev/lib/python3.8/site-packages/PIL/Image.py:3101\u001b[0m, in \u001b[0;36mfromarray\u001b[0;34m(obj, mode)\u001b[0m\n\u001b[1;32m   3099\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   3100\u001b[0m     ndmax \u001b[39m=\u001b[39m \u001b[39m4\u001b[39m\n\u001b[0;32m-> 3101\u001b[0m \u001b[39mif\u001b[39;00m ndim \u001b[39m>\u001b[39;49m ndmax:\n\u001b[1;32m   3102\u001b[0m     msg \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mToo many dimensions: \u001b[39m\u001b[39m{\u001b[39;00mndim\u001b[39m}\u001b[39;00m\u001b[39m > \u001b[39m\u001b[39m{\u001b[39;00mndmax\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3103\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Simple Feed-forward GAN\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim=20, out_dim=784):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(z_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, out_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_dim=784):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# Data Preparation\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize([0.5], [0.5])])\n",
    "dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=100, shuffle=True)\n",
    "\n",
    "# Initialize Networks\n",
    "z_dim = 20\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "G = Generator(z_dim=z_dim).to(device)\n",
    "D = Discriminator().to(device)\n",
    "\n",
    "# Initialize Optimizers\n",
    "lr = 0.0002\n",
    "G_optimizer = Adam(G.parameters(), lr=lr)\n",
    "D_optimizer = Adam(D.parameters(), lr=lr)\n",
    "\n",
    "# Initialize Loss Function\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# TensorBoard Summary Writer\n",
    "writer = SummaryWriter()\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 100\n",
    "fixed_noise = torch.randn(100, z_dim).to(device)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (real_images, _) in enumerate(dataloader):\n",
    "        real_images = real_images.view(real_images.size(0), -1).to(device)\n",
    "        batch_size = real_images.size(0)\n",
    "\n",
    "        # Train Discriminator\n",
    "        D_real = D(real_images)\n",
    "        D_real_loss = criterion(D_real, torch.ones(batch_size, 1).to(device))\n",
    "\n",
    "        z = torch.randn(batch_size, z_dim).to(device)\n",
    "        fake_images = G(z)\n",
    "        D_fake = D(fake_images)\n",
    "        D_fake_loss = criterion(D_fake, torch.zeros(batch_size, 1).to(device))\n",
    "\n",
    "        D_loss = D_real_loss + D_fake_loss\n",
    "\n",
    "        D_optimizer.zero_grad()\n",
    "        D_loss.backward()\n",
    "        D_optimizer.step()\n",
    "\n",
    "        # Train Generator\n",
    "        z = torch.randn(batch_size, z_dim).to(device)\n",
    "        fake_images = G(z)\n",
    "        D_fake = D(fake_images)\n",
    "        G_loss = criterion(D_fake, torch.ones(batch_size, 1).to(device))\n",
    "\n",
    "        G_optimizer.zero_grad()\n",
    "        G_loss.backward()\n",
    "        G_optimizer.step()\n",
    "\n",
    "        # Log Losses\n",
    "        if i % 50 == 0:\n",
    "            writer.add_scalar('Loss/Discriminator', D_loss.item(), epoch * len(dataloader) + i)\n",
    "            writer.add_scalar('Loss/Generator', G_loss.item(), epoch * len(dataloader) + i)\n",
    "\n",
    "    # Log Generated Images\n",
    "    if epoch % 5 == 0:\n",
    "        with torch.no_grad():\n",
    "            fake_images = G(fixed_noise)\n",
    "            img_grid = make_grid(fake_images.view(-1, 1, 28, 28), nrow=10, normalize=True)\n",
    "            writer.add_image('Generated Images', img_grid, global_step=epoch)\n",
    "\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/mnann/Documents/Code/AuthenticCursor/wandb/run-20230608_224604-4hdx3wg5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mnann/my-awesome-project/runs/4hdx3wg5' target=\"_blank\">vibrant-sky-2</a></strong> to <a href='https://wandb.ai/mnann/my-awesome-project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mnann/my-awesome-project' target=\"_blank\">https://wandb.ai/mnann/my-awesome-project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mnann/my-awesome-project/runs/4hdx3wg5' target=\"_blank\">https://wandb.ai/mnann/my-awesome-project/runs/4hdx3wg5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>▂▁▆▄▆█▇▇</td></tr><tr><td>loss</td><td>█▃▄▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>0.822</td></tr><tr><td>loss</td><td>0.15463</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">vibrant-sky-2</strong> at: <a href='https://wandb.ai/mnann/my-awesome-project/runs/4hdx3wg5' target=\"_blank\">https://wandb.ai/mnann/my-awesome-project/runs/4hdx3wg5</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230608_224604-4hdx3wg5/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "import random\n",
    "\n",
    "# start a new wandb run to track this script\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"my-awesome-project\",\n",
    "    \n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": 0.0000000001,\n",
    "    \"architecture\": \"CNN\",\n",
    "    \"dataset\": \"CIFAR-100\",\n",
    "    \"epochs\": 10,\n",
    "    }\n",
    ")\n",
    "\n",
    "# simulate training\n",
    "epochs = 10\n",
    "offset = random.random() / 5\n",
    "for epoch in range(2, epochs):\n",
    "    acc = 1 - 2 ** -epoch - random.random() / epoch - offset\n",
    "    loss = 2 ** -epoch + random.random() / epoch + offset\n",
    "    \n",
    "    # log metrics to wandb\n",
    "    wandb.log({\"acc\": acc, \"loss\": loss})\n",
    "    \n",
    "# [optional] finish the wandb run, necessary in notebooks\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvDev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
