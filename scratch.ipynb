{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[87], line 637\u001b[0m\n\u001b[1;32m    635\u001b[0m g_input \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((z, labels_one_hot), dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    636\u001b[0m fake_images \u001b[39m=\u001b[39m generator(g_input)\n\u001b[0;32m--> 637\u001b[0m fake_validities, d_fakeClass \u001b[39m=\u001b[39m discriminator(fake_images)\n\u001b[1;32m    638\u001b[0m \u001b[39m# g_loss should minimize the difference in predicting classes among the same classes\u001b[39;00m\n\u001b[1;32m    639\u001b[0m g_fakeClassLoss \u001b[39m=\u001b[39m classCriterion(d_fakeClass, labels_one_hot)\n",
      "File \u001b[0;32m~/Documents/Code/AuthenticCursor/venvDev/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/Code/AuthenticCursor/venvDev/lib/python3.8/site-packages/torch/amp/autocast_mode.py:14\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     12\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_autocast\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     13\u001b[0m     \u001b[39mwith\u001b[39;00m autocast_instance:\n\u001b[0;32m---> 14\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "Cell \u001b[0;32mIn[87], line 297\u001b[0m, in \u001b[0;36mDiscriminator.forward\u001b[0;34m(self, images)\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[39m@autocast\u001b[39m()\n\u001b[1;32m    296\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, images):\n\u001b[0;32m--> 297\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmain(images)\n\u001b[1;32m    298\u001b[0m     \u001b[39mreturn\u001b[39;00m x[:, \u001b[39m0\u001b[39m], nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39msoftmax(x[:, \u001b[39m1\u001b[39m:], dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Code/AuthenticCursor/venvDev/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/Code/AuthenticCursor/venvDev/lib/python3.8/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/Documents/Code/AuthenticCursor/venvDev/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/Code/AuthenticCursor/venvDev/lib/python3.8/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/Documents/Code/AuthenticCursor/venvDev/lib/python3.8/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    %pip install wandb\n",
    "    %pip install --upgrade \"kaleido==0.1.*\"\n",
    "    import kaleido\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.utils.data as data_utils\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "import torchvision.utils as vutils\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from io import BytesIO\n",
    "import plotly.io as pio\n",
    "import tempfile\n",
    "from PIL import Image\n",
    "import io\n",
    "import wandb\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import time\n",
    "from enum import Enum\n",
    "import plotly.express as px\n",
    "import math\n",
    "\n",
    "import dataclasses\n",
    "from typing import Any, Dict, List, Optional, Tuple, Union\n",
    "from dataclasses import dataclass\n",
    "\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "class TempFileContext:\n",
    "    def __enter__(self):\n",
    "        self.tmp_file = tempfile.NamedTemporaryFile(suffix=\".jpeg\", delete=False)\n",
    "        self.tmp_filename = self.tmp_file.name\n",
    "        return self.tmp_filename\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        self.tmp_file.close()\n",
    "        os.remove(self.tmp_filename)\n",
    "\n",
    "# As per the DCGAN paper: All the weights are initialized from a zero centered normal distribution with standard deviation 0.02\n",
    "def initialize_weights(model):\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.BatchNorm2d)):\n",
    "            nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "\n",
    "class GeneratorUpSample(nn.Module):\n",
    "    def __init__(self, channels_noise, channels_img, num_classes, features_g):\n",
    "        \"\"\"\n",
    "        channels_noise: The size of the input noise vector. This noise vector is a random input from which the generator begins the generation of a new sample.\n",
    "        channels_img: The number of output channels of the generator. This will typically be 1 for grayscale images or 3 for color (RGB) images.\n",
    "        num_classes: The number of distinct classes or labels that the generator should generate images for. This is used to form the one-hot vector of class labels, which is concatenated to the noise vector to provide the generator with information about the class of image to generate.\n",
    "        \"\"\"\n",
    "        # Conv2d formula => output_size = (input_size - 1) * stride - 2 * padding + kernel_size\n",
    "        super(Generator, self).__init__()\n",
    "        self.gen = nn.Sequential(\n",
    "            self.gen_block(channels_noise + num_classes, features_g * 4, kernel_size=7, stride=1, padding=0),  # output: (features_g*4) x 7 x 7 # Append class labels to input noise.\n",
    "            nn.Dropout(p=0.05),\n",
    "            self._block(features_g * 4, features_g * 2, kernel_size=4, stride=2, padding=1),  # output: (features_g*2) x 6 x 6\n",
    "            nn.Upsample(scale_factor=4, mode='bilinear'),  # output: (features_g*2) x 14 x 14\n",
    "            nn.Dropout(p=0.05),\n",
    "            self._block(features_g * 2, features_g, kernel_size=4, stride=2, padding=1),  # output: features_g x 14 x 14\n",
    "            nn.Upsample(scale_factor=5, mode='bilinear'),  # output: features_g x 28 x 28\n",
    "            nn.Dropout(p=0.05),\n",
    "            self._block(features_g, channels_img, kernel_size=7, stride=1, padding=2),  # output: channels_img x 28 x 28\n",
    "            nn.Tanh(),  # normalize [-1, 1]\n",
    "        )\n",
    "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "    def gen_block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size,\n",
    "                stride,\n",
    "                padding,\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "    @autocast() # automatically applies precisions to different operations to speed up calculations\n",
    "    def forward(self, noise):\n",
    "        return self.gen(noise)\n",
    "\n",
    "class GeneratorFractional(nn.Module):\n",
    "    def __init__(self, channels_noise, channels_img, num_classes, features_g):\n",
    "        \"\"\"\n",
    "        channels_noise: The size of the input noise vector. This noise vector is a random input from which the generator begins the generation of a new sample.\n",
    "        channels_img: The number of output channels of the generator. This will typically be 1 for grayscale images or 3 for color (RGB) images.\n",
    "        num_classes: The number of distinct classes or labels that the generator should generate images for. This is used to form the one-hot vector of class labels, which is concatenated to the noise vector to provide the generator with information about the class of image to generate.\n",
    "        \"\"\"\n",
    "        super(Generator, self).__init__()\n",
    "        self.gen = nn.Sequential(\n",
    "            self._block(channels_noise + num_classes, features_g * 8, kernel_size=4, stride=2, padding=1),  # output: (features_g*8) x 2 x 2\n",
    "            self._block(features_g * 8, features_g * 4, kernel_size=4, stride=2, padding=1),  # output: (features_g*4) x 4 x 4 \n",
    "            self._block(features_g * 4, features_g * 2, kernel_size=4, stride=2, padding=1),  # output: (features_g*2) x 8 x 8\n",
    "            self._block(features_g * 2, features_g, kernel_size=4, stride=2, padding=1),  # output: features_g x 16 x 16 \n",
    "            nn.ConvTranspose2d(features_g, channels_img, kernel_size=4, stride=2, padding=3),  # output: channels_img x 28 x 28\n",
    "            nn.Tanh(),  # normalize inputs to [-1, 1]\n",
    "        )\n",
    "\n",
    "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size,\n",
    "                stride,\n",
    "                padding,\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "    @autocast() # automatically applies precisions to different operations to speed up calculations\n",
    "    def forward(self, noise):\n",
    "        return self.gen(noise)\n",
    "\n",
    "class GeneratorConv(nn.Module):\n",
    "    \"\"\"\n",
    "    GeneratorPixelShuffle\n",
    "    \"\"\"\n",
    "    def __init__(self, channels_noise, channels_img, num_classes, features_g):\n",
    "        super(Generator, self).__init__()\n",
    "        self.initial = nn.Sequential(\n",
    "            nn.Linear(channels_noise + num_classes, features_g * 4 * 7 * 7, bias=False),\n",
    "            nn.BatchNorm1d(features_g * 4 * 7 * 7),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \"\"\"\n",
    "        In GANs, we start from a random noise vector in a latent space, but we want to generate 2D images. This transformation from a 1D noise vector to a 3D tensor is typically done using a dense layer, which learns to map the latent space effectively to the space of images during the training process.\n",
    "        Also, this fully connected layer allows the model to create complex mappings from the input noise vector to the output, which is essential when generating realistic images. The capacity of this layer can be adjusted via the number of neurons to control the complexity of the generated images.\n",
    "        \"\"\"\n",
    "        self.gen = nn.Sequential(\n",
    "            self._block(features_g * 4, features_g * 2, upscale_factor=2),  # output: (features_g*2) x 14 x 14 \n",
    "            self._block(features_g * 2, features_g, upscale_factor=2),  # output: features_g x 28 x 28 \n",
    "            nn.Conv2d(features_g, channels_img, kernel_size=4, stride=1, padding=3),  # output: channels_img x 28 x 28\n",
    "            nn.Tanh()  # normalize inputs to [-1, 1]\n",
    "        )\n",
    "\n",
    "    def _block(self, in_channels, out_channels, upscale_factor):\n",
    "        return nn.Sequential(\n",
    "            nn.Upsample(scale_factor=upscale_factor, mode='nearest'),\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=4, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "    def forward(self, noise):\n",
    "        noise = noise.view(noise.shape[0], -1)\n",
    "        x = self.initial(noise)\n",
    "        x = x.view(x.shape[0], -1, 7, 7)  # reshape into (batch_size, features_g * 4, 7, 7)\n",
    "        return self.gen(x)\n",
    "\n",
    "class DiscriminatorConv(nn.Module):\n",
    "    def __init__(self, channels_img, num_classes, num_kernels, kernel_dim, filters):\n",
    "        \"\"\"\n",
    "        channels_img: The number of input channels to the discriminator, corresponding to the number of channels in the images to be classified.\n",
    "        features_d: This is the base size of the feature maps in the discriminator. The number of neurons or nodes in each layer of the discriminator is a multiple of this base size.\n",
    "        num_classes: The number of distinct classes that the discriminator should be able to distinguish between. This is used to form the softmax output layer of the discriminator, which outputs a class probability distribution.\n",
    "        num_kernels and kernel_dim: These are parameters for the minibatch discrimination layer. The minibatch discrimination layer is designed to make the discriminator sensitive to the variety of samples within a minibatch, to encourage the generator to generate a variety of different samples. num_kernels is the number of unique patterns the layer can learn to identify, and kernel_dim is the size of these learned patterns.\n",
    "        \"\"\"\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(channels_img, filters, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(p=0.05),\n",
    "            self._block(filters, filters*2, 4, 2, 1),\n",
    "            nn.Dropout(p=0.05),\n",
    "        )\n",
    "        self.mbd = MinibatchDiscrimination(filters*2*7*7, num_kernels, kernel_dim)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(filters*2*7*7 + num_kernels, filters*8),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(filters*8, 1 + num_classes),\n",
    "        )\n",
    "\n",
    "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "    @autocast()\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.mbd(x)\n",
    "        out = self.fc(x)\n",
    "        return out[:, 0], nn.functional.softmax(out[:, 1:], dim=1)\n",
    "\n",
    "class GeneratorFFN(nn.Module):\n",
    "    def __init__(self, channels_noise, channels_img, num_classes, features_g):\n",
    "        super(Generator, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(channels_noise + num_classes, features_g * 4 * 7 * 7, bias=False),\n",
    "            nn.BatchNorm1d(features_g * 4 * 7 * 7),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(features_g * 4 * 7 * 7, features_g * 2 * 14 * 14, bias=False),\n",
    "            nn.BatchNorm1d(features_g * 2 * 14 * 14),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(features_g * 2 * 14 * 14, features_g * 28 * 28, bias=False),\n",
    "            nn.BatchNorm1d(features_g * 28 * 28),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(features_g * 28 * 28, channels_img * 28 * 28),\n",
    "            nn.Tanh()  # normalize inputs to [-1, 1]\n",
    "        )   \n",
    "    @autocast()\n",
    "    def forward(self, noise):\n",
    "        noise = noise.view(noise.shape[0], -1)\n",
    "        return self.net(noise).view(noise.shape[0], -1, 28, 28)\n",
    "\n",
    "class DiscriminatorFNN(nn.Module):\n",
    "    def __init__(self, channels_img, num_classes, num_kernels, kernel_dim, filters):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(channels_img * 28 * 28, filters * 2 * 14 * 14),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(p=0.05),\n",
    "            nn.Linear(filters * 2 * 14 * 14, filters * 4 * 7 * 7),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(p=0.05),\n",
    "            MinibatchDiscrimination(filters * 4 * 7 * 7, num_kernels, kernel_dim),\n",
    "            nn.Linear(filters * 4 * 7 * 7 + num_kernels, filters * 8),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(filters * 8, 1 + num_classes),\n",
    "        )\n",
    "    @autocast()\n",
    "    def forward(self, x):\n",
    "        x = self.net(x.view(x.size(0), -1))\n",
    "        return x[:, 0], nn.functional.softmax(x[:, 1:], dim=1)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    \"\"\"\n",
    "    from machinelearningmastery 128 filters\n",
    "    \"\"\"\n",
    "    def __init__(self, channels_noise, channels_img, num_classes, features_g):\n",
    "        super(Generator, self).__init__()\n",
    "        self.latent_dim = channels_noise\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(channels_noise + num_classes, features_g*7*7),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.BatchNorm1d(features_g*7*7),\n",
    "        )\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.ConvTranspose2d(features_g, features_g, 4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.BatchNorm2d(features_g),\n",
    "            nn.ConvTranspose2d(features_g, 1, 4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(1, 1, 7, stride=1, padding=3),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    @autocast()\n",
    "    def forward(self, z_and_label):\n",
    "        x = self.linear(z_and_label)\n",
    "        x = x.view(x.shape[0], -1, 7, 7)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    \"\"\"\n",
    "    # 64 filters\n",
    "    https://machinelearningmastery.com/how-to-develop-a-generative-adversarial-network-for-an-mnist-handwritten-digits-from-scratch-in-keras/\n",
    "    \"\"\"\n",
    "    def __init__(self, channels_img, num_classes, num_kernels, kernel_dim, filters):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(channels_img, filters, 3, stride=2, padding=1), # 4, 14, 14\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Conv2d(filters, filters, 3, stride=2, padding=1), # 4, 7, 7\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Flatten(),\n",
    "            MinibatchDiscrimination(filters * 7 * 7, num_kernels, kernel_dim),\n",
    "            nn.Linear(filters*7*7 + num_kernels, 1 + num_classes),\n",
    "        )\n",
    "    @autocast()\n",
    "    def forward(self, images):\n",
    "        x = self.main(images)\n",
    "        return x[:, 0], nn.functional.softmax(x[:, 1:], dim=1)\n",
    "\n",
    "def calculate_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "class MinibatchDiscrimination(nn.Module):\n",
    "    def __init__(self, input_features, num_kernels, kernel_dim):\n",
    "        super(MinibatchDiscrimination, self).__init__()\n",
    "        self.input_features = input_features\n",
    "        self.num_kernels = num_kernels\n",
    "        self.kernel_dim = kernel_dim\n",
    "        self.T = nn.Parameter(torch.randn(input_features, num_kernels * kernel_dim))\n",
    "    def forward(self, x):\n",
    "        M = torch.matmul(x, self.T).view(-1, self.num_kernels, self.kernel_dim)\n",
    "        diffs = M.unsqueeze(0) - M.transpose(0, 1).unsqueeze(2)\n",
    "        abs_diffs = torch.sum(torch.abs(diffs), dim=2)\n",
    "        minibatch_features = torch.sum(torch.exp(-abs_diffs), dim=2).T\n",
    "        return torch.cat((x, minibatch_features), dim=1)\n",
    "\n",
    "class LR_Metric(Enum):\n",
    "    VALIDITY = 1\n",
    "    AGE = 2\n",
    "    \n",
    "class CustomDataLoader:\n",
    "    def __init__(self, X, Y, batch_size, device):\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "        self.data = X.float().to(self.device)\n",
    "        self.targets = Y.to(self.device)\n",
    "        self.num_samples = len(self.data)\n",
    "    def __iter__(self):\n",
    "        self.indices = torch.randperm(self.num_samples, device=self.device)\n",
    "        self.idx = 0\n",
    "        return self\n",
    "    def __next__(self):\n",
    "        if self.idx >= self.num_samples:\n",
    "            raise StopIteration\n",
    "        indices = self.indices[self.idx:self.idx+self.batch_size]\n",
    "        batch_data = self.data[indices]\n",
    "        batch_targets = self.targets[indices]\n",
    "        self.idx += self.batch_size\n",
    "        return batch_data, batch_targets\n",
    "    def __len__(self):\n",
    "        return (self.num_samples + self.batch_size - 1) // self.batch_size\n",
    "\n",
    "image_size = 28 * 28\n",
    "@dataclass\n",
    "class Config:\n",
    "    latent_dim: int = 100\n",
    "    batch_size: int = 256 * 2\n",
    "    num_epochs: int = 60\n",
    "    num_kernels: int = 10\n",
    "    kernel_dim: int = 3\n",
    "    d_learning_rate: float = 0.0004\n",
    "    g_learning_rate: float = 0.0002\n",
    "    lr_restarts: int = 2\n",
    "    min_lr: float = 1e-10\n",
    "    lambda_class: int = 1\n",
    "    replay_buffer_size: int = 1000\n",
    "    features_g: int = 32 # 8 features for FFN, 128 for DeepConv\n",
    "    features_d: int = 32 # 4 features for FFN, 64 for DeepConv\n",
    "    logEnd: bool = True\n",
    "    standardization: bool = False\n",
    "\n",
    "c = Config(logEnd=True)\n",
    "\n",
    "train_data = MNIST(root='data/MNIST',train=True,download=True)\n",
    "test_data = MNIST(root='data/MNIST', train=False, download=True)\n",
    "\n",
    "if c.standardization:\n",
    "    mean = train_data.data.float().mean()\n",
    "    std = train_data.data.float().std()\n",
    "    normalized_train_data = (train_data.data.float() - mean) / std\n",
    "    normalized_test_data = (test_data.data.float() - mean) / std\n",
    "else:\n",
    "    # normalize to [-1, 1]\n",
    "    normalized_train_data = 2 * train_data.data.float() / 255 - 1\n",
    "    normalized_test_data = 2 * train_data.data.float() / 255 - 1\n",
    "\n",
    "preppedConfig = {}\n",
    "for k, v in dataclasses.asdict(c).items():\n",
    "    if dataclasses.is_dataclass(v):\n",
    "        preppedConfig[k] = dataclasses.asdict(v)\n",
    "    else:\n",
    "        preppedConfig[k] = v\n",
    "# wandb.init(project=\"mnist-gan\", config=c)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "CUDA = False\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    CUDA = True\n",
    "    # Because the performance of cuDNN algorithms to compute the convolution of different kernel sizes varies, \n",
    "    # the auto-tuner can run a benchmark to find the best algorithm (current algorithms are these, these, and these). \n",
    "    # It’s recommended to use turn on the setting when your input size doesn’t change often. If the input size changes often, \n",
    "    # the auto-tuner needs to benchmark too frequently, which might hurt the performance.\n",
    "\n",
    "train_loader = CustomDataLoader(normalized_train_data, train_data.targets, batch_size=c.batch_size, device=device)\n",
    "test_loader = CustomDataLoader(normalized_test_data, test_data.targets, batch_size=c.batch_size, device=device)\n",
    "\n",
    "# logging every epoch\n",
    "logsPerEpochNames = ['age', 'curGap', 'oldGap', 'oldScore', 'replayScore', 'replayValidity', 'oldValidity', 'accuracy', 'd_lr','g_lr']\n",
    "logsPerEpoch = {k: torch.zeros(c.num_epochs).to(device) for k in logsPerEpochNames}\n",
    "\n",
    "generator = Generator(c.latent_dim, 1, NUM_CLASSES, c.features_g).to(device)\n",
    "initialize_weights(generator)\n",
    "discriminator = Discriminator(1, NUM_CLASSES, c.num_kernels, c.kernel_dim, c.features_d).to(device)\n",
    "initialize_weights(discriminator)\n",
    "\n",
    "# Optimizers\n",
    "generator_optimizer = optim.Adam(generator.parameters(), lr=c.g_learning_rate, betas=(0.5, 0.9))\n",
    "discriminator_optimizer = optim.Adam(discriminator.parameters(), lr=c.d_learning_rate, betas=(0.5, 0.9))\n",
    "\n",
    "train_batches = len(train_loader)\n",
    "\n",
    "# logging every batch\n",
    "logsPerBatchNames = ['real_validity', 'fake_validity', 'd_fakeClassLoss', 'd_realClassLoss', 'd_fakeAccuracy', 'd_realAccuracy', 'd_loss_base', 'g_loss_base', 'g_fakeClassLoss']\n",
    "logsPerBatch = {k: torch.zeros(c.num_epochs * train_batches, device=device) for k in logsPerBatchNames}\n",
    "# logging is varied\n",
    "t_images = []\n",
    "\n",
    "class LearningRateScheduler:\n",
    "    def __init__(self, initial_lr, replay_buffer_size, total_batches, batch_size, METRIC=LR_Metric.VALIDITY):\n",
    "        self.initial_lr = initial_lr\n",
    "        self.replay_buffer_size = replay_buffer_size\n",
    "        self.total_batches = total_batches\n",
    "        self.batch_size = batch_size\n",
    "        self.METRIC = METRIC\n",
    "        self.samplesPerBatch = int(np.ceil(replay_buffer_size / total_batches))\n",
    "        self.filledIndex = 0\n",
    "        dtypeKwargs = {'dtype': torch.float16} if CUDA else {}\n",
    "        self.oldFake_validity = torch.zeros(self.replay_buffer_size, device=device, **dtypeKwargs)\n",
    "        self.oldReal_validity = torch.zeros(self.replay_buffer_size, device=device, **dtypeKwargs)\n",
    "        self.oldFake_validities = torch.zeros(self.replay_buffer_size, device=device, **dtypeKwargs)\n",
    "        self.z_replay = torch.zeros(self.replay_buffer_size, c.latent_dim + NUM_CLASSES, device=device)\n",
    "        self.age = torch.zeros(self.replay_buffer_size, device=device)\n",
    "        self.kickTopPercent = 0.25\n",
    "        self.openIndexes = torch.ones(self.replay_buffer_size, device=device)\n",
    "        self.real_validity_total = torch.zeros(1, device=device)\n",
    "        self.fake_validity_total = torch.zeros(1, device=device)\n",
    "        self.numSamples = 0\n",
    "        # DEBUG\n",
    "        self.replayValiditiesHistory = torch.zeros((c.num_epochs, self.replay_buffer_size), device=device, **dtypeKwargs)\n",
    "        self.fakeValiditiesHistory = torch.zeros((c.num_epochs), device=device, **dtypeKwargs)\n",
    "        self.realValiditiesHistory = torch.zeros((c.num_epochs), device=device, **dtypeKwargs)\n",
    "\n",
    "    def fillReplayBuffer(self, real_validity, real_validities, fake_validity, fake_validities, z):\n",
    "        \"\"\"\n",
    "        samples (amouting to replay_buffer_size) will be evenly provided by all batches to fill the replay buffer in 1 epoch\n",
    "        \"\"\"\n",
    "        with torch.no_grad() and torch.cuda.amp.autocast():\n",
    "            self.real_validity_total += real_validities.sum()\n",
    "            self.fake_validity_total += fake_validities.sum()\n",
    "            self.numSamples += len(z)\n",
    "            openings = (self.openIndexes > 0).sum().item()\n",
    "            numSamples = len(z)  \n",
    "            if self.filledIndex < self.replay_buffer_size: \n",
    "                # start filling the buffer front to back, only fill self.samplesPerBatch to prevent, the early batches from dominanting the replay buffer\n",
    "                remaining = self.replay_buffer_size - self.filledIndex\n",
    "                numSelected = np.min([remaining, numSamples, self.samplesPerBatch])\n",
    "                selected = np.random.choice(numSamples, numSelected, replace=False)\n",
    "                indexes = torch.arange(self.filledIndex, self.filledIndex + len(selected))\n",
    "                self.filledIndex += len(selected)\n",
    "            elif openings:\n",
    "                # randomly select samples to fill the openIndexes in the replay buffer\n",
    "                indexes = torch.nonzero(self.openIndexes).squeeze()\n",
    "                numSelected = np.min([openings, numSamples, self.samplesPerBatch])\n",
    "                selected = np.random.choice(numSamples, numSelected, replace=False)\n",
    "                indexes = np.random.choice(indexes.numel(), numSelected, replace=False)\n",
    "            else:\n",
    "                return\n",
    "            self.oldFake_validity[indexes] = fake_validity.repeat(len(indexes))\n",
    "            self.oldReal_validity[indexes] = real_validity.repeat(len(indexes))\n",
    "            self.oldFake_validities[indexes] = fake_validities[selected].squeeze()\n",
    "            self.z_replay[indexes,:] = z[selected]\n",
    "            self.age[indexes] = 0\n",
    "            self.openIndexes[indexes] = 0\n",
    "\n",
    "    def plotReplayValidities(self):\n",
    "        i_replays = (self.openIndexes == 0).nonzero().squeeze()\n",
    "        fig = go.Figure()\n",
    "        fig.add_trace(go.Histogram(x=self.oldReal_validity[i_replays].cpu().numpy(), name=\"real\"))\n",
    "        fig.add_trace(go.Histogram(x=self.oldFake_validity[i_replays].cpu().numpy(), name=\"fake\"))\n",
    "        fig.update_layout(barmode='overlay', title=\"saved validity scores histogram\")\n",
    "        fig.show()\n",
    "\n",
    "    def update_learning_rate(self, epoch, d, g):\n",
    "        with torch.no_grad():\n",
    "            i_replays = (self.openIndexes == 0).nonzero().squeeze()\n",
    "            z_replay = self.z_replay[i_replays]\n",
    "            fake_replay = g(z_replay)\n",
    "            replayFake_validities, _ = d(fake_replay)\n",
    "            replayFake_validities = replayFake_validities.squeeze()\n",
    "    \n",
    "            # if gaps are negatives then discriminator then fake images are getting higher validity scores than real ones\n",
    "            oldGap = (self.oldReal_validity[i_replays] - self.oldFake_validity[i_replays]).mean()\n",
    "            curGap = self.real_validity_total / self.numSamples - self.fake_validity_total / self.numSamples\n",
    "            # positive - smaller positive\n",
    "\n",
    "            replayScores = replayFake_validities - curGap.repeat(len(i_replays))\n",
    "            oldScores = self.oldFake_validities[i_replays] - oldGap.repeat(len(i_replays))\n",
    "            values = [ self.age[i_replays].mean(), curGap, oldGap, replayScores.mean(), oldScores.mean(), replayFake_validities.mean(), self.oldFake_validities[i_replays].mean()]\n",
    "            for name, value in zip(['age', 'curGap', 'oldGap', 'replayScore', 'oldScore', 'replayValidity', 'oldValidity'], values):\n",
    "                logsPerEpoch[name][epoch] = value\n",
    "\n",
    "            self.replayValiditiesHistory[epoch,i_replays] = replayFake_validities\n",
    "            self.fakeValiditiesHistory[epoch] = self.fake_validity_total / self.numSamples\n",
    "            self.realValiditiesHistory[epoch] = self.real_validity_total / self.numSamples\n",
    "               \n",
    "            if self.METRIC.value == LR_Metric.VALIDITY.value:\n",
    "                metric = replayFake_validities.squeeze()\n",
    "            elif self.METRIC.value == LR_Metric.AGE.value:\n",
    "                metric = self.age[i_replays].squeeze()\n",
    "                raise NotImplementedError(\"needs to be adjusted\")\n",
    "            else:\n",
    "                raise Exception(\"Invalid metric\")\n",
    "            # Kick out top 10% of the replay buffer based on replayScores scores, lowest to highest, drop the highest\n",
    "            # i_highestMetric = torch.argsort(metric)[-int(np.ceil(self.kickTopPercent * self.replay_buffer_size)):]\n",
    "            # self.openIndexes[i_highestMetric] = 1\n",
    "\n",
    "            # kick out first half for testing\n",
    "            # self.openIndexes[:int(self.replay_buffer_size/2)] = torch.ones(int(self.replay_buffer_size/2)).to(device)\n",
    "\n",
    "            self.age += 1\n",
    "\n",
    "class WarmRestartScheduler(torch.optim.lr_scheduler._LRScheduler):\n",
    "    def __init__(self, optimizer, T_0, T_mult=1, eta_max=0.1, last_epoch=-1):\n",
    "        self.T_0 = T_0\n",
    "        self.T_i = T_0\n",
    "        self.T_mult = T_mult\n",
    "        self.eta_max = eta_max\n",
    "        super(WarmRestartScheduler, self).__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        if self.last_epoch > self.T_i:\n",
    "            self.last_epoch = 0\n",
    "            self.T_i *= self.T_mult\n",
    "        return [self.eta_max * (self.last_epoch / self.T_i) for base_lr in self.base_lrs]\n",
    "\n",
    "    def _reset(self):\n",
    "        return WarmRestartScheduler(self.optimizer, self.T_0, self.T_mult, self.eta_max)\n",
    "\n",
    "class SineScheduler(torch.optim.lr_scheduler._LRScheduler):\n",
    "    def __init__(self, optimizer, T_0, T_mult=1, eta_min=0.01, eta_max=0.1, last_epoch=-1):\n",
    "        self.T_0 = T_0\n",
    "        self.T_mult = T_mult\n",
    "        self.eta_min = eta_min\n",
    "        self.eta_max = eta_max\n",
    "        super(SineScheduler, self).__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        return [self.eta_min + (self.eta_max - self.eta_min) * (1 + math.sin(2 * math.pi * self.last_epoch / self.T_0)) / 2\n",
    "                for base_lr in self.base_lrs]\n",
    "\n",
    "lr_scheduler_trial = LearningRateScheduler(initial_lr=0.001, replay_buffer_size=c.replay_buffer_size, total_batches=train_batches, batch_size=c.batch_size)\n",
    "# d_lr_scheduler = lr_scheduler.CosineAnnealingWarmRestarts(discriminator_optimizer, T_0=int(c.num_epochs/c.lr_restarts), T_mult=1, eta_min=c.min_lr)\n",
    "d_lr_scheduler = SineScheduler(discriminator_optimizer, T_0=int(c.num_epochs/c.lr_restarts), T_mult=1, eta_max=c.d_learning_rate, eta_min=c.min_lr)\n",
    "\n",
    "# d_lr_scheduler = lr_scheduler.\n",
    "classCriterion = nn.CrossEntropyLoss()\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "def createGridFakeImages(epoch=0, cubeSide=3, show=False, step=None, log=True):\n",
    "    fig = make_subplots(rows=1, cols=2,\n",
    "                        horizontal_spacing=0.01, \n",
    "                        shared_yaxes=True)\n",
    "    numImages = torch.tensor([cubeSide**2], device=device)\n",
    "    # Generate and plot fake images with labels\n",
    "    labels = torch.randint(0, 10, (numImages,), device=device)\n",
    "    labels_one_hot = torch.zeros(numImages, 10, device=device).scatter_(1, labels.view(numImages, 1), 1)\n",
    "    with torch.no_grad():\n",
    "        z = torch.randn(numImages, c.latent_dim, device=device)\n",
    "        g_input = torch.cat((z, labels_one_hot), dim=1)\n",
    "        fake_images = generator(g_input)\n",
    "        # fake_validities, d_fakeClass = discriminator(fake_images)\n",
    "        # g_fakeClassLoss = classCriterion(d_fakeClass, labels_one_hot)\n",
    "    fig = make_subplots(rows=cubeSide, cols=cubeSide, \n",
    "                        horizontal_spacing = 0.025,\n",
    "                        vertical_spacing = 0.04,\n",
    "                        subplot_titles=[str(label.item()) for label in labels])\n",
    "    fake_images = fake_images.squeeze().cpu().numpy()\n",
    "    for i in range(numImages):\n",
    "        row = int(i/cubeSide) + 1\n",
    "        col = int(i%cubeSide) + 1\n",
    "        imageFlipped = np.flip(fake_images[i], 0)\n",
    "        fig.add_trace(go.Heatmap(z=imageFlipped, \n",
    "                                colorscale='Greys',), row=row, col=col)\n",
    "    fig.update_layout(title_text=\"Generated Images epoch: \" + str(epoch), \n",
    "                    margin=dict(l=0, r=0, t=60, b=0),\n",
    "                    height=400, width=400, showlegend=False)\n",
    "    fig.update_traces(showscale=False)\n",
    "    fig.update_xaxes(visible=False)\n",
    "    fig.update_yaxes(visible=False)\n",
    "    if show:\n",
    "        fig.show()\n",
    "    if log:\n",
    "        if step is None:\n",
    "            raise Exception(\"step must be provided when logging an image\")\n",
    "        # Convert the figure to a JPEG image and log using wandb\n",
    "        image_bytes = pio.to_image(fig, format='jpeg')\n",
    "        if not c.logEnd:\n",
    "            with TempFileContext() as tmp_filename:\n",
    "                with open(tmp_filename, 'wb') as tmp_file:\n",
    "                    tmp_file.write(image_bytes)\n",
    "                wandb.log({'generator_output':wandb.Image(tmp_filename)}, step=step)\n",
    "        else:\n",
    "            t_images.append((image_bytes, step))\n",
    "\n",
    "def check_frozen_parameters(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if not param.requires_grad:\n",
    "            print(f\"Parameter '{name}' is frozen!\")\n",
    "\n",
    "scaler = GradScaler()\n",
    "# GradScaler with PyTorch's autocast prevents gradient underflow in mixed precision training.\n",
    "# It achieves this by scaling up the loss before backward pass to keep float16 gradients from vanishing.\n",
    "# After gradients are computed, they are scaled back before the optimizer updates the model weights.\n",
    "\n",
    "if not c.logEnd:\n",
    "  wandb.watch([generator, discriminator], log=\"all\")\n",
    "\n",
    "for epoch in range(c.num_epochs):\n",
    "    correct, total = 0, 0\n",
    "    epoch_metrics = {}\n",
    "    for i, (real_images, labels) in enumerate(train_loader):\n",
    "        batch_metrics = {}\n",
    "        # s_time = time.time()\n",
    "        # print(f\"Epoch {epoch}/{num_epochs} Batch {i}/{total_steps}\")\n",
    "        _batch_size = real_images.size(0)\n",
    "        real_images = real_images.unsqueeze(1)\n",
    "        labels_one_hot = torch.zeros(_batch_size, NUM_CLASSES, device=device).scatter_(1, labels.view(_batch_size, 1), 1).to(device)\n",
    "\n",
    "        # train generator\n",
    "        # Setting gradients to zeroes by model.zero_grad() or optimizer.zero_grad() would execute memset for all parameters and update gradients with reading and writing operations. \n",
    "        # However, setting the gradients as None would not execute memset and would update gradients with only writing operations.\n",
    "        generator_optimizer.zero_grad(set_to_none=True)\n",
    "        z = torch.randn(_batch_size, c.latent_dim).to(device)\n",
    "        g_input = torch.cat((z, labels_one_hot), dim=1)\n",
    "        fake_images = generator(g_input)\n",
    "        fake_validities, d_fakeClass = discriminator(fake_images)\n",
    "        # g_loss should minimize the difference in predicting classes among the same classes\n",
    "        g_fakeClassLoss = classCriterion(d_fakeClass, labels_one_hot)\n",
    "        # WGAN-GP\n",
    "        # g_loss = -torch.mean(fake_validities) + g_fakeClassLoss * lambda_class\n",
    "        d_logits_gen = fake_validities.view(-1)\n",
    "        # LSGAN\n",
    "        g_loss_base = criterion(d_logits_gen, torch.ones_like(d_logits_gen))\n",
    "        g_loss = g_loss_base + g_fakeClassLoss * c.lambda_class\n",
    "        # g_loss.backward()\n",
    "        # generator_optimizer.step()\n",
    "        scaler.scale(g_loss).backward()\n",
    "        scaler.step(generator_optimizer)\n",
    "        \n",
    "        # train discriminator\n",
    "        discriminator_optimizer.zero_grad(set_to_none=True)\n",
    "        real_validities, d_realClass = discriminator(real_images)\n",
    "        fake_validities, d_fakeClass = discriminator(fake_images.clone().detach())\n",
    "\n",
    "        loss_disc_real = criterion(real_validities, torch.ones_like(real_validities))\n",
    "        loss_disc_fake = criterion(fake_validities, -torch.ones_like(fake_validities)) # modified to -1 from normal LSGAN 0 target\n",
    "        # LSGAN\n",
    "        d_loss_base = (loss_disc_real + loss_disc_fake) / 2\n",
    "        \n",
    "        # gradient_penalty = compute_gradient_penalty(discriminator, real_images.data, fake_images.data)\n",
    "        # d_loss = -torch.mean(real_validities) + torch.mean(fake_validities) + lambda_gp * gradient_penalty\n",
    "        d_fakeClassLoss = classCriterion(d_fakeClass, labels_one_hot)\n",
    "        d_fakeAccuracy = (d_fakeClass.argmax(dim=1) == labels_one_hot.argmax(dim=1)).float().mean()\n",
    "        d_realClassLoss = classCriterion(d_realClass, labels_one_hot)\n",
    "        d_realAccuracy = (d_realClass.argmax(dim=1) == labels_one_hot.argmax(dim=1)).float().mean()\n",
    "        d_loss = d_loss_base + (d_fakeClassLoss + d_realClassLoss) / 2\n",
    "        # d_loss.backward()\n",
    "        # discriminator_optimizer.step()\n",
    "        scaler.scale(d_loss).backward()\n",
    "        scaler.step(discriminator_optimizer)\n",
    "\n",
    "        # print(\"g_loss_base: \", g_loss_base.item(), \"g_fakeClassLoss: \", g_fakeClassLoss.item(), \"d_loss_base: \", d_loss_base.item(), \"d_fakeClassLoss: \", d_fakeClassLoss.item(), \"d_realClassLoss: \", d_realClassLoss.item())\n",
    "\n",
    "        # if i == train_batches - 1:\n",
    "        #     fig = px.imshow(fake_images[0].detach().squeeze().cpu().numpy(), color_continuous_scale='Greys')\n",
    "        #     fig.show()\n",
    "        #     fig = px.imshow(real_images[0].detach().squeeze().cpu().numpy(), color_continuous_scale='Greys')\n",
    "        #     fig.show()\n",
    "\n",
    "        correct += (real_validities > 0).sum().item() + (fake_validities < 0).sum().item()\n",
    "        total += len(real_validities) + len(fake_validities)\n",
    "        # if (i+1) % 200 == 0:\n",
    "        #     print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{train_batches}], d_loss: {d_loss_base.item():.4f}, g_loss: {g_loss.item():.4f}\")\n",
    "        g_input = g_input.view(_batch_size, c.latent_dim + NUM_CLASSES)\n",
    "        real_validity = real_validities.mean()\n",
    "        fake_validity = fake_validities.mean()\n",
    "        lr_scheduler_trial.fillReplayBuffer(real_validity, real_validities, fake_validity, fake_validities, g_input)\n",
    "        # print(\"lr_scheduler_trial: \", time.time() - s_time)\n",
    "        i_step = epoch * train_batches + i\n",
    "        values = [real_validity, fake_validity, d_fakeClassLoss, d_realClassLoss, d_fakeAccuracy, d_realAccuracy, d_loss_base, g_loss_base, g_fakeClassLoss]\n",
    "        for name, value in zip(logsPerBatchNames, values):\n",
    "            logsPerBatch[name][i_step] = value\n",
    "\n",
    "        if not c.logEnd and i != train_batches - 1:\n",
    "            batch_metrics.update({name: value.item() for name, value in zip(logsPerBatchNames, values)})\n",
    "            wandb.log(batch_metrics, step=i_step)\n",
    "\n",
    "        scaler.update()\n",
    "    \n",
    "    check_frozen_parameters(discriminator)\n",
    "    check_frozen_parameters(generator)\n",
    "\n",
    "    d_lr_scheduler.step()\n",
    "    accuracy = correct / total\n",
    "\n",
    "    lr_scheduler_trial.update_learning_rate(epoch, discriminator, generator)\n",
    "\n",
    "    logsPerEpoch['accuracy'][epoch] = accuracy\n",
    "    logsPerEpoch['g_lr'][epoch] = generator_optimizer.param_groups[0]['lr']\n",
    "    logsPerEpoch['d_lr'][epoch] = discriminator_optimizer.param_groups[0]['lr']\n",
    "\n",
    "    if not c.logEnd:\n",
    "        epoch_metrics.update({name: logsPerEpoch[name][epoch].item() for name in logsPerEpochNames})\n",
    "        epoch_metrics.update(batch_metrics)\n",
    "        wandb.log(epoch_metrics, step=i_step)\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{c.num_epochs}], d_loss: {d_loss.item():.4f}, g_loss: {g_loss.item():.4f}, accuracy: {accuracy:.4f}, d_lr: {discriminator_optimizer.param_groups[0]['lr']:.6f}\")\n",
    "    if epoch % 10 == 0:\n",
    "        createGridFakeImages(epoch=epoch,cubeSide=5, show=True, log=True, step=i_step)\n",
    "\n",
    "for name in logsPerEpochNames:\n",
    "    logsPerEpoch[name] = logsPerEpoch[name].cpu().detach().numpy()\n",
    "for name in logsPerBatchNames:\n",
    "    logsPerBatch[name] = logsPerBatch[name].cpu().detach().numpy()\n",
    "\n",
    "def wandbLogAtEnd():\n",
    "    imageIndex = 0\n",
    "    _t_images = t_images.copy()\n",
    "    for epoch in range(c.num_epochs):\n",
    "        for i in range(train_batches):\n",
    "            step = epoch * train_batches + i\n",
    "            if i != train_batches - 1:\n",
    "                metrics = {name : logsPerBatch[name][step] for name in logsPerBatchNames}\n",
    "                wandb.log(metrics, step=step)\n",
    "        epochMetrics = {name : logsPerEpoch[name][epoch] for name in logsPerEpochNames}\n",
    "        epochMetrics.update(metrics)\n",
    "        if len(_t_images) and _t_images[0][1] == step:\n",
    "            with TempFileContext() as tmp_filename:\n",
    "                image_bytes = _t_images[0][0]\n",
    "                with open(tmp_filename, 'wb') as tmp_file:\n",
    "                    tmp_file.write(image_bytes)\n",
    "                epochMetrics['generator_output'] = wandb.Image(tmp_filename)\n",
    "                wandb.log(epochMetrics, step=step)\n",
    "            _t_images = _t_images[1:]\n",
    "        else:\n",
    "            wandb.log(epochMetrics, step=step)\n",
    "    wandb.finish()\n",
    "\n",
    "# if c.logEnd:\n",
    "#     wandbLogAtEnd()\n",
    "# else:\n",
    "#     wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid property specified for object of type plotly.graph_objs.Scatter: 'row'\n\nDid you mean \"dx\"?\n\n    Valid properties:\n        alignmentgroup\n            Set several traces linked to the same position axis or\n            matching axes to the same alignmentgroup. This controls\n            whether bars compute their positional range dependently\n            or independently.\n        cliponaxis\n            Determines whether or not markers and text nodes are\n            clipped about the subplot axes. To show markers and\n            text nodes above axis lines and tick labels, make sure\n            to set `xaxis.layer` and `yaxis.layer` to *below\n            traces*.\n        connectgaps\n            Determines whether or not gaps (i.e. {nan} or missing\n            values) in the provided data arrays are connected.\n        customdata\n            Assigns extra data each datum. This may be useful when\n            listening to hover, click and selection events. Note\n            that, \"scatter\" traces also appends customdata items in\n            the markers DOM elements\n        customdatasrc\n            Sets the source reference on Chart Studio Cloud for\n            `customdata`.\n        dx\n            Sets the x coordinate step. See `x0` for more info.\n        dy\n            Sets the y coordinate step. See `y0` for more info.\n        error_x\n            :class:`plotly.graph_objects.scatter.ErrorX` instance\n            or dict with compatible properties\n        error_y\n            :class:`plotly.graph_objects.scatter.ErrorY` instance\n            or dict with compatible properties\n        fill\n            Sets the area to fill with a solid color. Defaults to\n            \"none\" unless this trace is stacked, then it gets\n            \"tonexty\" (\"tonextx\") if `orientation` is \"v\" (\"h\") Use\n            with `fillcolor` if not \"none\". \"tozerox\" and \"tozeroy\"\n            fill to x=0 and y=0 respectively. \"tonextx\" and\n            \"tonexty\" fill between the endpoints of this trace and\n            the endpoints of the trace before it, connecting those\n            endpoints with straight lines (to make a stacked area\n            graph); if there is no trace before it, they behave\n            like \"tozerox\" and \"tozeroy\". \"toself\" connects the\n            endpoints of the trace (or each segment of the trace if\n            it has gaps) into a closed shape. \"tonext\" fills the\n            space between two traces if one completely encloses the\n            other (eg consecutive contour lines), and behaves like\n            \"toself\" if there is no trace before it. \"tonext\"\n            should not be used if one trace does not enclose the\n            other. Traces in a `stackgroup` will only fill to (or\n            be filled to) other traces in the same group. With\n            multiple `stackgroup`s or some traces stacked and some\n            not, if fill-linked traces are not already consecutive,\n            the later ones will be pushed down in the drawing\n            order.\n        fillcolor\n            Sets the fill color. Defaults to a half-transparent\n            variant of the line color, marker color, or marker line\n            color, whichever is available.\n        fillpattern\n            Sets the pattern within the marker.\n        groupnorm\n            Only relevant when `stackgroup` is used, and only the\n            first `groupnorm` found in the `stackgroup` will be\n            used - including if `visible` is \"legendonly\" but not\n            if it is `false`. Sets the normalization for the sum of\n            this `stackgroup`. With \"fraction\", the value of each\n            trace at each location is divided by the sum of all\n            trace values at that location. \"percent\" is the same\n            but multiplied by 100 to show percentages. If there are\n            multiple subplots, or multiple `stackgroup`s on one\n            subplot, each will be normalized within its own set.\n        hoverinfo\n            Determines which trace information appear on hover. If\n            `none` or `skip` are set, no information is displayed\n            upon hovering. But, if `none` is set, click and hover\n            events are still fired.\n        hoverinfosrc\n            Sets the source reference on Chart Studio Cloud for\n            `hoverinfo`.\n        hoverlabel\n            :class:`plotly.graph_objects.scatter.Hoverlabel`\n            instance or dict with compatible properties\n        hoveron\n            Do the hover effects highlight individual points\n            (markers or line points) or do they highlight filled\n            regions? If the fill is \"toself\" or \"tonext\" and there\n            are no markers or text, then the default is \"fills\",\n            otherwise it is \"points\".\n        hovertemplate\n            Template string used for rendering the information that\n            appear on hover box. Note that this will override\n            `hoverinfo`. Variables are inserted using %{variable},\n            for example \"y: %{y}\" as well as %{xother}, {%_xother},\n            {%_xother_}, {%xother_}. When showing info for several\n            points, \"xother\" will be added to those with different\n            x positions from the first point. An underscore before\n            or after \"(x|y)other\" will add a space on that side,\n            only when this field is shown. Numbers are formatted\n            using d3-format's syntax %{variable:d3-format}, for\n            example \"Price: %{y:$.2f}\".\n            https://github.com/d3/d3-format/tree/v1.4.5#d3-format\n            for details on the formatting syntax. Dates are\n            formatted using d3-time-format's syntax\n            %{variable|d3-time-format}, for example \"Day:\n            %{2019-01-01|%A}\". https://github.com/d3/d3-time-\n            format/tree/v2.2.3#locale_format for details on the\n            date formatting syntax. The variables available in\n            `hovertemplate` are the ones emitted as event data\n            described at this link\n            https://plotly.com/javascript/plotlyjs-events/#event-\n            data. Additionally, every attributes that can be\n            specified per-point (the ones that are `arrayOk: true`)\n            are available.  Anything contained in tag `<extra>` is\n            displayed in the secondary box, for example\n            \"<extra>{fullData.name}</extra>\". To hide the secondary\n            box completely, use an empty tag `<extra></extra>`.\n        hovertemplatesrc\n            Sets the source reference on Chart Studio Cloud for\n            `hovertemplate`.\n        hovertext\n            Sets hover text elements associated with each (x,y)\n            pair. If a single string, the same string appears over\n            all the data points. If an array of string, the items\n            are mapped in order to the this trace's (x,y)\n            coordinates. To be seen, trace `hoverinfo` must contain\n            a \"text\" flag.\n        hovertextsrc\n            Sets the source reference on Chart Studio Cloud for\n            `hovertext`.\n        ids\n            Assigns id labels to each datum. These ids for object\n            constancy of data points during animation. Should be an\n            array of strings, not numbers or any other type.\n        idssrc\n            Sets the source reference on Chart Studio Cloud for\n            `ids`.\n        legendgroup\n            Sets the legend group for this trace. Traces part of\n            the same legend group hide/show at the same time when\n            toggling legend items.\n        legendgrouptitle\n            :class:`plotly.graph_objects.scatter.Legendgrouptitle`\n            instance or dict with compatible properties\n        legendrank\n            Sets the legend rank for this trace. Items and groups\n            with smaller ranks are presented on top/left side while\n            with `*reversed* `legend.traceorder` they are on\n            bottom/right side. The default legendrank is 1000, so\n            that you can use ranks less than 1000 to place certain\n            items before all unranked items, and ranks greater than\n            1000 to go after all unranked items.\n        legendwidth\n            Sets the width (in px or fraction) of the legend for\n            this trace.\n        line\n            :class:`plotly.graph_objects.scatter.Line` instance or\n            dict with compatible properties\n        marker\n            :class:`plotly.graph_objects.scatter.Marker` instance\n            or dict with compatible properties\n        meta\n            Assigns extra meta information associated with this\n            trace that can be used in various text attributes.\n            Attributes such as trace `name`, graph, axis and\n            colorbar `title.text`, annotation `text`\n            `rangeselector`, `updatemenues` and `sliders` `label`\n            text all support `meta`. To access the trace `meta`\n            values in an attribute in the same trace, simply use\n            `%{meta[i]}` where `i` is the index or key of the\n            `meta` item in question. To access trace `meta` in\n            layout attributes, use `%{data[n[.meta[i]}` where `i`\n            is the index or key of the `meta` and `n` is the trace\n            index.\n        metasrc\n            Sets the source reference on Chart Studio Cloud for\n            `meta`.\n        mode\n            Determines the drawing mode for this scatter trace. If\n            the provided `mode` includes \"text\" then the `text`\n            elements appear at the coordinates. Otherwise, the\n            `text` elements appear on hover. If there are less than\n            20 points and the trace is not stacked then the default\n            is \"lines+markers\". Otherwise, \"lines\".\n        name\n            Sets the trace name. The trace name appear as the\n            legend item and on hover.\n        offsetgroup\n            Set several traces linked to the same position axis or\n            matching axes to the same offsetgroup where bars of the\n            same position coordinate will line up.\n        opacity\n            Sets the opacity of the trace.\n        orientation\n            Only relevant in the following cases: 1. when\n            `scattermode` is set to \"group\". 2. when `stackgroup`\n            is used, and only the first `orientation` found in the\n            `stackgroup` will be used - including if `visible` is\n            \"legendonly\" but not if it is `false`. Sets the\n            stacking direction. With \"v\" (\"h\"), the y (x) values of\n            subsequent traces are added. Also affects the default\n            value of `fill`.\n        selected\n            :class:`plotly.graph_objects.scatter.Selected` instance\n            or dict with compatible properties\n        selectedpoints\n            Array containing integer indices of selected points.\n            Has an effect only for traces that support selections.\n            Note that an empty array means an empty selection where\n            the `unselected` are turned on for all points, whereas,\n            any other non-array values means no selection all where\n            the `selected` and `unselected` styles have no effect.\n        showlegend\n            Determines whether or not an item corresponding to this\n            trace is shown in the legend.\n        stackgaps\n            Only relevant when `stackgroup` is used, and only the\n            first `stackgaps` found in the `stackgroup` will be\n            used - including if `visible` is \"legendonly\" but not\n            if it is `false`. Determines how we handle locations at\n            which other traces in this group have data but this one\n            does not. With *infer zero* we insert a zero at these\n            locations. With \"interpolate\" we linearly interpolate\n            between existing values, and extrapolate a constant\n            beyond the existing values.\n        stackgroup\n            Set several scatter traces (on the same subplot) to the\n            same stackgroup in order to add their y values (or\n            their x values if `orientation` is \"h\"). If blank or\n            omitted this trace will not be stacked. Stacking also\n            turns `fill` on by default, using \"tonexty\" (\"tonextx\")\n            if `orientation` is \"h\" (\"v\") and sets the default\n            `mode` to \"lines\" irrespective of point count. You can\n            only stack on a numeric (linear or log) axis. Traces in\n            a `stackgroup` will only fill to (or be filled to)\n            other traces in the same group. With multiple\n            `stackgroup`s or some traces stacked and some not, if\n            fill-linked traces are not already consecutive, the\n            later ones will be pushed down in the drawing order.\n        stream\n            :class:`plotly.graph_objects.scatter.Stream` instance\n            or dict with compatible properties\n        text\n            Sets text elements associated with each (x,y) pair. If\n            a single string, the same string appears over all the\n            data points. If an array of string, the items are\n            mapped in order to the this trace's (x,y) coordinates.\n            If trace `hoverinfo` contains a \"text\" flag and\n            \"hovertext\" is not set, these elements will be seen in\n            the hover labels.\n        textfont\n            Sets the text font.\n        textposition\n            Sets the positions of the `text` elements with respects\n            to the (x,y) coordinates.\n        textpositionsrc\n            Sets the source reference on Chart Studio Cloud for\n            `textposition`.\n        textsrc\n            Sets the source reference on Chart Studio Cloud for\n            `text`.\n        texttemplate\n            Template string used for rendering the information text\n            that appear on points. Note that this will override\n            `textinfo`. Variables are inserted using %{variable},\n            for example \"y: %{y}\". Numbers are formatted using\n            d3-format's syntax %{variable:d3-format}, for example\n            \"Price: %{y:$.2f}\".\n            https://github.com/d3/d3-format/tree/v1.4.5#d3-format\n            for details on the formatting syntax. Dates are\n            formatted using d3-time-format's syntax\n            %{variable|d3-time-format}, for example \"Day:\n            %{2019-01-01|%A}\". https://github.com/d3/d3-time-\n            format/tree/v2.2.3#locale_format for details on the\n            date formatting syntax. Every attributes that can be\n            specified per-point (the ones that are `arrayOk: true`)\n            are available.\n        texttemplatesrc\n            Sets the source reference on Chart Studio Cloud for\n            `texttemplate`.\n        uid\n            Assign an id to this trace, Use this to provide object\n            constancy between traces during animations and\n            transitions.\n        uirevision\n            Controls persistence of some user-driven changes to the\n            trace: `constraintrange` in `parcoords` traces, as well\n            as some `editable: true` modifications such as `name`\n            and `colorbar.title`. Defaults to `layout.uirevision`.\n            Note that other user-driven trace attribute changes are\n            controlled by `layout` attributes: `trace.visible` is\n            controlled by `layout.legend.uirevision`,\n            `selectedpoints` is controlled by\n            `layout.selectionrevision`, and `colorbar.(x|y)`\n            (accessible with `config: {editable: true}`) is\n            controlled by `layout.editrevision`. Trace changes are\n            tracked by `uid`, which only falls back on trace index\n            if no `uid` is provided. So if your app can add/remove\n            traces before the end of the `data` array, such that\n            the same trace has a different index, you can still\n            preserve user-driven changes if you give each trace a\n            `uid` that stays with it as it moves.\n        unselected\n            :class:`plotly.graph_objects.scatter.Unselected`\n            instance or dict with compatible properties\n        visible\n            Determines whether or not this trace is visible. If\n            \"legendonly\", the trace is not drawn, but can appear as\n            a legend item (provided that the legend itself is\n            visible).\n        x\n            Sets the x coordinates.\n        x0\n            Alternate to `x`. Builds a linear space of x\n            coordinates. Use with `dx` where `x0` is the starting\n            coordinate and `dx` the step.\n        xaxis\n            Sets a reference between this trace's x coordinates and\n            a 2D cartesian x axis. If \"x\" (the default value), the\n            x coordinates refer to `layout.xaxis`. If \"x2\", the x\n            coordinates refer to `layout.xaxis2`, and so on.\n        xcalendar\n            Sets the calendar system to use with `x` date data.\n        xhoverformat\n            Sets the hover text formatting rulefor `x`  using d3\n            formatting mini-languages which are very similar to\n            those in Python. For numbers, see:\n            https://github.com/d3/d3-format/tree/v1.4.5#d3-format.\n            And for dates see: https://github.com/d3/d3-time-\n            format/tree/v2.2.3#locale_format. We add two items to\n            d3's date formatter: \"%h\" for half of the year as a\n            decimal number as well as \"%{n}f\" for fractional\n            seconds with n digits. For example, *2016-10-13\n            09:15:23.456* with tickformat \"%H~%M~%S.%2f\" would\n            display *09~15~23.46*By default the values are\n            formatted using `xaxis.hoverformat`.\n        xperiod\n            Only relevant when the axis `type` is \"date\". Sets the\n            period positioning in milliseconds or \"M<n>\" on the x\n            axis. Special values in the form of \"M<n>\" could be\n            used to declare the number of months. In this case `n`\n            must be a positive integer.\n        xperiod0\n            Only relevant when the axis `type` is \"date\". Sets the\n            base for period positioning in milliseconds or date\n            string on the x0 axis. When `x0period` is round number\n            of weeks, the `x0period0` by default would be on a\n            Sunday i.e. 2000-01-02, otherwise it would be at\n            2000-01-01.\n        xperiodalignment\n            Only relevant when the axis `type` is \"date\". Sets the\n            alignment of data points on the x axis.\n        xsrc\n            Sets the source reference on Chart Studio Cloud for\n            `x`.\n        y\n            Sets the y coordinates.\n        y0\n            Alternate to `y`. Builds a linear space of y\n            coordinates. Use with `dy` where `y0` is the starting\n            coordinate and `dy` the step.\n        yaxis\n            Sets a reference between this trace's y coordinates and\n            a 2D cartesian y axis. If \"y\" (the default value), the\n            y coordinates refer to `layout.yaxis`. If \"y2\", the y\n            coordinates refer to `layout.yaxis2`, and so on.\n        ycalendar\n            Sets the calendar system to use with `y` date data.\n        yhoverformat\n            Sets the hover text formatting rulefor `y`  using d3\n            formatting mini-languages which are very similar to\n            those in Python. For numbers, see:\n            https://github.com/d3/d3-format/tree/v1.4.5#d3-format.\n            And for dates see: https://github.com/d3/d3-time-\n            format/tree/v2.2.3#locale_format. We add two items to\n            d3's date formatter: \"%h\" for half of the year as a\n            decimal number as well as \"%{n}f\" for fractional\n            seconds with n digits. For example, *2016-10-13\n            09:15:23.456* with tickformat \"%H~%M~%S.%2f\" would\n            display *09~15~23.46*By default the values are\n            formatted using `yaxis.hoverformat`.\n        yperiod\n            Only relevant when the axis `type` is \"date\". Sets the\n            period positioning in milliseconds or \"M<n>\" on the y\n            axis. Special values in the form of \"M<n>\" could be\n            used to declare the number of months. In this case `n`\n            must be a positive integer.\n        yperiod0\n            Only relevant when the axis `type` is \"date\". Sets the\n            base for period positioning in milliseconds or date\n            string on the y0 axis. When `y0period` is round number\n            of weeks, the `y0period0` by default would be on a\n            Sunday i.e. 2000-01-02, otherwise it would be at\n            2000-01-01.\n        yperiodalignment\n            Only relevant when the axis `type` is \"date\". Sets the\n            alignment of data points on the y axis.\n        ysrc\n            Sets the source reference on Chart Studio Cloud for\n            `y`.\n        \nDid you mean \"dx\"?\n\nBad property path:\nrow\n^^^",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[88], line 21\u001b[0m\n\u001b[1;32m     15\u001b[0m     y_color \u001b[39m=\u001b[39m (\u001b[39m'\u001b[39m\u001b[39mrgba(\u001b[39m\u001b[39m'\u001b[39m\u001b[39m+\u001b[39m\u001b[39mstr\u001b[39m(np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrandint(\u001b[39m0\u001b[39m, high \u001b[39m=\u001b[39m \u001b[39m256\u001b[39m))\u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39m,\u001b[39m\u001b[39m'\u001b[39m\u001b[39m+\u001b[39m\n\u001b[1;32m     16\u001b[0m                 \u001b[39mstr\u001b[39m(np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrandint(\u001b[39m0\u001b[39m, high \u001b[39m=\u001b[39m \u001b[39m256\u001b[39m))\u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39m,\u001b[39m\u001b[39m'\u001b[39m\u001b[39m+\u001b[39m\n\u001b[1;32m     17\u001b[0m                 \u001b[39mstr\u001b[39m(np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrandint(\u001b[39m0\u001b[39m, high \u001b[39m=\u001b[39m \u001b[39m256\u001b[39m)))\n\u001b[1;32m     18\u001b[0m     fig\u001b[39m.\u001b[39madd_trace(go\u001b[39m.\u001b[39mScatter(x\u001b[39m=\u001b[39mx, y\u001b[39m=\u001b[39mchange, mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlines\u001b[39m\u001b[39m'\u001b[39m, \n\u001b[1;32m     19\u001b[0m         line\u001b[39m=\u001b[39m\u001b[39mdict\u001b[39m(width\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m, dash\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mdot\u001b[39m\u001b[39m'\u001b[39m),\n\u001b[1;32m     20\u001b[0m         showlegend\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m), row\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, col\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m---> 21\u001b[0m     fig\u001b[39m.\u001b[39madd_trace(go\u001b[39m.\u001b[39;49mScatter(x\u001b[39m=\u001b[39;49mx, y\u001b[39m=\u001b[39;49my, mode\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mlines\u001b[39;49m\u001b[39m'\u001b[39;49m, line\u001b[39m=\u001b[39;49m\u001b[39mdict\u001b[39;49m(color\u001b[39m=\u001b[39;49my_color\u001b[39m+\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m,0.01)\u001b[39;49m\u001b[39m'\u001b[39;49m), row\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, col\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m))\n\u001b[1;32m     23\u001b[0m std \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     24\u001b[0m upperBound \u001b[39m=\u001b[39m history\u001b[39m.\u001b[39mmean(axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m+\u001b[39m std \u001b[39m*\u001b[39m history\u001b[39m.\u001b[39mstd(axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Code/AuthenticCursor/venvDev/lib/python3.8/site-packages/plotly/graph_objs/_scatter.py:3513\u001b[0m, in \u001b[0;36mScatter.__init__\u001b[0;34m(self, arg, alignmentgroup, cliponaxis, connectgaps, customdata, customdatasrc, dx, dy, error_x, error_y, fill, fillcolor, fillpattern, groupnorm, hoverinfo, hoverinfosrc, hoverlabel, hoveron, hovertemplate, hovertemplatesrc, hovertext, hovertextsrc, ids, idssrc, legendgroup, legendgrouptitle, legendrank, legendwidth, line, marker, meta, metasrc, mode, name, offsetgroup, opacity, orientation, selected, selectedpoints, showlegend, stackgaps, stackgroup, stream, text, textfont, textposition, textpositionsrc, textsrc, texttemplate, texttemplatesrc, uid, uirevision, unselected, visible, x, x0, xaxis, xcalendar, xhoverformat, xperiod, xperiod0, xperiodalignment, xsrc, y, y0, yaxis, ycalendar, yhoverformat, yperiod, yperiod0, yperiodalignment, ysrc, **kwargs)\u001b[0m\n\u001b[1;32m   3509\u001b[0m arg\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m   3511\u001b[0m \u001b[39m# Process unknown kwargs\u001b[39;00m\n\u001b[1;32m   3512\u001b[0m \u001b[39m# ----------------------\u001b[39;00m\n\u001b[0;32m-> 3513\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process_kwargs(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mdict\u001b[39;49m(arg, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs))\n\u001b[1;32m   3515\u001b[0m \u001b[39m# Reset skip_invalid\u001b[39;00m\n\u001b[1;32m   3516\u001b[0m \u001b[39m# ------------------\u001b[39;00m\n\u001b[1;32m   3517\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_skip_invalid \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Code/AuthenticCursor/venvDev/lib/python3.8/site-packages/plotly/basedatatypes.py:4368\u001b[0m, in \u001b[0;36mBasePlotlyType._process_kwargs\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m   4366\u001b[0m     \u001b[39mself\u001b[39m[k] \u001b[39m=\u001b[39m v\n\u001b[1;32m   4367\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_skip_invalid:\n\u001b[0;32m-> 4368\u001b[0m     \u001b[39mraise\u001b[39;00m err\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid property specified for object of type plotly.graph_objs.Scatter: 'row'\n\nDid you mean \"dx\"?\n\n    Valid properties:\n        alignmentgroup\n            Set several traces linked to the same position axis or\n            matching axes to the same alignmentgroup. This controls\n            whether bars compute their positional range dependently\n            or independently.\n        cliponaxis\n            Determines whether or not markers and text nodes are\n            clipped about the subplot axes. To show markers and\n            text nodes above axis lines and tick labels, make sure\n            to set `xaxis.layer` and `yaxis.layer` to *below\n            traces*.\n        connectgaps\n            Determines whether or not gaps (i.e. {nan} or missing\n            values) in the provided data arrays are connected.\n        customdata\n            Assigns extra data each datum. This may be useful when\n            listening to hover, click and selection events. Note\n            that, \"scatter\" traces also appends customdata items in\n            the markers DOM elements\n        customdatasrc\n            Sets the source reference on Chart Studio Cloud for\n            `customdata`.\n        dx\n            Sets the x coordinate step. See `x0` for more info.\n        dy\n            Sets the y coordinate step. See `y0` for more info.\n        error_x\n            :class:`plotly.graph_objects.scatter.ErrorX` instance\n            or dict with compatible properties\n        error_y\n            :class:`plotly.graph_objects.scatter.ErrorY` instance\n            or dict with compatible properties\n        fill\n            Sets the area to fill with a solid color. Defaults to\n            \"none\" unless this trace is stacked, then it gets\n            \"tonexty\" (\"tonextx\") if `orientation` is \"v\" (\"h\") Use\n            with `fillcolor` if not \"none\". \"tozerox\" and \"tozeroy\"\n            fill to x=0 and y=0 respectively. \"tonextx\" and\n            \"tonexty\" fill between the endpoints of this trace and\n            the endpoints of the trace before it, connecting those\n            endpoints with straight lines (to make a stacked area\n            graph); if there is no trace before it, they behave\n            like \"tozerox\" and \"tozeroy\". \"toself\" connects the\n            endpoints of the trace (or each segment of the trace if\n            it has gaps) into a closed shape. \"tonext\" fills the\n            space between two traces if one completely encloses the\n            other (eg consecutive contour lines), and behaves like\n            \"toself\" if there is no trace before it. \"tonext\"\n            should not be used if one trace does not enclose the\n            other. Traces in a `stackgroup` will only fill to (or\n            be filled to) other traces in the same group. With\n            multiple `stackgroup`s or some traces stacked and some\n            not, if fill-linked traces are not already consecutive,\n            the later ones will be pushed down in the drawing\n            order.\n        fillcolor\n            Sets the fill color. Defaults to a half-transparent\n            variant of the line color, marker color, or marker line\n            color, whichever is available.\n        fillpattern\n            Sets the pattern within the marker.\n        groupnorm\n            Only relevant when `stackgroup` is used, and only the\n            first `groupnorm` found in the `stackgroup` will be\n            used - including if `visible` is \"legendonly\" but not\n            if it is `false`. Sets the normalization for the sum of\n            this `stackgroup`. With \"fraction\", the value of each\n            trace at each location is divided by the sum of all\n            trace values at that location. \"percent\" is the same\n            but multiplied by 100 to show percentages. If there are\n            multiple subplots, or multiple `stackgroup`s on one\n            subplot, each will be normalized within its own set.\n        hoverinfo\n            Determines which trace information appear on hover. If\n            `none` or `skip` are set, no information is displayed\n            upon hovering. But, if `none` is set, click and hover\n            events are still fired.\n        hoverinfosrc\n            Sets the source reference on Chart Studio Cloud for\n            `hoverinfo`.\n        hoverlabel\n            :class:`plotly.graph_objects.scatter.Hoverlabel`\n            instance or dict with compatible properties\n        hoveron\n            Do the hover effects highlight individual points\n            (markers or line points) or do they highlight filled\n            regions? If the fill is \"toself\" or \"tonext\" and there\n            are no markers or text, then the default is \"fills\",\n            otherwise it is \"points\".\n        hovertemplate\n            Template string used for rendering the information that\n            appear on hover box. Note that this will override\n            `hoverinfo`. Variables are inserted using %{variable},\n            for example \"y: %{y}\" as well as %{xother}, {%_xother},\n            {%_xother_}, {%xother_}. When showing info for several\n            points, \"xother\" will be added to those with different\n            x positions from the first point. An underscore before\n            or after \"(x|y)other\" will add a space on that side,\n            only when this field is shown. Numbers are formatted\n            using d3-format's syntax %{variable:d3-format}, for\n            example \"Price: %{y:$.2f}\".\n            https://github.com/d3/d3-format/tree/v1.4.5#d3-format\n            for details on the formatting syntax. Dates are\n            formatted using d3-time-format's syntax\n            %{variable|d3-time-format}, for example \"Day:\n            %{2019-01-01|%A}\". https://github.com/d3/d3-time-\n            format/tree/v2.2.3#locale_format for details on the\n            date formatting syntax. The variables available in\n            `hovertemplate` are the ones emitted as event data\n            described at this link\n            https://plotly.com/javascript/plotlyjs-events/#event-\n            data. Additionally, every attributes that can be\n            specified per-point (the ones that are `arrayOk: true`)\n            are available.  Anything contained in tag `<extra>` is\n            displayed in the secondary box, for example\n            \"<extra>{fullData.name}</extra>\". To hide the secondary\n            box completely, use an empty tag `<extra></extra>`.\n        hovertemplatesrc\n            Sets the source reference on Chart Studio Cloud for\n            `hovertemplate`.\n        hovertext\n            Sets hover text elements associated with each (x,y)\n            pair. If a single string, the same string appears over\n            all the data points. If an array of string, the items\n            are mapped in order to the this trace's (x,y)\n            coordinates. To be seen, trace `hoverinfo` must contain\n            a \"text\" flag.\n        hovertextsrc\n            Sets the source reference on Chart Studio Cloud for\n            `hovertext`.\n        ids\n            Assigns id labels to each datum. These ids for object\n            constancy of data points during animation. Should be an\n            array of strings, not numbers or any other type.\n        idssrc\n            Sets the source reference on Chart Studio Cloud for\n            `ids`.\n        legendgroup\n            Sets the legend group for this trace. Traces part of\n            the same legend group hide/show at the same time when\n            toggling legend items.\n        legendgrouptitle\n            :class:`plotly.graph_objects.scatter.Legendgrouptitle`\n            instance or dict with compatible properties\n        legendrank\n            Sets the legend rank for this trace. Items and groups\n            with smaller ranks are presented on top/left side while\n            with `*reversed* `legend.traceorder` they are on\n            bottom/right side. The default legendrank is 1000, so\n            that you can use ranks less than 1000 to place certain\n            items before all unranked items, and ranks greater than\n            1000 to go after all unranked items.\n        legendwidth\n            Sets the width (in px or fraction) of the legend for\n            this trace.\n        line\n            :class:`plotly.graph_objects.scatter.Line` instance or\n            dict with compatible properties\n        marker\n            :class:`plotly.graph_objects.scatter.Marker` instance\n            or dict with compatible properties\n        meta\n            Assigns extra meta information associated with this\n            trace that can be used in various text attributes.\n            Attributes such as trace `name`, graph, axis and\n            colorbar `title.text`, annotation `text`\n            `rangeselector`, `updatemenues` and `sliders` `label`\n            text all support `meta`. To access the trace `meta`\n            values in an attribute in the same trace, simply use\n            `%{meta[i]}` where `i` is the index or key of the\n            `meta` item in question. To access trace `meta` in\n            layout attributes, use `%{data[n[.meta[i]}` where `i`\n            is the index or key of the `meta` and `n` is the trace\n            index.\n        metasrc\n            Sets the source reference on Chart Studio Cloud for\n            `meta`.\n        mode\n            Determines the drawing mode for this scatter trace. If\n            the provided `mode` includes \"text\" then the `text`\n            elements appear at the coordinates. Otherwise, the\n            `text` elements appear on hover. If there are less than\n            20 points and the trace is not stacked then the default\n            is \"lines+markers\". Otherwise, \"lines\".\n        name\n            Sets the trace name. The trace name appear as the\n            legend item and on hover.\n        offsetgroup\n            Set several traces linked to the same position axis or\n            matching axes to the same offsetgroup where bars of the\n            same position coordinate will line up.\n        opacity\n            Sets the opacity of the trace.\n        orientation\n            Only relevant in the following cases: 1. when\n            `scattermode` is set to \"group\". 2. when `stackgroup`\n            is used, and only the first `orientation` found in the\n            `stackgroup` will be used - including if `visible` is\n            \"legendonly\" but not if it is `false`. Sets the\n            stacking direction. With \"v\" (\"h\"), the y (x) values of\n            subsequent traces are added. Also affects the default\n            value of `fill`.\n        selected\n            :class:`plotly.graph_objects.scatter.Selected` instance\n            or dict with compatible properties\n        selectedpoints\n            Array containing integer indices of selected points.\n            Has an effect only for traces that support selections.\n            Note that an empty array means an empty selection where\n            the `unselected` are turned on for all points, whereas,\n            any other non-array values means no selection all where\n            the `selected` and `unselected` styles have no effect.\n        showlegend\n            Determines whether or not an item corresponding to this\n            trace is shown in the legend.\n        stackgaps\n            Only relevant when `stackgroup` is used, and only the\n            first `stackgaps` found in the `stackgroup` will be\n            used - including if `visible` is \"legendonly\" but not\n            if it is `false`. Determines how we handle locations at\n            which other traces in this group have data but this one\n            does not. With *infer zero* we insert a zero at these\n            locations. With \"interpolate\" we linearly interpolate\n            between existing values, and extrapolate a constant\n            beyond the existing values.\n        stackgroup\n            Set several scatter traces (on the same subplot) to the\n            same stackgroup in order to add their y values (or\n            their x values if `orientation` is \"h\"). If blank or\n            omitted this trace will not be stacked. Stacking also\n            turns `fill` on by default, using \"tonexty\" (\"tonextx\")\n            if `orientation` is \"h\" (\"v\") and sets the default\n            `mode` to \"lines\" irrespective of point count. You can\n            only stack on a numeric (linear or log) axis. Traces in\n            a `stackgroup` will only fill to (or be filled to)\n            other traces in the same group. With multiple\n            `stackgroup`s or some traces stacked and some not, if\n            fill-linked traces are not already consecutive, the\n            later ones will be pushed down in the drawing order.\n        stream\n            :class:`plotly.graph_objects.scatter.Stream` instance\n            or dict with compatible properties\n        text\n            Sets text elements associated with each (x,y) pair. If\n            a single string, the same string appears over all the\n            data points. If an array of string, the items are\n            mapped in order to the this trace's (x,y) coordinates.\n            If trace `hoverinfo` contains a \"text\" flag and\n            \"hovertext\" is not set, these elements will be seen in\n            the hover labels.\n        textfont\n            Sets the text font.\n        textposition\n            Sets the positions of the `text` elements with respects\n            to the (x,y) coordinates.\n        textpositionsrc\n            Sets the source reference on Chart Studio Cloud for\n            `textposition`.\n        textsrc\n            Sets the source reference on Chart Studio Cloud for\n            `text`.\n        texttemplate\n            Template string used for rendering the information text\n            that appear on points. Note that this will override\n            `textinfo`. Variables are inserted using %{variable},\n            for example \"y: %{y}\". Numbers are formatted using\n            d3-format's syntax %{variable:d3-format}, for example\n            \"Price: %{y:$.2f}\".\n            https://github.com/d3/d3-format/tree/v1.4.5#d3-format\n            for details on the formatting syntax. Dates are\n            formatted using d3-time-format's syntax\n            %{variable|d3-time-format}, for example \"Day:\n            %{2019-01-01|%A}\". https://github.com/d3/d3-time-\n            format/tree/v2.2.3#locale_format for details on the\n            date formatting syntax. Every attributes that can be\n            specified per-point (the ones that are `arrayOk: true`)\n            are available.\n        texttemplatesrc\n            Sets the source reference on Chart Studio Cloud for\n            `texttemplate`.\n        uid\n            Assign an id to this trace, Use this to provide object\n            constancy between traces during animations and\n            transitions.\n        uirevision\n            Controls persistence of some user-driven changes to the\n            trace: `constraintrange` in `parcoords` traces, as well\n            as some `editable: true` modifications such as `name`\n            and `colorbar.title`. Defaults to `layout.uirevision`.\n            Note that other user-driven trace attribute changes are\n            controlled by `layout` attributes: `trace.visible` is\n            controlled by `layout.legend.uirevision`,\n            `selectedpoints` is controlled by\n            `layout.selectionrevision`, and `colorbar.(x|y)`\n            (accessible with `config: {editable: true}`) is\n            controlled by `layout.editrevision`. Trace changes are\n            tracked by `uid`, which only falls back on trace index\n            if no `uid` is provided. So if your app can add/remove\n            traces before the end of the `data` array, such that\n            the same trace has a different index, you can still\n            preserve user-driven changes if you give each trace a\n            `uid` that stays with it as it moves.\n        unselected\n            :class:`plotly.graph_objects.scatter.Unselected`\n            instance or dict with compatible properties\n        visible\n            Determines whether or not this trace is visible. If\n            \"legendonly\", the trace is not drawn, but can appear as\n            a legend item (provided that the legend itself is\n            visible).\n        x\n            Sets the x coordinates.\n        x0\n            Alternate to `x`. Builds a linear space of x\n            coordinates. Use with `dx` where `x0` is the starting\n            coordinate and `dx` the step.\n        xaxis\n            Sets a reference between this trace's x coordinates and\n            a 2D cartesian x axis. If \"x\" (the default value), the\n            x coordinates refer to `layout.xaxis`. If \"x2\", the x\n            coordinates refer to `layout.xaxis2`, and so on.\n        xcalendar\n            Sets the calendar system to use with `x` date data.\n        xhoverformat\n            Sets the hover text formatting rulefor `x`  using d3\n            formatting mini-languages which are very similar to\n            those in Python. For numbers, see:\n            https://github.com/d3/d3-format/tree/v1.4.5#d3-format.\n            And for dates see: https://github.com/d3/d3-time-\n            format/tree/v2.2.3#locale_format. We add two items to\n            d3's date formatter: \"%h\" for half of the year as a\n            decimal number as well as \"%{n}f\" for fractional\n            seconds with n digits. For example, *2016-10-13\n            09:15:23.456* with tickformat \"%H~%M~%S.%2f\" would\n            display *09~15~23.46*By default the values are\n            formatted using `xaxis.hoverformat`.\n        xperiod\n            Only relevant when the axis `type` is \"date\". Sets the\n            period positioning in milliseconds or \"M<n>\" on the x\n            axis. Special values in the form of \"M<n>\" could be\n            used to declare the number of months. In this case `n`\n            must be a positive integer.\n        xperiod0\n            Only relevant when the axis `type` is \"date\". Sets the\n            base for period positioning in milliseconds or date\n            string on the x0 axis. When `x0period` is round number\n            of weeks, the `x0period0` by default would be on a\n            Sunday i.e. 2000-01-02, otherwise it would be at\n            2000-01-01.\n        xperiodalignment\n            Only relevant when the axis `type` is \"date\". Sets the\n            alignment of data points on the x axis.\n        xsrc\n            Sets the source reference on Chart Studio Cloud for\n            `x`.\n        y\n            Sets the y coordinates.\n        y0\n            Alternate to `y`. Builds a linear space of y\n            coordinates. Use with `dy` where `y0` is the starting\n            coordinate and `dy` the step.\n        yaxis\n            Sets a reference between this trace's y coordinates and\n            a 2D cartesian y axis. If \"y\" (the default value), the\n            y coordinates refer to `layout.yaxis`. If \"y2\", the y\n            coordinates refer to `layout.yaxis2`, and so on.\n        ycalendar\n            Sets the calendar system to use with `y` date data.\n        yhoverformat\n            Sets the hover text formatting rulefor `y`  using d3\n            formatting mini-languages which are very similar to\n            those in Python. For numbers, see:\n            https://github.com/d3/d3-format/tree/v1.4.5#d3-format.\n            And for dates see: https://github.com/d3/d3-time-\n            format/tree/v2.2.3#locale_format. We add two items to\n            d3's date formatter: \"%h\" for half of the year as a\n            decimal number as well as \"%{n}f\" for fractional\n            seconds with n digits. For example, *2016-10-13\n            09:15:23.456* with tickformat \"%H~%M~%S.%2f\" would\n            display *09~15~23.46*By default the values are\n            formatted using `yaxis.hoverformat`.\n        yperiod\n            Only relevant when the axis `type` is \"date\". Sets the\n            period positioning in milliseconds or \"M<n>\" on the y\n            axis. Special values in the form of \"M<n>\" could be\n            used to declare the number of months. In this case `n`\n            must be a positive integer.\n        yperiod0\n            Only relevant when the axis `type` is \"date\". Sets the\n            base for period positioning in milliseconds or date\n            string on the y0 axis. When `y0period` is round number\n            of weeks, the `y0period0` by default would be on a\n            Sunday i.e. 2000-01-02, otherwise it would be at\n            2000-01-01.\n        yperiodalignment\n            Only relevant when the axis `type` is \"date\". Sets the\n            alignment of data points on the y axis.\n        ysrc\n            Sets the source reference on Chart Studio Cloud for\n            `y`.\n        \nDid you mean \"dx\"?\n\nBad property path:\nrow\n^^^"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from plotly.subplots import make_subplots\n",
    "import numpy as np\n",
    "\n",
    "history = lr_scheduler_trial.replayValiditiesHistory.detach().cpu().numpy()\n",
    "show_replays = 1000\n",
    "sample_indexes = np.random.choice(history.shape[1], show_replays, replace=False)\n",
    "\n",
    "# Create a subplot with 2 rows\n",
    "fig = make_subplots(rows=4, cols=1, specs=[[{\"secondary_y\": False}],[{\"secondary_y\": True}],[{\"secondary_y\": True}],[{\"secondary_y\": True}]])\n",
    "\n",
    "for i_tracked_z in sample_indexes:\n",
    "    x = np.arange(history.shape[0])\n",
    "    y = history[:,i_tracked_z]\n",
    "    change = y - y[0]\n",
    "    y_color = ('rgba('+str(np.random.randint(0, high = 256))+','+\n",
    "                str(np.random.randint(0, high = 256))+','+\n",
    "                str(np.random.randint(0, high = 256)))\n",
    "    fig.add_trace(go.Scatter(x=x, y=change, mode='lines', \n",
    "        line=dict(width=0.5, dash='dot'),\n",
    "        showlegend=False), row=1, col=1)\n",
    "    fig.add_trace(go.Scatter(x=x, y=y, mode='lines', line=dict(color=y_color+',0.01)')), row=2, col=1)\n",
    "\n",
    "std = 1\n",
    "upperBound = history.mean(axis=1) + std * history.std(axis=1)\n",
    "lowerBound = history.mean(axis=1) - std * history.std(axis=1)\n",
    "x = np.arange(history.shape[0])\n",
    "y = history.mean(axis=1)\n",
    "\n",
    "fig.add_trace(go.Scatter(x=x, y=upperBound, mode='lines', line=dict(color='rgba(0,0,0,0)'), showlegend=False), row=2, col=1)\n",
    "fig.add_trace(go.Scatter(x=x, y=lowerBound, mode='lines', fill='tonexty', line=dict(color='rgba(0,0,0,0)'), showlegend=False), row=2, col=1)\n",
    "fig.add_trace(go.Scatter(x=x, y=y, mode='lines', line=dict(color='rgba(0,0,0,1)'), showlegend=False), row=2, col=1)\n",
    "\n",
    "x = np.arange(history.shape[0])\n",
    "y = lr_scheduler_trial.fakeValiditiesHistory.detach().cpu().numpy()\n",
    "# self.oldFake_validity = torch.zeros(self.replay_buffer_size, device=device, dtype=torch.float16)\n",
    "# self.oldReal_validity = torch.zeros(self.replay_buffer_size, device=device, dtype=torch.float16)\n",
    "fig.add_trace(go.Scatter(x=x, y=y, mode='lines', name=\"fake\"), row=2, col=1, secondary_y=True)\n",
    "y = lr_scheduler_trial.realValiditiesHistory.detach().cpu().numpy()\n",
    "fig.add_trace(go.Scatter(x=x, y=y, mode='lines', name=\"real\"), row=2, col=1, secondary_y=True)\n",
    "\n",
    "fig.add_trace(go.Scatter(x=np.arange(len(logsPerEpoch['accuracy'])), y=logsPerEpoch['accuracy'], mode='lines', name=f\"accuracy\"), row=3, col=1)\n",
    "# add discriminator learning rate on a separate axis\n",
    "fig.add_trace(go.Scatter(x=np.arange(len(logsPerEpoch['d_lr'])), y=logsPerEpoch['d_lr'], mode='lines', name=f\"d_lr\"), row=3, col=1, secondary_y=True)\n",
    "fig.add_trace(go.Scatter(x=np.arange(len(logsPerEpoch['g_lr'])), y=logsPerEpoch['g_lr'], mode='lines', name=f\"g_lr\"), row=3, col=1, secondary_y=True)\n",
    "\n",
    "d_loss = logsPerBatch['d_loss_base'].reshape(c.num_epochs, -1)\n",
    "g_loss = logsPerBatch['g_loss_base'].reshape(c.num_epochs, -1)\n",
    "\n",
    "fig.add_trace(go.Scatter(x=np.arange(len(d_loss.mean(axis=1))), y=d_loss.mean(axis=1), mode='lines', name=f\"d_loss\"), row=4, col=1)\n",
    "fig.add_trace(go.Scatter(x=np.arange(len(g_loss.mean(axis=1))), y=g_loss.mean(axis=1), mode='lines', name=f\"g_loss\"), row=4, col=1)\n",
    "# Update the y-axis of the second subplot\n",
    "fig.update_yaxes(range=[0, 1], row=3, col=1, secondary_y=False)\n",
    "fig.update_layout(width=1200, height=1000, xaxis_title='epochs', yaxis_title='change in validity')\n",
    "            # yaxis_title='Y Coordinate',\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standalone Discriminator model\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, 3, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Conv2d(64, 64, 3, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64*7*7, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "\n",
    "# Standalone Generator model\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128*7*7),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.BatchNorm1d(128*7*7),\n",
    "            # Reshape is not needed as view() function will be used in the forward pass\n",
    "            nn.ConvTranspose2d(128, 128, 4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ConvTranspose2d(128, 1, 4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(1, 1, 7, stride=1, padding=3),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, input):\n",
    "        input = input.view(-1, self.latent_dim)\n",
    "        return self.main(input).view(-1, 1, 28, 28)\n",
    "\n",
    "\n",
    "generator = Generator(c.latent_dim,1, NUM_CLASSES, 32).to(device)\n",
    "numImages = 10\n",
    "labels = torch.randint(0, 10, (numImages,), device=device)\n",
    "labels_one_hot = torch.zeros(numImages, 10, device=device).scatter_(1, labels.view(numImages, 1), 1)\n",
    "z = torch.randn(numImages, c.latent_dim, device=device)\n",
    "g_input = torch.cat((z, labels_one_hot), dim=1)\n",
    "g_input = g_input.view(numImages, c.latent_dim + NUM_CLASSES, 1, 1)\n",
    "fake_images = generator(g_input)\n",
    "fake_images.shape\n",
    "\n",
    "discriminator = Discriminator(1, NUM_CLASSES, 5, 3, 32).to(device)\n",
    "discriminator(fake_images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(c.latent_dim, 1, NUM_CLASSES, c.features_g)\n",
    "calculate_parameters(generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feed foward parameters, more than 10X\n",
    "29698704 # generator\n",
    "2510299 # discrimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.optim import lr_scheduler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create an optimizer\n",
    "initial_lr = 0.01\n",
    "min_lr = 0.0005\n",
    "optimizer = torch.optim.SGD([torch.randn(1, requires_grad=True)], lr= initial_lr)\n",
    "\n",
    "import math\n",
    "import torch\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "\n",
    "class WarmRestartScheduler(_LRScheduler):\n",
    "    def __init__(self, optimizer, T_0, T_mult=1, eta_max=0.1, last_epoch=-1):\n",
    "        self.T_0 = T_0\n",
    "        self.T_i = T_0\n",
    "        self.T_mult = T_mult\n",
    "        self.eta_max = eta_max\n",
    "        super(WarmRestartScheduler, self).__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        if self.last_epoch > self.T_i:\n",
    "            self.last_epoch = 0\n",
    "            self.T_i *= self.T_mult\n",
    "        return [self.eta_max * (self.last_epoch / self.T_i) for base_lr in self.base_lrs]\n",
    "\n",
    "    def _reset(self):\n",
    "        return WarmRestartScheduler(self.optimizer, self.T_0, self.T_mult, self.eta_max)\n",
    "\n",
    "\n",
    "class SineScheduler(torch.optim.lr_scheduler._LRScheduler):\n",
    "    def __init__(self, optimizer, T_0, T_mult=1, eta_min=0.01, eta_max=0.1, last_epoch=-1):\n",
    "        self.T_0 = T_0\n",
    "        self.T_mult = T_mult\n",
    "        self.eta_min = eta_min\n",
    "        self.eta_max = eta_max\n",
    "        super(SineScheduler, self).__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        return [self.eta_min + (self.eta_max - self.eta_min) * (1 + math.sin(2 * math.pi * self.last_epoch / self.T_0)) / 2\n",
    "                for base_lr in self.base_lrs]\n",
    "\n",
    "# Define the number of epochs\n",
    "num_epochs = 100\n",
    "cycles = 4\n",
    "# Learning rate schedulers\n",
    "# cosineAnnealingWarmRestarts = lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=int(c.num_epochs/cycles), T_mult=1, eta_min=min_lr)\n",
    "schedulers = {\n",
    "    # \"LambdaLR\": lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda epoch: 0.95 ** epoch),\n",
    "    # \"MultiplicativeLR\": lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lambda epoch: 0.95),\n",
    "    # \"StepLR\": lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1),\n",
    "    # \"MultiStepLR\": lr_scheduler.MultiStepLR(optimizer, milestones=[30, 80], gamma=0.1),\n",
    "    # \"ConstantLR\": lr_scheduler.ConstantLR(optimizer),\n",
    "    # \"LinearLR\" : lr_scheduler.LinearLR(optimizer),\n",
    "    # \"ExponentialLR\": lr_scheduler.ExponentialLR(optimizer, gamma=0.1),\n",
    "    # \"PolynomialLR\": lr_scheduler.PolynomialLR(optimizer,total_iters=4, power=1.0),\n",
    "    # \"CosineAnnealingLR\": lr_scheduler.CosineAnnealingLR(optimizer, T_max=50, eta_min=0),\n",
    "    # \"ChainedScheduler\" : lr_scheduler.ChainedScheduler([lr_scheduler.ConstantLR(optimizer, total_iters=10), cosineAnnealingWarmRestarts]),\n",
    "    # \"SequentialLR\": lr_scheduler.SequentialLR(optimizer, schedulers=[lr_scheduler.ConstantLR(optimizer, factor=0.1, total_iters=2), lr_scheduler.ExponentialLR(optimizer, gamma=0.9)], milestones=[2]),\n",
    "    # \"ReduceLROnPlateau\": lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10),\n",
    "    # \"CyclicLR\": lr_scheduler.CyclicLR(optimizer, base_lr=0.01, max_lr=1, step_size_up=5, mode='triangular2'),\n",
    "    # \"OneCycleLR\": lr_scheduler.OneCycleLR(optimizer, max_lr=1, total_steps=num_epochs),\n",
    "    # \"CosineAnnealingWarmRestarts\": lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=int(num_epochs/cycles), T_mult=1, eta_min=min_lr),\n",
    "    # \"WarmRestartScheduler\" : WarmRestartScheduler(optimizer, T_0=int(num_epochs/cycles), T_mult=1, eta_max=initial_lr),\n",
    "    \"SineScheduler\" : SineScheduler(optimizer, T_0=int(num_epochs/2), T_mult=1, eta_max=initial_lr, eta_min=min_lr, last_epoch=-1)\n",
    "}\n",
    "\n",
    "\n",
    "# Create a plot for each scheduler\n",
    "for name, scheduler in schedulers.items():\n",
    "    lrs = []\n",
    "    for epoch in range(num_epochs):\n",
    "        optimizer.step()\n",
    "        lrs.append(optimizer.param_groups[0][\"lr\"])\n",
    "        if name != \"ReduceLROnPlateau\":\n",
    "            scheduler.step()\n",
    "        else:\n",
    "            scheduler.step(epoch)  # Assume loss is decreasing with epoch for this example\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(lrs)\n",
    "    plt.title(name)\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvDev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
