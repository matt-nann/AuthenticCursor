{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mnann/Documents/Code/AuthenticCursor/venvDev/lib/python3.8/site-packages/torch/amp/autocast_mode.py:204: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:gwhevlb4) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dry-haze-76</strong> at: <a href='https://wandb.ai/mnann/mnist-gan/runs/gwhevlb4' target=\"_blank\">https://wandb.ai/mnann/mnist-gan/runs/gwhevlb4</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230615_225234-gwhevlb4/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:gwhevlb4). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/mnann/Documents/Code/AuthenticCursor/wandb/run-20230615_225403-r014x2e3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mnann/mnist-gan/runs/r014x2e3' target=\"_blank\">logical-dream-77</a></strong> to <a href='https://wandb.ai/mnann/mnist-gan' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mnann/mnist-gan' target=\"_blank\">https://wandb.ai/mnann/mnist-gan</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mnann/mnist-gan/runs/r014x2e3' target=\"_blank\">https://wandb.ai/mnann/mnist-gan/runs/r014x2e3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mnann/Documents/Code/AuthenticCursor/venvDev/lib/python3.8/site-packages/torch/cuda/amp/grad_scaler.py:120: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fake_images:  torch.Size([512, 1, 28, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mnann/Documents/Code/AuthenticCursor/venvDev/lib/python3.8/site-packages/torch/amp/autocast_mode.py:204: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fake_images:  torch.Size([512, 1, 28, 28])\n",
      "fake_images:  torch.Size([512, 1, 28, 28])\n",
      "fake_images:  torch.Size([512, 1, 28, 28])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[72], line 449\u001b[0m\n\u001b[1;32m    447\u001b[0m g_input \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((z, labels_one_hot), dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    448\u001b[0m g_input \u001b[39m=\u001b[39m g_input\u001b[39m.\u001b[39mview(_batch_size, c\u001b[39m.\u001b[39mlatent_dim \u001b[39m+\u001b[39m NUM_CLASSES, \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[0;32m--> 449\u001b[0m fake_images \u001b[39m=\u001b[39m generator(g_input)\n\u001b[1;32m    450\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mfake_images: \u001b[39m\u001b[39m\"\u001b[39m, fake_images\u001b[39m.\u001b[39mshape)\n\u001b[1;32m    451\u001b[0m fake_validities, d_fakeClass \u001b[39m=\u001b[39m discriminator(fake_images)\n",
      "File \u001b[0;32m~/Documents/Code/AuthenticCursor/venvDev/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/Code/AuthenticCursor/venvDev/lib/python3.8/site-packages/torch/amp/autocast_mode.py:14\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     12\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_autocast\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     13\u001b[0m     \u001b[39mwith\u001b[39;00m autocast_instance:\n\u001b[0;32m---> 14\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "Cell \u001b[0;32mIn[72], line 99\u001b[0m, in \u001b[0;36mGenerator.forward\u001b[0;34m(self, noise)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[39m@autocast\u001b[39m() \u001b[39m# automatically applies precisions to different operations to speed up calculations\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, noise):\n\u001b[0;32m---> 99\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgen(noise)\n",
      "File \u001b[0;32m~/Documents/Code/AuthenticCursor/venvDev/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/Code/AuthenticCursor/venvDev/lib/python3.8/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/Documents/Code/AuthenticCursor/venvDev/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/Code/AuthenticCursor/venvDev/lib/python3.8/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/Documents/Code/AuthenticCursor/venvDev/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/Code/AuthenticCursor/venvDev/lib/python3.8/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/Documents/Code/AuthenticCursor/venvDev/lib/python3.8/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    %pip install wandb\n",
    "    %pip install --upgrade \"kaleido==0.1.*\"\n",
    "    import kaleido\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.utils.data as data_utils\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "import torchvision.utils as vutils\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from io import BytesIO\n",
    "import plotly.io as pio\n",
    "import tempfile\n",
    "from PIL import Image\n",
    "import io\n",
    "import wandb\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import time\n",
    "from enum import Enum\n",
    "import plotly.express as px\n",
    "\n",
    "import dataclasses\n",
    "from typing import Any, Dict, List, Optional, Tuple, Union\n",
    "from dataclasses import dataclass\n",
    "\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "class TempFileContext:\n",
    "    def __enter__(self):\n",
    "        self.tmp_file = tempfile.NamedTemporaryFile(suffix=\".jpeg\", delete=False)\n",
    "        self.tmp_filename = self.tmp_file.name\n",
    "        return self.tmp_filename\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        self.tmp_file.close()\n",
    "        os.remove(self.tmp_filename)\n",
    "\n",
    "# As per the DCGAN paper: All the weights are initialized from a zero centered normal distribution with standard deviation 0.02\n",
    "def initialize_weights(model):\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.BatchNorm2d)):\n",
    "            nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, channels_noise, channels_img, num_classes, features_g):\n",
    "        \"\"\"\n",
    "        channels_noise: The size of the input noise vector. This noise vector is a random input from which the generator begins the generation of a new sample.\n",
    "        channels_img: The number of output channels of the generator. This will typically be 1 for grayscale images or 3 for color (RGB) images.\n",
    "        num_classes: The number of distinct classes or labels that the generator should generate images for. This is used to form the one-hot vector of class labels, which is concatenated to the noise vector to provide the generator with information about the class of image to generate.\n",
    "        \"\"\"\n",
    "        # Conv2d formula => output_size = (input_size - 1) * stride - 2 * padding + kernel_size\n",
    "        super(Generator, self).__init__()\n",
    "        self.gen = nn.Sequential(\n",
    "            self.gen_block(channels_noise + num_classes, features_g * 4, kernel_size=7, stride=1, padding=0),  # output: (features_g*4) x 7 x 7 # Append class labels to input noise.\n",
    "            nn.Dropout(p=0.05),\n",
    "            self._block(features_g * 4, features_g * 2, kernel_size=3, stride=1, padding=1),  # output: (features_g*2) x 6 x 6\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear'),  # output: (features_g*2) x 14 x 14\n",
    "            nn.Dropout(p=0.05),\n",
    "            self._block(features_g * 2, features_g, kernel_size=3, stride=1, padding=1),  # output: features_g x 14 x 14\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear'),  # output: features_g x 28 x 28\n",
    "            nn.Dropout(p=0.05),\n",
    "            self._block(features_g, channels_img, kernel_size=5, stride=1, padding=2),  # output: channels_img x 28 x 28\n",
    "            nn.Sigmoid(), # normalize inputs to [0, 1]\n",
    "        )\n",
    "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "    def gen_block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size,\n",
    "                stride,\n",
    "                padding,\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "    @autocast() # automatically applies precisions to different operations to speed up calculations\n",
    "    def forward(self, noise):\n",
    "        return self.gen(noise)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, channels_img, num_classes, num_kernels, kernel_dim, filters):\n",
    "        \"\"\"\n",
    "        channels_img: The number of input channels to the discriminator, corresponding to the number of channels in the images to be classified.\n",
    "        features_d: This is the base size of the feature maps in the discriminator. The number of neurons or nodes in each layer of the discriminator is a multiple of this base size.\n",
    "        num_classes: The number of distinct classes that the discriminator should be able to distinguish between. This is used to form the softmax output layer of the discriminator, which outputs a class probability distribution.\n",
    "        num_kernels and kernel_dim: These are parameters for the minibatch discrimination layer. The minibatch discrimination layer is designed to make the discriminator sensitive to the variety of samples within a minibatch, to encourage the generator to generate a variety of different samples. num_kernels is the number of unique patterns the layer can learn to identify, and kernel_dim is the size of these learned patterns.\n",
    "        \"\"\"\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(channels_img, filters, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(p=0.05),\n",
    "            self._block(filters, filters*2, 4, 2, 1),\n",
    "            nn.Dropout(p=0.05),\n",
    "        )\n",
    "        self.mbd = MinibatchDiscrimination(filters*2*7*7, num_kernels, kernel_dim)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(filters*2*7*7 + num_kernels, filters*8),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(filters*8, 1 + num_classes),\n",
    "        )\n",
    "\n",
    "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "    @autocast()\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.mbd(x)\n",
    "        out = self.fc(x)\n",
    "        return out[:, 0], nn.functional.softmax(out[:, 1:], dim=1)\n",
    "\n",
    "class MinibatchDiscrimination(nn.Module):\n",
    "    def __init__(self, input_features, num_kernels, kernel_dim):\n",
    "        super(MinibatchDiscrimination, self).__init__()\n",
    "        self.input_features = input_features\n",
    "        self.num_kernels = num_kernels\n",
    "        self.kernel_dim = kernel_dim\n",
    "        self.T = nn.Parameter(torch.randn(input_features, num_kernels * kernel_dim))\n",
    "    def forward(self, x):\n",
    "        M = torch.matmul(x, self.T).view(-1, self.num_kernels, self.kernel_dim)\n",
    "        diffs = M.unsqueeze(0) - M.transpose(0, 1).unsqueeze(2)\n",
    "        abs_diffs = torch.sum(torch.abs(diffs), dim=2)\n",
    "        minibatch_features = torch.sum(torch.exp(-abs_diffs), dim=2).T\n",
    "        return torch.cat((x, minibatch_features), dim=1)\n",
    "\n",
    "class LR_Metric(Enum):\n",
    "    VALIDITY = 1\n",
    "    AGE = 2\n",
    "    \n",
    "class CustomDataLoader:\n",
    "    def __init__(self, X, Y, batch_size, device):\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "        self.data = X.float().to(self.device)\n",
    "        self.targets = Y.to(self.device)\n",
    "        self.num_samples = len(self.data)\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.indices = torch.randperm(self.num_samples, device=self.device)\n",
    "        self.idx = 0\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.idx >= self.num_samples:\n",
    "            raise StopIteration\n",
    "\n",
    "        indices = self.indices[self.idx:self.idx+self.batch_size]\n",
    "        batch_data = self.data[indices]\n",
    "        batch_targets = self.targets[indices]\n",
    "\n",
    "        self.idx += self.batch_size\n",
    "\n",
    "        return batch_data, batch_targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return (self.num_samples + self.batch_size - 1) // self.batch_size\n",
    "\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize((0.1307,), (0.3081,))\n",
    "# ])\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,)),\n",
    "])\n",
    "train_data = MNIST(root='data/MNIST',train=True,download=True,transform=transform)\n",
    "normalized_train_data = train_data.data.float() / 255\n",
    "test_data = MNIST(root='data/MNIST', train=False, download=True, transform=transform)\n",
    "normalized_test_data = test_data.data.float() / 255\n",
    "# Hyperparameters\n",
    "image_size = 28 * 28\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    latent_dim: int = 100\n",
    "    batch_size: int = 256 * 2\n",
    "    num_epochs: int = 100\n",
    "    num_kernels: int = 10\n",
    "    kernel_dim: int = 3\n",
    "    d_learning_rate: float = 0.0002\n",
    "    g_learning_rate: float = 0.00001\n",
    "    lr_restarts: int = 5\n",
    "    min_lr: float = 1e-10\n",
    "    lambda_class: int = 1\n",
    "    replay_buffer_size: int = 1000\n",
    "    features_g: int = 256\n",
    "    features_d: int = 64\n",
    "    logEnd: bool = True\n",
    "\n",
    "c = Config(logEnd=False)\n",
    "preppedConfig = {}\n",
    "for k, v in dataclasses.asdict(c).items():\n",
    "    if dataclasses.is_dataclass(v):\n",
    "        preppedConfig[k] = dataclasses.asdict(v)\n",
    "    else:\n",
    "        preppedConfig[k] = v\n",
    "wandb.init(project=\"mnist-gan\", config=c)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    # Because the performance of cuDNN algorithms to compute the convolution of different kernel sizes varies, \n",
    "    # the auto-tuner can run a benchmark to find the best algorithm (current algorithms are these, these, and these). \n",
    "    # It’s recommended to use turn on the setting when your input size doesn’t change often. If the input size changes often, \n",
    "    # the auto-tuner needs to benchmark too frequently, which might hurt the performance.\n",
    "\n",
    "train_loader = CustomDataLoader(normalized_train_data, train_data.targets, batch_size=c.batch_size, device=device)\n",
    "test_loader = CustomDataLoader(normalized_test_data, test_data.targets, batch_size=c.batch_size, device=device)\n",
    "\n",
    "# logging every epoch\n",
    "t_age = torch.zeros(c.num_epochs).to(device)\n",
    "t_curGap = torch.zeros(c.num_epochs).to(device)\n",
    "t_oldGap = torch.zeros(c.num_epochs).to(device)\n",
    "t_oldScore = torch.zeros(c.num_epochs).to(device)\n",
    "t_replayScore = torch.zeros(c.num_epochs).to(device)\n",
    "t_replayValidity = torch.zeros(c.num_epochs).to(device)\n",
    "t_oldValidity = torch.zeros(c.num_epochs).to(device)\n",
    "t_accuracy = torch.zeros(c.num_epochs).to(device)\n",
    "t_d_lr = torch.zeros(c.num_epochs).to(device)\n",
    "\n",
    "generator = Generator(c.latent_dim, 1, NUM_CLASSES, c.features_g).to(device)\n",
    "initialize_weights(generator)\n",
    "discriminator = Discriminator(1, NUM_CLASSES, c.num_kernels, c.kernel_dim, c.features_d).to(device)\n",
    "initialize_weights(discriminator)\n",
    "\n",
    "# Optimizers\n",
    "generator_optimizer = optim.Adam(generator.parameters(), lr=c.g_learning_rate, betas=(0.5, 0.9))\n",
    "discriminator_optimizer = optim.Adam(discriminator.parameters(), lr=c.d_learning_rate, betas=(0.5, 0.9))\n",
    "\n",
    "train_batches = len(train_loader)\n",
    "\n",
    "# logging every batch\n",
    "t_real_validity = torch.zeros(c.num_epochs * train_batches, device=device)\n",
    "t_fake_validity = torch.zeros(c.num_epochs * train_batches, device=device)\n",
    "t_d_fakeClassLoss = torch.zeros(c.num_epochs * train_batches, device=device)\n",
    "t_d_realClassLoss = torch.zeros(c.num_epochs * train_batches, device=device)\n",
    "t_d_fakeAccuracy = torch.zeros(c.num_epochs * train_batches, device=device)\n",
    "t_d_realAccuracy = torch.zeros(c.num_epochs * train_batches, device=device)\n",
    "t_d_loss_base = torch.zeros(c.num_epochs * train_batches, device=device)\n",
    "t_g_loss_base = torch.zeros(c.num_epochs * train_batches, device=device)\n",
    "t_g_fakeClassLoss = torch.zeros(c.num_epochs * train_batches, device=device)\n",
    "# logging is varied\n",
    "t_images = []\n",
    "\n",
    "class LearningRateScheduler:\n",
    "    def __init__(self, initial_lr, replay_buffer_size, total_batches, batch_size, METRIC=LR_Metric.VALIDITY):\n",
    "        self.initial_lr = initial_lr\n",
    "        self.replay_buffer_size = replay_buffer_size\n",
    "        self.total_batches = total_batches\n",
    "        self.batch_size = batch_size\n",
    "        self.METRIC = METRIC\n",
    "        self.samplesPerBatch = int(np.ceil(replay_buffer_size / total_batches))\n",
    "        self.filledIndex = 0\n",
    "        # self.oldFake_validity = torch.zeros(self.replay_buffer_size, device=device, dtype=torch.float16)\n",
    "        # self.oldReal_validity = torch.zeros(self.replay_buffer_size, device=device, dtype=torch.float16)\n",
    "        # self.oldFake_validities = torch.zeros(self.replay_buffer_size, device=device, dtype=torch.float16)\n",
    "        self.oldFake_validity = torch.zeros(self.replay_buffer_size, device=device) #, dtype=torch.float16)\n",
    "        self.oldReal_validity = torch.zeros(self.replay_buffer_size, device=device) #, dtype=torch.float16)\n",
    "        self.oldFake_validities = torch.zeros(self.replay_buffer_size, device=device) #, dtype=torch.float16)\n",
    "        self.z_replay = torch.zeros(self.replay_buffer_size, c.latent_dim + NUM_CLASSES, device=device)\n",
    "        self.age = torch.zeros(self.replay_buffer_size, device=device)\n",
    "        self.kickTopPercent = 0.25\n",
    "        self.openIndexes = torch.ones(self.replay_buffer_size, device=device)\n",
    "        self.real_validity_total = torch.zeros(1, device=device)\n",
    "        self.fake_validity_total = torch.zeros(1, device=device)\n",
    "        self.numSamples = 0\n",
    "\n",
    "    def fillReplayBuffer(self, real_validity, real_validities, fake_validity, fake_validities, z):\n",
    "        \"\"\"\n",
    "        samples (amouting to replay_buffer_size) will be evenly provided by all batches to fill the replay buffer in 1 epoch\n",
    "        \"\"\"\n",
    "        with torch.no_grad() and torch.cuda.amp.autocast():\n",
    "            self.real_validity_total += real_validities.sum()\n",
    "            self.fake_validity_total += fake_validities.sum()\n",
    "            self.numSamples += len(z)\n",
    "            openings = (self.openIndexes > 0).sum().item()\n",
    "            numSamples = len(z)  \n",
    "            if self.filledIndex < self.replay_buffer_size: \n",
    "                # start filling the buffer front to back, only fill self.samplesPerBatch to prevent, the early batches from dominanting the replay buffer\n",
    "                remaining = self.replay_buffer_size - self.filledIndex\n",
    "                numSelected = np.min([remaining, numSamples, self.samplesPerBatch])\n",
    "                selected = np.random.choice(numSamples, numSelected, replace=False)\n",
    "                indexes = torch.arange(self.filledIndex, self.filledIndex + len(selected))\n",
    "                self.filledIndex += len(selected)\n",
    "            elif openings:\n",
    "                # randomly select samples to fill the openIndexes in the replay buffer\n",
    "                indexes = torch.nonzero(self.openIndexes).squeeze()\n",
    "                numSelected = np.min([openings, numSamples, self.samplesPerBatch])\n",
    "                selected = np.random.choice(numSamples, numSelected, replace=False)\n",
    "                indexes = np.random.choice(indexes.numel(), numSelected, replace=False)\n",
    "            else:\n",
    "                return\n",
    "            self.oldFake_validity[indexes] = fake_validity.repeat(len(indexes))\n",
    "            self.oldReal_validity[indexes] = real_validity.repeat(len(indexes))\n",
    "            self.oldFake_validities[indexes] = fake_validities[selected].squeeze()\n",
    "            self.z_replay[indexes,:] = z[selected]\n",
    "            self.age[indexes] = 0\n",
    "            self.openIndexes[indexes] = 0\n",
    "\n",
    "    def plotReplayValidities(self):\n",
    "        i_replays = (self.openIndexes == 0).nonzero().squeeze()\n",
    "        fig = go.Figure()\n",
    "        fig.add_trace(go.Histogram(x=self.oldReal_validity[i_replays].cpu().numpy(), name=\"real\"))\n",
    "        fig.add_trace(go.Histogram(x=self.oldFake_validity[i_replays].cpu().numpy(), name=\"fake\"))\n",
    "        fig.update_layout(barmode='overlay', title=\"saved validity scores histogram\")\n",
    "        fig.show()\n",
    "\n",
    "    def update_learning_rate(self, epoch, d, g):\n",
    "        with torch.no_grad():\n",
    "            i_replays = (self.openIndexes == 0).nonzero().squeeze()\n",
    "            z_replay = self.z_replay[i_replays]\n",
    "            z_replay = z_replay.view(len(z_replay), c.latent_dim + NUM_CLASSES, 1, 1)\n",
    "            fake_replay = g(z_replay)\n",
    "            replayFake_validities, _ = d(fake_replay)\n",
    "            replayFake_validities = replayFake_validities.squeeze()\n",
    "    \n",
    "            # if gaps are negatives then discriminator then fake images are getting higher validity scores than real ones\n",
    "            oldGap = (self.oldReal_validity[i_replays] - self.oldFake_validity[i_replays]).mean()\n",
    "            curGap = self.real_validity_total / self.numSamples - self.fake_validity_total / self.numSamples\n",
    "            # positive - smaller positive\n",
    "\n",
    "            replayScores = replayFake_validities - curGap.repeat(len(i_replays))\n",
    "            oldScores = self.oldFake_validities[i_replays] - oldGap.repeat(len(i_replays))\n",
    "\n",
    "            # logging\n",
    "            t_age[epoch] = self.age[i_replays].mean()\n",
    "            t_curGap[epoch] = curGap\n",
    "            t_oldGap[epoch] = oldGap\n",
    "            t_replayScore[epoch] = replayScores.mean()\n",
    "            t_oldScore[epoch] = oldScores.mean()\n",
    "            t_replayValidity[epoch] = replayFake_validities.mean()\n",
    "            t_oldValidity[epoch] = self.oldFake_validities[i_replays].mean()\n",
    "            # try:\n",
    "            #     wandb.log({\"curGap\": curGap, \"oldGap\": oldGap, \"replayScore\": replayScores.mean().item(), \"oldScore\": oldScores.mean().item(), 'avgAge': self.age[i_replays].mean().item()})\n",
    "            # except:\n",
    "            #     ...\n",
    "            if self.METRIC.value == LR_Metric.VALIDITY.value:\n",
    "                metric = replayFake_validities.squeeze()\n",
    "            elif self.METRIC.value == LR_Metric.AGE.value:\n",
    "                metric = self.age[i_replays].squeeze()\n",
    "                raise NotImplementedError(\"needs to be adjusted\")\n",
    "            else:\n",
    "                raise Exception(\"Invalid metric\")\n",
    "            # Kick out top 10% of the replay buffer based on replayScores scores\n",
    "            # lowest to highest, drop the highest\n",
    "            i_highestMetric = torch.argsort(metric)[-int(np.ceil(self.kickTopPercent * self.replay_buffer_size)):]\n",
    "            self.openIndexes[i_highestMetric] = 1\n",
    "\n",
    "            # kick out first half for testing\n",
    "            # self.openIndexes[:int(self.replay_buffer_size/2)] = torch.ones(int(self.replay_buffer_size/2)).to(device)\n",
    "\n",
    "            self.age += 1\n",
    "\n",
    "lr_scheduler_trial = LearningRateScheduler(initial_lr=0.001, replay_buffer_size=c.replay_buffer_size, total_batches=train_batches, batch_size=c.batch_size)\n",
    "d_lr_scheduler = lr_scheduler.CosineAnnealingWarmRestarts(discriminator_optimizer, T_0=int(c.num_epochs/c.lr_restarts), T_mult=1, eta_min=c.min_lr)\n",
    "\n",
    "# d_lr_scheduler = lr_scheduler.\n",
    "classCriterion = nn.CrossEntropyLoss()\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "def createGridFakeImages(epoch=0, cubeSide=4, show=False, step=None, log=True):\n",
    "    fig = make_subplots(rows=1, cols=2,\n",
    "                        horizontal_spacing=0.01, \n",
    "                        shared_yaxes=True)\n",
    "    numImages = torch.tensor([cubeSide**2], device=device)\n",
    "    # Generate and plot fake images with labels\n",
    "    labels = torch.randint(0, 10, (numImages,), device=device)\n",
    "    labels_one_hot = torch.zeros(numImages, 10, device=device).scatter_(1, labels.view(numImages, 1), 1)\n",
    "    with torch.no_grad():\n",
    "        z = torch.randn(numImages, c.latent_dim, device=device)\n",
    "        g_input = torch.cat((z, labels_one_hot), dim=1)\n",
    "        g_input = g_input.view(numImages, c.latent_dim + NUM_CLASSES, 1, 1)\n",
    "        fake_images = generator(g_input)\n",
    "        # fake_validities, d_fakeClass = discriminator(fake_images)\n",
    "        # g_fakeClassLoss = classCriterion(d_fakeClass, labels_one_hot)\n",
    "    fig = make_subplots(rows=cubeSide, cols=cubeSide, \n",
    "                        horizontal_spacing = 0.025,\n",
    "                        vertical_spacing = 0.04,\n",
    "                        subplot_titles=[str(label.item()) for label in labels])\n",
    "    fake_images = fake_images.squeeze().cpu().numpy()\n",
    "    for i in range(numImages):\n",
    "        row = int(i/cubeSide) + 1\n",
    "        col = int(i%cubeSide) + 1\n",
    "        imageFlipped = np.flip(fake_images[i], 0)\n",
    "        fig.add_trace(go.Heatmap(z=imageFlipped, \n",
    "                                colorscale='Greys',), row=row, col=col)\n",
    "    fig.update_layout(title_text=\"Generated Images epoch: \" + str(epoch), \n",
    "                    margin=dict(l=0, r=0, t=60, b=0),\n",
    "                    height=800, width=800, showlegend=False)\n",
    "    fig.update_traces(showscale=False)\n",
    "    fig.update_xaxes(visible=False)\n",
    "    fig.update_yaxes(visible=False)\n",
    "    if show:\n",
    "        fig.show()\n",
    "    if log:\n",
    "        if step is None:\n",
    "            raise Exception(\"step must be provided when logging an image\")\n",
    "        # Convert the figure to a JPEG image and log using wandb\n",
    "        image_bytes = pio.to_image(fig, format='jpeg')\n",
    "        t_images.append((image_bytes, step))\n",
    "\n",
    "scaler = GradScaler()\n",
    "# GradScaler with PyTorch's autocast prevents gradient underflow in mixed precision training.\n",
    "# It achieves this by scaling up the loss before backward pass to keep float16 gradients from vanishing.\n",
    "# After gradients are computed, they are scaled back before the optimizer updates the model weights.\n",
    "\n",
    "for epoch in range(c.num_epochs):\n",
    "    correct, total = 0, 0\n",
    "    for i, (real_images, labels) in enumerate(train_loader):\n",
    "        # s_time = time.time()\n",
    "        # print(f\"Epoch {epoch}/{num_epochs} Batch {i}/{total_steps}\")\n",
    "        _batch_size = real_images.size(0)\n",
    "        real_images = real_images.unsqueeze(1)\n",
    "        labels_one_hot = torch.zeros(_batch_size, NUM_CLASSES, device=device).scatter_(1, labels.view(_batch_size, 1), 1).to(device)\n",
    "\n",
    "        # train generator\n",
    "        # Setting gradients to zeroes by model.zero_grad() or optimizer.zero_grad() would execute memset for all parameters and update gradients with reading and writing operations. \n",
    "        # However, setting the gradients as None would not execute memset and would update gradients with only writing operations.\n",
    "        generator_optimizer.zero_grad(set_to_none=True)\n",
    "        z = torch.randn(_batch_size, c.latent_dim).to(device)\n",
    "        g_input = torch.cat((z, labels_one_hot), dim=1)\n",
    "        g_input = g_input.view(_batch_size, c.latent_dim + NUM_CLASSES, 1, 1)\n",
    "        fake_images = generator(g_input)\n",
    "        fake_validities, d_fakeClass = discriminator(fake_images)\n",
    "        # g_loss should minimize the difference in predicting classes among the same classes\n",
    "        g_fakeClassLoss = classCriterion(d_fakeClass, labels_one_hot)\n",
    "        # WGAN-GP\n",
    "        # g_loss = -torch.mean(fake_validities) + g_fakeClassLoss * lambda_class\n",
    "        d_logits_gen = fake_validities.view(-1)\n",
    "        # LSGAN\n",
    "        g_loss_base = criterion(d_logits_gen, torch.ones_like(d_logits_gen))\n",
    "        g_loss = g_loss_base + g_fakeClassLoss * c.lambda_class\n",
    "        # g_loss.backward()\n",
    "        # generator_optimizer.step()\n",
    "        scaler.scale(g_loss).backward()\n",
    "        scaler.step(generator_optimizer)\n",
    "        \n",
    "        # train discriminator\n",
    "        discriminator_optimizer.zero_grad(set_to_none=True)\n",
    "        real_validities, d_realClass = discriminator(real_images)\n",
    "        fake_validities, d_fakeClass = discriminator(fake_images.clone().detach())\n",
    "\n",
    "        loss_disc_real = criterion(real_validities, torch.ones_like(real_validities))\n",
    "        loss_disc_fake = criterion(fake_validities, -torch.ones_like(fake_validities)) # modified to -1 from normal LSGAN 0 target\n",
    "        # LSGAN\n",
    "        d_loss_base = (loss_disc_real + loss_disc_fake) / 2\n",
    "        \n",
    "        # gradient_penalty = compute_gradient_penalty(discriminator, real_images.data, fake_images.data)\n",
    "        # d_loss = -torch.mean(real_validities) + torch.mean(fake_validities) + lambda_gp * gradient_penalty\n",
    "        d_fakeClassLoss = classCriterion(d_fakeClass, labels_one_hot)\n",
    "        d_fakeAccuracy = (d_fakeClass.argmax(dim=1) == labels_one_hot.argmax(dim=1)).float().mean()\n",
    "        d_realClassLoss = classCriterion(d_realClass, labels_one_hot)\n",
    "        d_realAccuracy = (d_realClass.argmax(dim=1) == labels_one_hot.argmax(dim=1)).float().mean()\n",
    "        d_loss = d_loss_base + (d_fakeClassLoss + d_realClassLoss) / 2\n",
    "        # d_loss.backward()\n",
    "        # discriminator_optimizer.step()\n",
    "        scaler.scale(d_loss).backward()\n",
    "        scaler.step(discriminator_optimizer)\n",
    "\n",
    "        # print(\"g_loss_base: \", g_loss_base.item(), \"g_fakeClassLoss: \", g_fakeClassLoss.item(), \"d_loss_base: \", d_loss_base.item(), \"d_fakeClassLoss: \", d_fakeClassLoss.item(), \"d_realClassLoss: \", d_realClassLoss.item())\n",
    "\n",
    "        # if i == train_batches - 1:\n",
    "        #     fig = px.imshow(fake_images[0].detach().squeeze().cpu().numpy(), color_continuous_scale='Greys')\n",
    "        #     fig.show()\n",
    "        #     fig = px.imshow(real_images[0].detach().squeeze().cpu().numpy(), color_continuous_scale='Greys')\n",
    "        #     fig.show()\n",
    "\n",
    "        correct += (real_validities > 0).sum().item() + (fake_validities < 0).sum().item()\n",
    "        total += len(real_validities) + len(fake_validities)\n",
    "        # if (i+1) % 200 == 0:\n",
    "        #     print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{train_batches}], d_loss: {d_loss_base.item():.4f}, g_loss: {g_loss.item():.4f}\")\n",
    "        g_input = g_input.view(_batch_size, c.latent_dim + NUM_CLASSES)\n",
    "        real_validity = real_validities.mean()\n",
    "        fake_validity = fake_validities.mean()\n",
    "        lr_scheduler_trial.fillReplayBuffer(real_validity, real_validities, fake_validity, fake_validities, g_input)\n",
    "        # print(\"lr_scheduler_trial: \", time.time() - s_time)\n",
    "        i_step = epoch * train_batches + i\n",
    "        t_real_validity[i_step] = real_validity\n",
    "        t_fake_validity[i_step] = fake_validity\n",
    "        t_d_fakeClassLoss[i_step] = d_fakeClassLoss\n",
    "        t_d_realClassLoss[i_step] = d_realClassLoss\n",
    "        t_d_fakeAccuracy[i_step] = d_fakeAccuracy\n",
    "        t_d_realAccuracy[i_step] = d_realAccuracy\n",
    "        t_d_loss_base[i_step] = d_loss_base\n",
    "        t_g_loss_base[i_step] = g_loss_base\n",
    "        t_g_fakeClassLoss[i_step] = g_fakeClassLoss\n",
    "\n",
    "        scaler.update()\n",
    "            \n",
    "    d_lr_scheduler.step()\n",
    "    accuracy = correct / total\n",
    "\n",
    "    lr_scheduler_trial.update_learning_rate(epoch, discriminator, generator)\n",
    "\n",
    "    t_accuracy[epoch] = accuracy\n",
    "    t_d_lr[epoch] = discriminator_optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{c.num_epochs}], d_loss: {d_loss.item():.4f}, g_loss: {g_loss.item():.4f}, accuracy: {accuracy:.4f}, d_lr: {discriminator_optimizer.param_groups[0]['lr']:.6f}\")\n",
    "    if epoch % 20 == 0:\n",
    "        createGridFakeImages(epoch=epoch,cubeSide=5, show=True, log=True, step=i_step)\n",
    "\n",
    "_t_age = t_age.cpu().detach().numpy()\n",
    "_t_curGap = t_curGap.cpu().detach().numpy()\n",
    "_t_oldGap = t_oldGap.cpu().detach().numpy()\n",
    "_t_oldScore = t_oldScore.cpu().detach().numpy()\n",
    "_t_replayScore = t_replayScore.cpu().detach().numpy()\n",
    "_t_replayValidity = t_replayValidity.cpu().detach().numpy()\n",
    "_t_oldValidity = t_oldValidity.cpu().detach().numpy()\n",
    "_t_accuracy = t_accuracy.cpu().detach().numpy()\n",
    "_t_d_lr = t_d_lr.cpu().detach().numpy()\n",
    "\n",
    "_t_real_validity = t_real_validity.cpu().detach().numpy()\n",
    "_t_fake_validity = t_fake_validity.cpu().detach().numpy()\n",
    "_t_d_fakeClassLoss = t_d_fakeClassLoss.cpu().detach().numpy()\n",
    "_t_d_realClassLoss = t_d_realClassLoss.cpu().detach().numpy()\n",
    "_t_d_fakeAccuracy = t_d_fakeAccuracy.cpu().detach().numpy()\n",
    "_t_d_realAccuracy = t_d_realAccuracy.cpu().detach().numpy()\n",
    "_t_d_loss_base = t_d_loss_base.cpu().detach().numpy()\n",
    "_t_g_loss_base = t_g_loss_base.cpu().detach().numpy()\n",
    "\n",
    "imageIndex = 0\n",
    "\n",
    "_t_images = t_images.copy()\n",
    "\n",
    "for epoch in range(c.num_epochs):\n",
    "    for i in range(train_batches):\n",
    "        step = epoch * train_batches + i\n",
    "        if i != train_batches - 1:\n",
    "            metrics = {'real_validity': _t_real_validity[step],\n",
    "                        'fake_validity': _t_fake_validity[step], \n",
    "                        'd_fakeClassLoss': _t_d_fakeClassLoss[step], \n",
    "                        'd_realClassLoss': _t_d_realClassLoss[step], \n",
    "                        'd_fakeAccuracy': _t_d_fakeAccuracy[step], \n",
    "                        'd_realAccuracy': _t_d_realAccuracy[step], \n",
    "                        'd_loss_base': _t_d_loss_base[step], \n",
    "                        'g_loss_base': _t_g_loss_base[step]}\n",
    "            wandb.log(metrics, step=step)\n",
    "    epochMetrics = {'avgAge': _t_age[epoch], \n",
    "                       'curGap': _t_curGap[epoch], \n",
    "                       'oldGap': _t_oldGap[epoch], \n",
    "                       'oldScore': _t_oldScore[epoch], \n",
    "                       'replayScore': _t_replayScore[epoch], \n",
    "                       'replayValidity': _t_replayValidity[epoch], \n",
    "                       'oldValidity': _t_oldValidity[epoch], \n",
    "                       'accuracy': _t_accuracy[epoch], \n",
    "                       'd_lr': _t_d_lr[epoch], }\n",
    "    epochMetrics.update(metrics)\n",
    "    if len(_t_images) and _t_images[0][1] == step:\n",
    "        with TempFileContext() as tmp_filename:\n",
    "            image_bytes = _t_images[0][0]\n",
    "            with open(tmp_filename, 'wb') as tmp_file:\n",
    "                tmp_file.write(image_bytes)\n",
    "            epochMetrics['generator_output'] = wandb.Image(tmp_filename)\n",
    "            wandb.log(epochMetrics, step=step)\n",
    "        _t_images = _t_images[1:]\n",
    "    else:\n",
    "        wandb.log(epochMetrics, step=step)\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 1, 28, 28])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, channels_noise, channels_img, num_classes, features_g):\n",
    "        \"\"\"\n",
    "        channels_noise: The size of the input noise vector. This noise vector is a random input from which the generator begins the generation of a new sample.\n",
    "        channels_img: The number of output channels of the generator. This will typically be 1 for grayscale images or 3 for color (RGB) images.\n",
    "        num_classes: The number of distinct classes or labels that the generator should generate images for. This is used to form the one-hot vector of class labels, which is concatenated to the noise vector to provide the generator with information about the class of image to generate.\n",
    "        \"\"\"\n",
    "        # Conv2d formula => output_size = (input_size - 1) * stride - 2 * padding + kernel_size\n",
    "        super(Generator, self).__init__()\n",
    "        self.gen = nn.Sequential(\n",
    "            self.gen_block(channels_noise + num_classes, features_g * 4, kernel_size=7, stride=1, padding=0),  # output: (features_g*4) x 7 x 7 # Append class labels to input noise.\n",
    "            nn.Dropout(p=0.05),\n",
    "            self._block(features_g * 4, features_g * 2, kernel_size=3, stride=1, padding=1),  # output: (features_g*2) x 6 x 6\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),  # output: (features_g*2) x 14 x 14\n",
    "            nn.Dropout(p=0.05),\n",
    "            self._block(features_g * 2, features_g, kernel_size=3, stride=1, padding=1),  # output: features_g x 14 x 14\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),  # output: features_g x 28 x 28\n",
    "            nn.Dropout(p=0.05),\n",
    "            self._block(features_g, channels_img, kernel_size=5, stride=1, padding=2),  # output: channels_img x 28 x 28\n",
    "            nn.Sigmoid(), # normalize inputs to [0, 1]\n",
    "        )\n",
    "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "    # self.gen_block(channels_noise + num_classes, 256, kernel_size=7, stride=1, padding=0), # Append class labels to input noise.\n",
    "    #         self.gen_block(256, 128, kernel_size=4, stride=2, padding=1),\n",
    "    #         nn.ConvTranspose2d(128, channels_img, kernel_size=4, stride=2, padding=1),\n",
    "    #         nn.Tanh(),\n",
    "    #     )\n",
    "\n",
    "    def gen_block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size,\n",
    "                stride,\n",
    "                padding,\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "    # @autocast() # automatically applies precisions to different operations to speed up calculations\n",
    "    def forward(self, noise):\n",
    "        return self.gen(noise)\n",
    "\n",
    "z = torch.randn(_batch_size, c.latent_dim).to(device)\n",
    "gen = Generator(c.latent_dim, 1, NUM_CLASSES, c.features_g)\n",
    "labels_onehot = torch.zeros(_batch_size, NUM_CLASSES).to(device)\n",
    "labels_onehot.scatter_(1, torch.randint(0, NUM_CLASSES, (_batch_size, 1)).to(device), 1)\n",
    "g_input = torch.cat((z, labels_onehot), dim=1)\n",
    "g_input = g_input.view(g_input.size(0), g_input.size(1), 1, 1)\n",
    "gen(g_input).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.) tensor(0.)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No active exception to reraise",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[70], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfor\u001b[39;00m i, (imgs, labels) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader):\n\u001b[1;32m      2\u001b[0m     \u001b[39mprint\u001b[39m(imgs\u001b[39m.\u001b[39mmax(), imgs\u001b[39m.\u001b[39mmin())\n\u001b[0;32m----> 3\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No active exception to reraise"
     ]
    }
   ],
   "source": [
    "for i, (imgs, labels) in enumerate(train_loader):\n",
    "    print(imgs.max(), imgs.min())\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "batch_size = 64\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "train_data = datasets.MNIST(\n",
    "    root='./dataset/minst/',\n",
    "    train=True,\n",
    "    download=False,\n",
    "    transform=transform\n",
    ")\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_data,\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "train_data.train_data.to(torch.device(\"cuda:0\"))  # put data into GPU entirely\n",
    "train_data.train_labels.to(torch.device(\"cuda:0\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numImages = torch.tensor([2000]).to(device)\n",
    "# Generate and plot fake images with labels\n",
    "labels = torch.randint(0, 10, (numImages,)).to(device)\n",
    "labels_one_hot = torch.zeros(numImages, 10).to(device).scatter_(1, labels.view(numImages, 1), 1)\n",
    "with torch.no_grad():\n",
    "    z = torch.randn(numImages, c.latent_dim).to(device)\n",
    "    g_input = torch.cat((z, labels_one_hot), dim=1)\n",
    "    g_input = g_input.view(numImages, c.latent_dim + NUM_CLASSES, 1, 1)\n",
    "    fake_images = generator(g_input)\n",
    "    fake_validities, d_fakeClass = discriminator(fake_images)\n",
    "    g_fakeClassLoss = classCriterion(d_fakeClass, labels_one_hot)\n",
    "\n",
    "fake_images = fake_images.view(fake_images.size(0), 1, 28, 28)\n",
    "label_text = [str(label.item()) for label in labels]\n",
    "plt.figure(figsize=(5, 5))\n",
    "for i in range(numImages):\n",
    "    plt.subplot(int(numImages**0.5), int(numImages**0.5), i+1)\n",
    "    plt.axis('off')\n",
    "    plt.title(label_text[i], fontsize=10)\n",
    "    plt.imshow(fake_images[i].cpu().squeeze(), cmap='gray')\n",
    "plt.subplots_adjust(wspace=0.25, hspace=0.25)\n",
    "plt.suptitle(\"Generated Images epoch: \" + str(epoch), fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "# # save plotly\n",
    "\n",
    "# # Save the figure to a file\n",
    "# image_path = \"image.jpg\"\n",
    "# plt.savefig(image_path)\n",
    "# # Convert the saved image file to wandb.Image and log using wandb\n",
    "# with open(image_path, \"rb\") as img_file:\n",
    "#     img_data = img_file.read()\n",
    "#     image = Image.open(io.BytesIO(img_data))\n",
    "#     wandb.log({\"generator_output\": wandb.Image(image)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import lr_scheduler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create an optimizer\n",
    "initial_lr = 0.001\n",
    "min_lr = 0.000001\n",
    "optimizer = torch.optim.SGD([torch.randn(1, requires_grad=True)], lr= initial_lr)\n",
    "\n",
    "# Define the number of epochs\n",
    "num_epochs = 100\n",
    "cycles = 4\n",
    "# Learning rate schedulers\n",
    "cosineAnnealingWarmRestarts = lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=int(c.num_epochs/cycles), T_mult=1, eta_min=min_lr)\n",
    "schedulers = {\n",
    "    # \"LambdaLR\": lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda epoch: 0.95 ** epoch),\n",
    "    # \"MultiplicativeLR\": lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lambda epoch: 0.95),\n",
    "    # \"StepLR\": lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1),\n",
    "    # \"MultiStepLR\": lr_scheduler.MultiStepLR(optimizer, milestones=[30, 80], gamma=0.1),\n",
    "    # \"ConstantLR\": lr_scheduler.ConstantLR(optimizer),\n",
    "    # \"LinearLR\" : lr_scheduler.LinearLR(optimizer),\n",
    "    # \"ExponentialLR\": lr_scheduler.ExponentialLR(optimizer, gamma=0.1),\n",
    "    # \"PolynomialLR\": lr_scheduler.PolynomialLR(optimizer,total_iters=4, power=1.0),\n",
    "    # \"CosineAnnealingLR\": lr_scheduler.CosineAnnealingLR(optimizer, T_max=50, eta_min=0),\n",
    "    \"ChainedScheduler\" : lr_scheduler.ChainedScheduler([lr_scheduler.ConstantLR(optimizer, total_iters=10), cosineAnnealingWarmRestarts]),\n",
    "    # \"SequentialLR\": lr_scheduler.SequentialLR(optimizer, schedulers=[lr_scheduler.ConstantLR(optimizer, factor=0.1, total_iters=2), lr_scheduler.ExponentialLR(optimizer, gamma=0.9)], milestones=[2]),\n",
    "    # \"ReduceLROnPlateau\": lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10),\n",
    "    # \"CyclicLR\": lr_scheduler.CyclicLR(optimizer, base_lr=0.01, max_lr=1, step_size_up=5, mode='triangular2'),\n",
    "    # \"OneCycleLR\": lr_scheduler.OneCycleLR(optimizer, max_lr=1, total_steps=num_epochs),\n",
    "    # \"CosineAnnealingWarmRestarts\": lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=int(num_epochs/cycles), T_mult=1, eta_min=min_lr)\n",
    "}\n",
    "\n",
    "# Create a plot for each scheduler\n",
    "for name, scheduler in schedulers.items():\n",
    "    lrs = []\n",
    "    for epoch in range(num_epochs):\n",
    "        optimizer.step()\n",
    "        lrs.append(optimizer.param_groups[0][\"lr\"])\n",
    "        if name != \"ReduceLROnPlateau\":\n",
    "            scheduler.step()\n",
    "        else:\n",
    "            scheduler.step(epoch)  # Assume loss is decreasing with epoch for this example\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(lrs)\n",
    "    plt.title(name)\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvDev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
