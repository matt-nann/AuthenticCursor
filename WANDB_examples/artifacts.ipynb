{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import TensorDataset\n",
    "import wandb\n",
    "import os\n",
    "\n",
    "# Ensure deterministic behavior\n",
    "torch.backends.cudnn.deterministic = True\n",
    "random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed_all(0)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Data parameters\n",
    "num_classes = 10\n",
    "input_shape = (1, 28, 28)\n",
    "\n",
    "# drop slow mirror from list of MNIST mirrors\n",
    "torchvision.datasets.MNIST.mirrors = [mirror for mirror in torchvision.datasets.MNIST.mirrors\n",
    "                                      if not mirror.startswith(\"http://yann.lecun.com\")]\n",
    "\n",
    "def load(train_size=45_000):\n",
    "    \"\"\"\n",
    "    # Load the data\n",
    "    \"\"\"\n",
    "\n",
    "    # the data, split between train and test sets\n",
    "    train = torchvision.datasets.MNIST(\"./\", train=True, download=True)\n",
    "    test = torchvision.datasets.MNIST(\"./\", train=False, download=True)\n",
    "    (x_train, y_train), (x_test, y_test) = (train.data, train.targets), (test.data, test.targets)\n",
    "\n",
    "    # split off a validation set for hyperparameter tuning\n",
    "    x_train, x_val = x_train[:train_size], x_train[train_size:]\n",
    "    y_train, y_val = y_train[:train_size], y_train[train_size:]\n",
    "\n",
    "    training_set = TensorDataset(x_train, y_train)\n",
    "    validation_set = TensorDataset(x_val, y_val)\n",
    "    test_set = TensorDataset(x_test, y_test)\n",
    "\n",
    "    datasets = [training_set, validation_set, test_set]\n",
    "\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/mnann/Documents/Code/AuthenticCursor/WANDB_examples/wandb/run-20230611_104334-rib9yead</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mnann/artifacts-example/runs/rib9yead' target=\"_blank\">toasty-lion-15</a></strong> to <a href='https://wandb.ai/mnann/artifacts-example' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mnann/artifacts-example' target=\"_blank\">https://wandb.ai/mnann/artifacts-example</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mnann/artifacts-example/runs/rib9yead' target=\"_blank\">https://wandb.ai/mnann/artifacts-example/runs/rib9yead</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">toasty-lion-15</strong> at: <a href='https://wandb.ai/mnann/artifacts-example/runs/rib9yead' target=\"_blank\">https://wandb.ai/mnann/artifacts-example/runs/rib9yead</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230611_104334-rib9yead/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def load_and_log():\n",
    "\n",
    "    # üöÄ start a run, with a type to label it and a project it can call home\n",
    "    with wandb.init(project=\"artifacts-example\", job_type=\"load-data\") as run:\n",
    "        \n",
    "        datasets = load()  # separate code for loading the datasets\n",
    "        names = [\"training\", \"validation\", \"test\"]\n",
    "\n",
    "        # üè∫ create our Artifact\n",
    "        raw_data = wandb.Artifact(\n",
    "            \"mnist-raw\", type=\"dataset\",\n",
    "            description=\"Raw MNIST dataset, split into train/val/test\",\n",
    "            metadata={\"source\": \"torchvision.datasets.MNIST\",\n",
    "                      \"sizes\": [len(dataset) for dataset in datasets]})\n",
    "\n",
    "        for name, data in zip(names, datasets):\n",
    "            # üê£ Store a new file in the artifact, and write something into its contents.\n",
    "            with raw_data.new_file(name + \".pt\", mode=\"wb\") as file:\n",
    "                x, y = data.tensors\n",
    "                torch.save((x, y), file)\n",
    "\n",
    "        # ‚úçÔ∏è Save the artifact to W&B.\n",
    "        run.log_artifact(raw_data)\n",
    "\n",
    "load_and_log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(dataset, normalize=True, expand_dims=True):\n",
    "    \"\"\"\n",
    "    ## Prepare the data\n",
    "    \"\"\"\n",
    "    x, y = dataset.tensors\n",
    "\n",
    "    if normalize:\n",
    "        # Scale images to the [0, 1] range\n",
    "        x = x.type(torch.float32) / 255\n",
    "\n",
    "    if expand_dims:\n",
    "        # Make sure images have shape (1, 28, 28)\n",
    "        x = torch.unsqueeze(x, 1)\n",
    "    \n",
    "    return TensorDataset(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_log(steps):\n",
    "\n",
    "    with wandb.init(project=\"artifacts-example\", job_type=\"preprocess-data\") as run:\n",
    "\n",
    "        processed_data = wandb.Artifact(\n",
    "            \"mnist-preprocess\", type=\"dataset\",\n",
    "            description=\"Preprocessed MNIST dataset\",\n",
    "            metadata=steps)\n",
    "         \n",
    "        # ‚úîÔ∏è declare which artifact we'll be using\n",
    "        raw_data_artifact = run.use_artifact('mnist-raw:latest')\n",
    "\n",
    "        # üì• if need be, download the artifact\n",
    "        raw_dataset = raw_data_artifact.download()\n",
    "        \n",
    "        for split in [\"training\", \"validation\", \"test\"]:\n",
    "            raw_split = read(raw_dataset, split)\n",
    "            processed_dataset = preprocess(raw_split, **steps)\n",
    "\n",
    "            with processed_data.new_file(split + \".pt\", mode=\"wb\") as file:\n",
    "                x, y = processed_dataset.tensors\n",
    "                torch.save((x, y), file)\n",
    "\n",
    "        run.log_artifact(processed_data)\n",
    "\n",
    "\n",
    "def read(data_dir, split):\n",
    "    filename = split + \".pt\"\n",
    "    x, y = torch.load(os.path.join(data_dir, filename))\n",
    "\n",
    "    return TensorDataset(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/mnann/Documents/Code/AuthenticCursor/WANDB_examples/wandb/run-20230611_104338-lsbrnvfx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mnann/artifacts-example/runs/lsbrnvfx' target=\"_blank\">robust-firebrand-16</a></strong> to <a href='https://wandb.ai/mnann/artifacts-example' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mnann/artifacts-example' target=\"_blank\">https://wandb.ai/mnann/artifacts-example</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mnann/artifacts-example/runs/lsbrnvfx' target=\"_blank\">https://wandb.ai/mnann/artifacts-example/runs/lsbrnvfx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact mnist-raw:latest, 98.19MB. 3 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   3 of 3 files downloaded.  \n",
      "Done. 0:0:0.2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">robust-firebrand-16</strong> at: <a href='https://wandb.ai/mnann/artifacts-example/runs/lsbrnvfx' target=\"_blank\">https://wandb.ai/mnann/artifacts-example/runs/lsbrnvfx</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230611_104338-lsbrnvfx/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "steps = {\"normalize\": True,\n",
    "         \"expand_dims\": True}\n",
    "\n",
    "preprocess_and_log(steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import floor\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, hidden_layer_sizes=[32, 64],\n",
    "                  kernel_sizes=[3],\n",
    "                  activation=\"ReLU\",\n",
    "                  pool_sizes=[2],\n",
    "                  dropout=0.5,\n",
    "                  num_classes=num_classes,\n",
    "                  input_shape=input_shape):\n",
    "      \n",
    "        super(ConvNet, self).__init__()\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "              nn.Conv2d(in_channels=input_shape[0], out_channels=hidden_layer_sizes[0], kernel_size=kernel_sizes[0]),\n",
    "              getattr(nn, activation)(),\n",
    "              nn.MaxPool2d(kernel_size=pool_sizes[0])\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "              nn.Conv2d(in_channels=hidden_layer_sizes[0], out_channels=hidden_layer_sizes[-1], kernel_size=kernel_sizes[-1]),\n",
    "              getattr(nn, activation)(),\n",
    "              nn.MaxPool2d(kernel_size=pool_sizes[-1])\n",
    "        )\n",
    "        self.layer3 = nn.Sequential(\n",
    "              nn.Flatten(),\n",
    "              nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "        fc_input_dims = floor((input_shape[1] - kernel_sizes[0] + 1) / pool_sizes[0]) # layer 1 output size\n",
    "        fc_input_dims = floor((fc_input_dims - kernel_sizes[-1] + 1) / pool_sizes[-1]) # layer 2 output size\n",
    "        fc_input_dims = fc_input_dims*fc_input_dims*hidden_layer_sizes[-1] # layer 3 output size\n",
    "\n",
    "        self.fc = nn.Linear(fc_input_dims, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/mnann/Documents/Code/AuthenticCursor/WANDB_examples/wandb/run-20230611_104456-lta6rrxn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mnann/artifacts-example/runs/lta6rrxn' target=\"_blank\">glorious-bush-18</a></strong> to <a href='https://wandb.ai/mnann/artifacts-example' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mnann/artifacts-example' target=\"_blank\">https://wandb.ai/mnann/artifacts-example</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mnann/artifacts-example/runs/lta6rrxn' target=\"_blank\">https://wandb.ai/mnann/artifacts-example/runs/lta6rrxn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">glorious-bush-18</strong> at: <a href='https://wandb.ai/mnann/artifacts-example/runs/lta6rrxn' target=\"_blank\">https://wandb.ai/mnann/artifacts-example/runs/lta6rrxn</a><br/>Synced 4 W&B file(s), 0 media file(s), 1 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230611_104456-lta6rrxn/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def build_model_and_log(config):\n",
    "    with wandb.init(project=\"artifacts-example\", job_type=\"initialize\", config=config) as run:\n",
    "        config = wandb.config\n",
    "        \n",
    "        model = ConvNet(**config)\n",
    "\n",
    "        model_artifact = wandb.Artifact(\n",
    "            \"convnet\", type=\"model\",\n",
    "            description=\"Simple AlexNet style CNN\",\n",
    "            metadata=dict(config))\n",
    "\n",
    "        torch.save(model.state_dict(), \"initialized_model.pth\")\n",
    "        # ‚ûï another way to add a file to an Artifact\n",
    "        model_artifact.add_file(\"initialized_model.pth\")\n",
    "\n",
    "        wandb.save(\"initialized_model.pth\")\n",
    "\n",
    "        run.log_artifact(model_artifact)\n",
    "\n",
    "model_config = {\"hidden_layer_sizes\": [32, 64],\n",
    "                \"kernel_sizes\": [3],\n",
    "                \"activation\": \"ReLU\",\n",
    "                \"pool_sizes\": [2],\n",
    "                \"dropout\": 0.5,\n",
    "                \"num_classes\": 10}\n",
    "\n",
    "build_model_and_log(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def train(model, train_loader, valid_loader, config, run):\n",
    "    optimizer = getattr(torch.optim, config.optimizer)(model.parameters())\n",
    "    model.train()\n",
    "    example_ct = 0\n",
    "    for epoch in range(config.epochs):\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = F.cross_entropy(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            example_ct += len(data)\n",
    "\n",
    "            if batch_idx % config.batch_log_interval == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0%})]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                    batch_idx / len(train_loader), loss.item()))\n",
    "                \n",
    "                train_log(loss, example_ct, epoch)\n",
    "\n",
    "        # evaluate the model on the validation set at each epoch\n",
    "        loss, accuracy = test(model, valid_loader)  \n",
    "        test_log(loss, accuracy, example_ct, epoch)\n",
    "\n",
    "        if epoch % config.checkpoint_interval == 0:\n",
    "            checkpoint = {\n",
    "                \"model\": model.state_dict(),\n",
    "                \"optimizer\": optimizer.state_dict(),\n",
    "                \"epoch\": epoch\n",
    "            }\n",
    "            torch.save(checkpoint, \"checkpoint.pth\")\n",
    "            checkpoint_artifact = wandb.Artifact(\n",
    "                \"model-checkpoint\", type=\"model\",\n",
    "                description=\"Model checkpoint\",\n",
    "                metadata=dict(config, epoch=epoch, accuracy=accuracy, example_ct=example_ct)\n",
    "            )\n",
    "            checkpoint_artifact.add_file(\"checkpoint.pth\")\n",
    "            wandb.save(\"checkpoint.pth\")\n",
    "            run.log_artifact(checkpoint_artifact)\n",
    "    \n",
    "def test(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.cross_entropy(output, target, reduction='sum')  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    \n",
    "    return test_loss, accuracy\n",
    "\n",
    "\n",
    "def train_log(loss, example_ct, epoch):\n",
    "    loss = float(loss)\n",
    "\n",
    "    # where the magic happens\n",
    "    wandb.log({\"epoch\": epoch, \"train/loss\": loss}, step=example_ct)\n",
    "    print(f\"Loss after \" + str(example_ct).zfill(5) + f\" examples: {loss:.3f}\")\n",
    "    \n",
    "\n",
    "def test_log(loss, accuracy, example_ct, epoch):\n",
    "    loss = float(loss)\n",
    "    accuracy = float(accuracy)\n",
    "\n",
    "    # where the magic happens\n",
    "    wandb.log({\"epoch\": epoch, \"validation/loss\": loss, \"validation/accuracy\": accuracy}, step=example_ct)\n",
    "    print(f\"Loss/accuracy after \" + str(example_ct).zfill(5) + f\" examples: {loss:.3f}/{accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def evaluate(model, test_loader):\n",
    "    \"\"\"\n",
    "    ## Evaluate the trained model\n",
    "    \"\"\"\n",
    "\n",
    "    loss, accuracy = test(model, test_loader)\n",
    "    highest_losses, hardest_examples, true_labels, predictions = get_hardest_k_examples(model, test_loader.dataset)\n",
    "\n",
    "    return loss, accuracy, highest_losses, hardest_examples, true_labels, predictions\n",
    "\n",
    "def get_hardest_k_examples(model, testing_set, k=32):\n",
    "    model.eval()\n",
    "\n",
    "    loader = DataLoader(testing_set, 1, shuffle=False)\n",
    "\n",
    "    # get the losses and predictions for each item in the dataset\n",
    "    losses = None\n",
    "    predictions = None\n",
    "    with torch.no_grad():\n",
    "        for data, target in loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss = F.cross_entropy(output, target)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            \n",
    "            if losses is None:\n",
    "                losses = loss.view((1, 1))\n",
    "                predictions = pred\n",
    "            else:\n",
    "                losses = torch.cat((losses, loss.view((1, 1))), 0)\n",
    "                predictions = torch.cat((predictions, pred), 0)\n",
    "\n",
    "    argsort_loss = torch.argsort(losses, dim=0).cpu()\n",
    "\n",
    "    highest_k_losses = losses[argsort_loss[-k:]]\n",
    "    hardest_k_examples = testing_set[argsort_loss[-k:]][0]\n",
    "    true_labels = testing_set[argsort_loss[-k:]][1]\n",
    "    predicted_labels = predictions[argsort_loss[-k:]]\n",
    "\n",
    "    return highest_k_losses, hardest_k_examples, true_labels, predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_and_log(config):\n",
    "\n",
    "    with wandb.init(project=\"artifacts-example\", job_type=\"train\", config=config) as run:\n",
    "        config = wandb.config\n",
    "\n",
    "        data = run.use_artifact('mnist-preprocess:latest')\n",
    "        data_dir = data.download()\n",
    "\n",
    "        training_dataset =  read(data_dir, \"training\")\n",
    "        validation_dataset = read(data_dir, \"validation\")\n",
    "\n",
    "        train_loader = DataLoader(training_dataset, batch_size=config.batch_size)\n",
    "        validation_loader = DataLoader(validation_dataset, batch_size=config.batch_size)\n",
    "        \n",
    "        model_artifact = run.use_artifact(\"convnet:latest\")\n",
    "        model_dir = model_artifact.download()\n",
    "        model_path = os.path.join(model_dir, \"initialized_model.pth\")\n",
    "        model_config = model_artifact.metadata\n",
    "        config.update(model_config)\n",
    "\n",
    "        model = ConvNet(**model_config)\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "        model = model.to(device)\n",
    " \n",
    "        train(model, train_loader, validation_loader, config, run)\n",
    "\n",
    "    return model\n",
    "\n",
    "    \n",
    "def evaluate_and_log(config=None):\n",
    "    \n",
    "    with wandb.init(project=\"artifacts-example\", job_type=\"report\", config=config) as run:\n",
    "        data = run.use_artifact('mnist-preprocess:latest')\n",
    "        data_dir = data.download()\n",
    "        testing_set = read(data_dir, \"test\")\n",
    "\n",
    "        test_loader = torch.utils.data.DataLoader(testing_set, batch_size=128, shuffle=False)\n",
    "\n",
    "        model_artifact = run.use_artifact(\"trained-model:latest\")\n",
    "        model_dir = model_artifact.download()\n",
    "        model_path = os.path.join(model_dir, \"trained_model.pth\")\n",
    "        model_config = model_artifact.metadata\n",
    "\n",
    "        model = ConvNet(**model_config)\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "        model.to(device)\n",
    "\n",
    "        loss, accuracy, highest_losses, hardest_examples, true_labels, preds = evaluate(model, test_loader)\n",
    "\n",
    "        run.summary.update({\"loss\": loss, \"accuracy\": accuracy})\n",
    "\n",
    "        wandb.log({\"high-loss-examples\":\n",
    "            [wandb.Image(hard_example, caption=str(int(pred)) + \",\" +  str(int(label)))\n",
    "             for hard_example, pred, label in zip(hardest_examples, preds, true_labels)]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/mnann/Documents/Code/AuthenticCursor/WANDB_examples/wandb/run-20230611_105400-k69itjpo</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mnann/artifacts-example/runs/k69itjpo' target=\"_blank\">blooming-morning-22</a></strong> to <a href='https://wandb.ai/mnann/artifacts-example' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mnann/artifacts-example' target=\"_blank\">https://wandb.ai/mnann/artifacts-example</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mnann/artifacts-example/runs/k69itjpo' target=\"_blank\">https://wandb.ai/mnann/artifacts-example/runs/k69itjpo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact mnist-preprocess:latest, 210.35MB. 3 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   3 of 3 files downloaded.  \n",
      "Done. 0:0:0.6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/45000 (0%)]\tLoss: 2.311475\n",
      "Loss after 00128 examples: 2.311\n",
      "Train Epoch: 0 [3200/45000 (7%)]\tLoss: 1.050662\n",
      "Loss after 03328 examples: 1.051\n",
      "Train Epoch: 0 [6400/45000 (14%)]\tLoss: 0.470708\n",
      "Loss after 06528 examples: 0.471\n",
      "Train Epoch: 0 [9600/45000 (21%)]\tLoss: 0.341590\n",
      "Loss after 09728 examples: 0.342\n",
      "Train Epoch: 0 [12800/45000 (28%)]\tLoss: 0.268526\n",
      "Loss after 12928 examples: 0.269\n",
      "Train Epoch: 0 [16000/45000 (36%)]\tLoss: 0.346665\n",
      "Loss after 16128 examples: 0.347\n",
      "Train Epoch: 0 [19200/45000 (43%)]\tLoss: 0.229439\n",
      "Loss after 19328 examples: 0.229\n",
      "Train Epoch: 0 [22400/45000 (50%)]\tLoss: 0.290198\n",
      "Loss after 22528 examples: 0.290\n",
      "Train Epoch: 0 [25600/45000 (57%)]\tLoss: 0.161129\n",
      "Loss after 25728 examples: 0.161\n",
      "Train Epoch: 0 [28800/45000 (64%)]\tLoss: 0.141200\n",
      "Loss after 28928 examples: 0.141\n",
      "Train Epoch: 0 [32000/45000 (71%)]\tLoss: 0.236443\n",
      "Loss after 32128 examples: 0.236\n",
      "Train Epoch: 0 [35200/45000 (78%)]\tLoss: 0.164023\n",
      "Loss after 35328 examples: 0.164\n",
      "Train Epoch: 0 [38400/45000 (85%)]\tLoss: 0.130169\n",
      "Loss after 38528 examples: 0.130\n",
      "Train Epoch: 0 [41600/45000 (92%)]\tLoss: 0.124026\n",
      "Loss after 41728 examples: 0.124\n",
      "Train Epoch: 0 [44800/45000 (99%)]\tLoss: 0.211179\n",
      "Loss after 44928 examples: 0.211\n",
      "Loss/accuracy after 45000 examples: 0.113/96.627\n",
      "Train Epoch: 1 [0/45000 (0%)]\tLoss: 0.143940\n",
      "Loss after 45128 examples: 0.144\n",
      "Train Epoch: 1 [3200/45000 (7%)]\tLoss: 0.064108\n",
      "Loss after 48328 examples: 0.064\n",
      "Train Epoch: 1 [6400/45000 (14%)]\tLoss: 0.104878\n",
      "Loss after 51528 examples: 0.105\n",
      "Train Epoch: 1 [9600/45000 (21%)]\tLoss: 0.084811\n",
      "Loss after 54728 examples: 0.085\n",
      "Train Epoch: 1 [12800/45000 (28%)]\tLoss: 0.058793\n",
      "Loss after 57928 examples: 0.059\n",
      "Train Epoch: 1 [16000/45000 (36%)]\tLoss: 0.089075\n",
      "Loss after 61128 examples: 0.089\n",
      "Train Epoch: 1 [19200/45000 (43%)]\tLoss: 0.074186\n",
      "Loss after 64328 examples: 0.074\n",
      "Train Epoch: 1 [22400/45000 (50%)]\tLoss: 0.072618\n",
      "Loss after 67528 examples: 0.073\n",
      "Train Epoch: 1 [25600/45000 (57%)]\tLoss: 0.083986\n",
      "Loss after 70728 examples: 0.084\n",
      "Train Epoch: 1 [28800/45000 (64%)]\tLoss: 0.034438\n",
      "Loss after 73928 examples: 0.034\n",
      "Train Epoch: 1 [32000/45000 (71%)]\tLoss: 0.138906\n",
      "Loss after 77128 examples: 0.139\n",
      "Train Epoch: 1 [35200/45000 (78%)]\tLoss: 0.090532\n",
      "Loss after 80328 examples: 0.091\n",
      "Train Epoch: 1 [38400/45000 (85%)]\tLoss: 0.080360\n",
      "Loss after 83528 examples: 0.080\n",
      "Train Epoch: 1 [41600/45000 (92%)]\tLoss: 0.052268\n",
      "Loss after 86728 examples: 0.052\n",
      "Train Epoch: 1 [44800/45000 (99%)]\tLoss: 0.095002\n",
      "Loss after 89928 examples: 0.095\n",
      "Loss/accuracy after 90000 examples: 0.073/97.733\n",
      "Train Epoch: 2 [0/45000 (0%)]\tLoss: 0.091946\n",
      "Loss after 90128 examples: 0.092\n",
      "Train Epoch: 2 [3200/45000 (7%)]\tLoss: 0.030583\n",
      "Loss after 93328 examples: 0.031\n",
      "Train Epoch: 2 [6400/45000 (14%)]\tLoss: 0.080938\n",
      "Loss after 96528 examples: 0.081\n",
      "Train Epoch: 2 [9600/45000 (21%)]\tLoss: 0.051802\n",
      "Loss after 99728 examples: 0.052\n",
      "Train Epoch: 2 [12800/45000 (28%)]\tLoss: 0.044807\n",
      "Loss after 102928 examples: 0.045\n",
      "Train Epoch: 2 [16000/45000 (36%)]\tLoss: 0.046397\n",
      "Loss after 106128 examples: 0.046\n",
      "Train Epoch: 2 [19200/45000 (43%)]\tLoss: 0.051966\n",
      "Loss after 109328 examples: 0.052\n",
      "Train Epoch: 2 [22400/45000 (50%)]\tLoss: 0.036805\n",
      "Loss after 112528 examples: 0.037\n",
      "Train Epoch: 2 [25600/45000 (57%)]\tLoss: 0.061983\n",
      "Loss after 115728 examples: 0.062\n",
      "Train Epoch: 2 [28800/45000 (64%)]\tLoss: 0.018508\n",
      "Loss after 118928 examples: 0.019\n",
      "Train Epoch: 2 [32000/45000 (71%)]\tLoss: 0.119412\n",
      "Loss after 122128 examples: 0.119\n",
      "Train Epoch: 2 [35200/45000 (78%)]\tLoss: 0.075214\n",
      "Loss after 125328 examples: 0.075\n",
      "Train Epoch: 2 [38400/45000 (85%)]\tLoss: 0.079117\n",
      "Loss after 128528 examples: 0.079\n",
      "Train Epoch: 2 [41600/45000 (92%)]\tLoss: 0.036037\n",
      "Loss after 131728 examples: 0.036\n",
      "Train Epoch: 2 [44800/45000 (99%)]\tLoss: 0.053438\n",
      "Loss after 134928 examples: 0.053\n",
      "Loss/accuracy after 135000 examples: 0.063/97.993\n",
      "Train Epoch: 3 [0/45000 (0%)]\tLoss: 0.082626\n",
      "Loss after 135128 examples: 0.083\n",
      "Train Epoch: 3 [3200/45000 (7%)]\tLoss: 0.018534\n",
      "Loss after 138328 examples: 0.019\n",
      "Train Epoch: 3 [6400/45000 (14%)]\tLoss: 0.066729\n",
      "Loss after 141528 examples: 0.067\n",
      "Train Epoch: 3 [9600/45000 (21%)]\tLoss: 0.032349\n",
      "Loss after 144728 examples: 0.032\n",
      "Train Epoch: 3 [12800/45000 (28%)]\tLoss: 0.038310\n",
      "Loss after 147928 examples: 0.038\n",
      "Train Epoch: 3 [16000/45000 (36%)]\tLoss: 0.026710\n",
      "Loss after 151128 examples: 0.027\n",
      "Train Epoch: 3 [19200/45000 (43%)]\tLoss: 0.038680\n",
      "Loss after 154328 examples: 0.039\n",
      "Train Epoch: 3 [22400/45000 (50%)]\tLoss: 0.020585\n",
      "Loss after 157528 examples: 0.021\n",
      "Train Epoch: 3 [25600/45000 (57%)]\tLoss: 0.053685\n",
      "Loss after 160728 examples: 0.054\n",
      "Train Epoch: 3 [28800/45000 (64%)]\tLoss: 0.011330\n",
      "Loss after 163928 examples: 0.011\n",
      "Train Epoch: 3 [32000/45000 (71%)]\tLoss: 0.094625\n",
      "Loss after 167128 examples: 0.095\n",
      "Train Epoch: 3 [35200/45000 (78%)]\tLoss: 0.066720\n",
      "Loss after 170328 examples: 0.067\n",
      "Train Epoch: 3 [38400/45000 (85%)]\tLoss: 0.073124\n",
      "Loss after 173528 examples: 0.073\n",
      "Train Epoch: 3 [41600/45000 (92%)]\tLoss: 0.028469\n",
      "Loss after 176728 examples: 0.028\n",
      "Train Epoch: 3 [44800/45000 (99%)]\tLoss: 0.032034\n",
      "Loss after 179928 examples: 0.032\n",
      "Loss/accuracy after 180000 examples: 0.060/98.173\n",
      "Train Epoch: 4 [0/45000 (0%)]\tLoss: 0.076903\n",
      "Loss after 180128 examples: 0.077\n",
      "Train Epoch: 4 [3200/45000 (7%)]\tLoss: 0.014514\n",
      "Loss after 183328 examples: 0.015\n",
      "Train Epoch: 4 [6400/45000 (14%)]\tLoss: 0.055983\n",
      "Loss after 186528 examples: 0.056\n",
      "Train Epoch: 4 [9600/45000 (21%)]\tLoss: 0.025155\n",
      "Loss after 189728 examples: 0.025\n",
      "Train Epoch: 4 [12800/45000 (28%)]\tLoss: 0.033943\n",
      "Loss after 192928 examples: 0.034\n",
      "Train Epoch: 4 [16000/45000 (36%)]\tLoss: 0.017423\n",
      "Loss after 196128 examples: 0.017\n",
      "Train Epoch: 4 [19200/45000 (43%)]\tLoss: 0.026835\n",
      "Loss after 199328 examples: 0.027\n",
      "Train Epoch: 4 [22400/45000 (50%)]\tLoss: 0.015397\n",
      "Loss after 202528 examples: 0.015\n",
      "Train Epoch: 4 [25600/45000 (57%)]\tLoss: 0.053619\n",
      "Loss after 205728 examples: 0.054\n",
      "Train Epoch: 4 [28800/45000 (64%)]\tLoss: 0.007672\n",
      "Loss after 208928 examples: 0.008\n",
      "Train Epoch: 4 [32000/45000 (71%)]\tLoss: 0.072695\n",
      "Loss after 212128 examples: 0.073\n",
      "Train Epoch: 4 [35200/45000 (78%)]\tLoss: 0.059383\n",
      "Loss after 215328 examples: 0.059\n",
      "Train Epoch: 4 [38400/45000 (85%)]\tLoss: 0.067376\n",
      "Loss after 218528 examples: 0.067\n",
      "Train Epoch: 4 [41600/45000 (92%)]\tLoss: 0.022908\n",
      "Loss after 221728 examples: 0.023\n",
      "Train Epoch: 4 [44800/45000 (99%)]\tLoss: 0.021816\n",
      "Loss after 224928 examples: 0.022\n",
      "Loss/accuracy after 225000 examples: 0.059/98.180\n",
      "Train Epoch: 5 [0/45000 (0%)]\tLoss: 0.071660\n",
      "Loss after 225128 examples: 0.072\n",
      "Train Epoch: 5 [3200/45000 (7%)]\tLoss: 0.012224\n",
      "Loss after 228328 examples: 0.012\n",
      "Train Epoch: 5 [6400/45000 (14%)]\tLoss: 0.048019\n",
      "Loss after 231528 examples: 0.048\n",
      "Train Epoch: 5 [9600/45000 (21%)]\tLoss: 0.022661\n",
      "Loss after 234728 examples: 0.023\n",
      "Train Epoch: 5 [12800/45000 (28%)]\tLoss: 0.028439\n",
      "Loss after 237928 examples: 0.028\n",
      "Train Epoch: 5 [16000/45000 (36%)]\tLoss: 0.012316\n",
      "Loss after 241128 examples: 0.012\n",
      "Train Epoch: 5 [19200/45000 (43%)]\tLoss: 0.016803\n",
      "Loss after 244328 examples: 0.017\n",
      "Train Epoch: 5 [22400/45000 (50%)]\tLoss: 0.015294\n",
      "Loss after 247528 examples: 0.015\n",
      "Train Epoch: 5 [25600/45000 (57%)]\tLoss: 0.053666\n",
      "Loss after 250728 examples: 0.054\n",
      "Train Epoch: 5 [28800/45000 (64%)]\tLoss: 0.005869\n",
      "Loss after 253928 examples: 0.006\n",
      "Train Epoch: 5 [32000/45000 (71%)]\tLoss: 0.056862\n",
      "Loss after 257128 examples: 0.057\n",
      "Train Epoch: 5 [35200/45000 (78%)]\tLoss: 0.049053\n",
      "Loss after 260328 examples: 0.049\n",
      "Train Epoch: 5 [38400/45000 (85%)]\tLoss: 0.060348\n",
      "Loss after 263528 examples: 0.060\n",
      "Train Epoch: 5 [41600/45000 (92%)]\tLoss: 0.018613\n",
      "Loss after 266728 examples: 0.019\n",
      "Train Epoch: 5 [44800/45000 (99%)]\tLoss: 0.017266\n",
      "Loss after 269928 examples: 0.017\n",
      "Loss/accuracy after 270000 examples: 0.059/98.260\n",
      "Train Epoch: 6 [0/45000 (0%)]\tLoss: 0.065924\n",
      "Loss after 270128 examples: 0.066\n",
      "Train Epoch: 6 [3200/45000 (7%)]\tLoss: 0.010834\n",
      "Loss after 273328 examples: 0.011\n",
      "Train Epoch: 6 [6400/45000 (14%)]\tLoss: 0.040848\n",
      "Loss after 276528 examples: 0.041\n",
      "Train Epoch: 6 [9600/45000 (21%)]\tLoss: 0.019642\n",
      "Loss after 279728 examples: 0.020\n",
      "Train Epoch: 6 [12800/45000 (28%)]\tLoss: 0.021656\n",
      "Loss after 282928 examples: 0.022\n",
      "Train Epoch: 6 [16000/45000 (36%)]\tLoss: 0.009593\n",
      "Loss after 286128 examples: 0.010\n",
      "Train Epoch: 6 [19200/45000 (43%)]\tLoss: 0.011131\n",
      "Loss after 289328 examples: 0.011\n",
      "Train Epoch: 6 [22400/45000 (50%)]\tLoss: 0.016401\n",
      "Loss after 292528 examples: 0.016\n",
      "Train Epoch: 6 [25600/45000 (57%)]\tLoss: 0.050028\n",
      "Loss after 295728 examples: 0.050\n",
      "Train Epoch: 6 [28800/45000 (64%)]\tLoss: 0.005065\n",
      "Loss after 298928 examples: 0.005\n",
      "Train Epoch: 6 [32000/45000 (71%)]\tLoss: 0.045286\n",
      "Loss after 302128 examples: 0.045\n",
      "Train Epoch: 6 [35200/45000 (78%)]\tLoss: 0.039873\n",
      "Loss after 305328 examples: 0.040\n",
      "Train Epoch: 6 [38400/45000 (85%)]\tLoss: 0.048867\n",
      "Loss after 308528 examples: 0.049\n",
      "Train Epoch: 6 [41600/45000 (92%)]\tLoss: 0.014518\n",
      "Loss after 311728 examples: 0.015\n",
      "Train Epoch: 6 [44800/45000 (99%)]\tLoss: 0.016451\n",
      "Loss after 314928 examples: 0.016\n",
      "Loss/accuracy after 315000 examples: 0.058/98.247\n",
      "Train Epoch: 7 [0/45000 (0%)]\tLoss: 0.055824\n",
      "Loss after 315128 examples: 0.056\n",
      "Train Epoch: 7 [3200/45000 (7%)]\tLoss: 0.009641\n",
      "Loss after 318328 examples: 0.010\n",
      "Train Epoch: 7 [6400/45000 (14%)]\tLoss: 0.030119\n",
      "Loss after 321528 examples: 0.030\n",
      "Train Epoch: 7 [9600/45000 (21%)]\tLoss: 0.020151\n",
      "Loss after 324728 examples: 0.020\n",
      "Train Epoch: 7 [12800/45000 (28%)]\tLoss: 0.016379\n",
      "Loss after 327928 examples: 0.016\n",
      "Train Epoch: 7 [16000/45000 (36%)]\tLoss: 0.008410\n",
      "Loss after 331128 examples: 0.008\n",
      "Train Epoch: 7 [19200/45000 (43%)]\tLoss: 0.008256\n",
      "Loss after 334328 examples: 0.008\n",
      "Train Epoch: 7 [22400/45000 (50%)]\tLoss: 0.015224\n",
      "Loss after 337528 examples: 0.015\n",
      "Train Epoch: 7 [25600/45000 (57%)]\tLoss: 0.045736\n",
      "Loss after 340728 examples: 0.046\n",
      "Train Epoch: 7 [28800/45000 (64%)]\tLoss: 0.004280\n",
      "Loss after 343928 examples: 0.004\n",
      "Train Epoch: 7 [32000/45000 (71%)]\tLoss: 0.036560\n",
      "Loss after 347128 examples: 0.037\n",
      "Train Epoch: 7 [35200/45000 (78%)]\tLoss: 0.030937\n",
      "Loss after 350328 examples: 0.031\n",
      "Train Epoch: 7 [38400/45000 (85%)]\tLoss: 0.041467\n",
      "Loss after 353528 examples: 0.041\n",
      "Train Epoch: 7 [41600/45000 (92%)]\tLoss: 0.011630\n",
      "Loss after 356728 examples: 0.012\n",
      "Train Epoch: 7 [44800/45000 (99%)]\tLoss: 0.014572\n",
      "Loss after 359928 examples: 0.015\n",
      "Loss/accuracy after 360000 examples: 0.059/98.307\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>train/loss</td><td>‚ñà‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>validation/accuracy</td><td>‚ñÅ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>validation/loss</td><td>‚ñà‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>7</td></tr><tr><td>train/loss</td><td>0.01457</td></tr><tr><td>validation/accuracy</td><td>98.30666</td></tr><tr><td>validation/loss</td><td>0.05864</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">blooming-morning-22</strong> at: <a href='https://wandb.ai/mnann/artifacts-example/runs/k69itjpo' target=\"_blank\">https://wandb.ai/mnann/artifacts-example/runs/k69itjpo</a><br/>Synced 5 W&B file(s), 0 media file(s), 4 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230611_105400-k69itjpo/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/mnann/Documents/Code/AuthenticCursor/WANDB_examples/wandb/run-20230611_105625-fdwmn94u</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mnann/artifacts-example/runs/fdwmn94u' target=\"_blank\">apricot-feather-23</a></strong> to <a href='https://wandb.ai/mnann/artifacts-example' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mnann/artifacts-example' target=\"_blank\">https://wandb.ai/mnann/artifacts-example</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mnann/artifacts-example/runs/fdwmn94u' target=\"_blank\">https://wandb.ai/mnann/artifacts-example/runs/fdwmn94u</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact mnist-preprocess:latest, 210.35MB. 3 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   3 of 3 files downloaded.  \n",
      "Done. 0:0:0.6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>98.73</td></tr><tr><td>loss</td><td>0.0375</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">apricot-feather-23</strong> at: <a href='https://wandb.ai/mnann/artifacts-example/runs/fdwmn94u' target=\"_blank\">https://wandb.ai/mnann/artifacts-example/runs/fdwmn94u</a><br/>Synced 5 W&B file(s), 32 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230611_105625-fdwmn94u/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_config = dict(batch_size=128, epochs=8, batch_log_interval=25, optimizer=\"Adam\", checkpoint_interval=2)\n",
    "model = train_and_log(train_config)\n",
    "evaluate_and_log()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvDev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
